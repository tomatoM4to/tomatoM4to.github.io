# 조작(Manipulations)

이전에 텐서를 초기화하고 연산을 하고 이젠 조작을 해보겠습니다. 텐서에 대해 마지막으로 다루는 장입니다.

## 인덱싱
텐서의 인덱싱은 np 모듈 사용 방법과 유사합니다. 텐서의 인덱싱은 0부터 시작합니다.

```python
import torch

x = torch.tensor([[1, 2, 3], [4, 5, 6]])

print(x[0, 0]) # 1
print(x[0, 1]) # 2
print(x[1, 0]) # 4
```


## 슬라이싱
열을 슬라이상 하거나 행을 슬라이싱 할 수 있습니다. 그리고 각 원소를 출력하는 방법 입니다.

```python
import torch

x = torch.tensor([[1, 2, 3], [4, 5, 6]])

print(x[:, 0]) # 1, 4
print(x[:, 1]) # 2, 5
print(x[:, 2]) # 3, 6

print(x[0, :]) # 1, 2, 3
print(x[1, :]) # 4, 5, 6
```

## 텐서의 크기나 모양 변경
텐서의 크기나 모양을 변경하는 방법입니다. `view` 메서드를 사용합니다. `view`의 메서드를 사용할시 주의할점은 변경전 변경후 텐서 안의 원소의 개수가 유지 되어야 합니다.

```python
import torch

x = torch.tensor([[1, 2, 3], [4, 5, 6]])

print(x)

y = x.view(6)
print(y) # 2차원 텐서가 6개의 원소를 가진 1차원 텐서로 변경

z = x.view(3, -1) # 행은 3개로 고정하고 열은 자동으로 맞춰줌
print(z)
```

## item
텐서에 값이 단 하나라도 존재하면 숫자 값을 얻을 수 있습니다. 이때 `item` 메서드를 사용합니다. 참고로 `item` 메서드는 스칼라 값 하나만 존재 해야 합니다.

```python
import torch

x = torch.tensor(1)
print(x.item()) # 1

y = torch.tensor([1, 2])
print(y.item()) # 에러 발생
```


## 차원 추소
굉장히 많이 사용하는 연산입니다. 이 연산은 차원을 축소 하는 연산입니다. 축소를 할시 남겨진 부분은 제거가 되기에 어떻게 보면 제거 라 봐도 무방합니다.

```python
import torch

x = torch.rand(3, 3, 3) # 3차원 텐서 생성
print(x)
print(x.shape) # 3, 3, 3

t = tensor.squeeze() # 차원이 1인 차원을 제거?
print(t) # 모.. 줄어들긴 했네
print(t.shape)
```

## 차원 증가
차원을 증가시키는 방법입니다. `unsqueeze` 메서드를 사용합니다. 이 메서드는 인자로 dim을 정의 해주어야 합니다.

```python
import torch

x = torch.tensor([1, 2, 3])
print(x)

y = x.unsqueeze(dim = 0) # 0번째 차원을 기준으로 차원을 늘려줌
print(y)
print(y.shape) # 1, 3

z = y.unsqueeze(dim = 2) # 2번째 차원을 기준으로 차원을 늘려줌
print(z)
print(z.shape) # 1, 3, 1
```

**OUTPUT**
```
tensor([1, 2, 3])

tensor([[1, 2, 3]])
torch.Size([1, 3])

tensor([[[1],
         [2],
         [3]]])
torch.Size([1, 3, 1])
```

## 텐서간 결합
`stack` 메서드를 사용하면 텐서를 결합할 수 있습니다. 이때 결합할때 차원이 같아야 합니다.

TODO: 차원이 같아야 하는건 학인 해야함
```python
import torch

x = torch.tensor([1, 2, 3])
y = torch.tensor([4, 5, 6])
z = torch.tensor([7, 8, 9])

t = torch.stack([x, y, z])

print(t)
```

**OUTPUT**
```python
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
```

또 `cat` 라는 메서드를 사용할수 있습니다. `stack`과 듀사하지만 쌓을 차원정보인 `dim`이 존재 해야 합니다. 내부적인 동작으론 해당 차원을 늘려준 후 결합을 합니다.

`cat` 은 `concatenate`의 줄임말 입니다.

```python
import torch

x = torch.tensor([1, 2, 3])
y = torch.tensor([4, 5, 6])

t = torch.cat([x, y])
print(t)
print(t.size())

x = torch.tensor([1, 2, 3, 4])
y = torch.tensor([5, 6, 7])
t = torch.cat([x, y])
print(t)
print(t.size())
```

**OUTPUT**
```shell
tensor([1, 2, 3, 4, 5, 6])
torch.Size([6])
tensor([1, 2, 3, 4, 5, 6, 7])
torch.Size([7])
```


## 텐서 분할
`chunk`와 `split` 메서드를 사용하면 텐서를 분할할 수 있습니다. `chunk`는 동일한 크기로 텐서를 분할하고 `split`은 지정한 크기로 텐서를 분할합니다.

```python
import torch

x = torch.tensor([
    [1, 2, 3, 4, 5, 6],
    [7, 8, 9, 10, 11, 12],
    [13, 14, 15, 16, 17, 18]
])

t1, t2, t3 = torch.chunk(x, 3, dim=1) # x 를 3개로 나눠주는데 두번째 차원 기준으로 나눠줌
print(t1)
print(t2)
print(t3)
```

`spli`은 `chunk`와 동일한 기능이지만, 나눌때 어떤 방식인지 차이가 있습니다. `chunk`는 몇개로 누눌거냐, `split`은 누눌 크기를 지정합니다.

```python
import torch

x = torch.tensor([
    [1, 2, 3, 4, 5, 6],
    [7, 8, 9, 10, 11, 12],
    [13, 14, 15, 16, 17, 18]
])

t1, t2 = torch.split(x, 3, dim=1) # 두번째 인자로 텐서의 크기가 몇인지 지정
print(t1)
print(t2)
```

## torch to numpy, numpy to torch
이전에 torch 자체가 numpy와 굉장히 유사한 라이브러리라 했습니다. 그래서 기능상 토치의 값을 numpy로 변환하거나 numpy 값을 토치로 변환하는 기능이 있습니다.

```python
import torch
import numpy as np

x = torch.tensor([1, 2, 3])
y = x.numpy()

print(x)
print(y)

y.add_(1) # numpy 값에 1을 더함

print(x) # 2, 3, 4
print(y) # 2, 3, 4
```

특징중 하나는 토치와 넘파이는 같은 메모리를 공유한다는 점입니다. 그래서 넘파이 값에 연산을 하면 토치 값도 변경됩니다.

그 반대도 가능 합니다.
```python
import torch
import numpy as np

x = np.array([1, 2, 3])
y = torch.from_numpy(x)

print(x)
print(y)
```

주의할 점은 메모리를 공유하는 동작은 CPU에 한해서만 가능합니다. GPU에 올라간 텐서는 별도의 메모리 공간에 존재해 메모리 공유가 불가능합니다.