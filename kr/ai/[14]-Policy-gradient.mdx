DQN 에선 행동을 0, 1, 2, 3, ...

해놨는데 이건 0.0 ~ 1.0

그냥 통짜로 액션 선택

# REINFORCE
서튼이란 사람이 만든 알고리즘

목적: 에피소드 내에서 얻을 수 있는 ...

총 보상을 고려? 언제 총 보상을 더하야 하나? 이전까지 그런것 한적 없음

그래서 $G_t$ 로 바꾸기 위해 시그마 안쪽으로 넣어줌, t가 없어서 그냥 바꾸지 못함, 근데 게임 같은건 플레이 하면 되니까 타입스탭 같은거 신경 안써도 되서 더 효율적으로 바꿀수 있음


몬테카를로는 에피소드가 끝나야 학습을 할 수 있음

# Variance vs bias
강화학습 기준으로 설명

몬테카를로는 우측위라 할수 있음, Q-learning 같은 템포럴 어저구는 좌측 아래

둘중 어느게 좋나? 그럴수 없음, 트레이드 오프 관계임 한쪽이 좋아지면 한쪽이 나빠짐

# Speed and Direction in Update Rule

상태에 의존적이라면 $b(S_t)$ 는 만족

# 공식

첫번째 줄 왼쪽에서 *1 트릭 사용, 모든 확률 1 로 변횐


빠르게 메게변수 세타가 학습됨: 이 식이 매우 중요함

gt - 어쩌구 이게 중요 -: 상태 가치 함수!

상태에만 의존적인 대표적인 함수: 상태 가치 함수

