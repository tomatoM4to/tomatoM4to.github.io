# 문제 발생
이번 시간에는 강화학습에서 가장 중요한 알고리즘이라 할수 있는 DQN에 대해 알아보겠습니다.

지난 시간에 Q-table로 Q-learning를 구현하고, 이 Q-table의 한계 때문에 Network를 이용하여 Q-learning을 구현하는 방법을 알아보았습니다. 하지만 코드를 실행하면 알수 있듯 그 결과가 매우 형편없었습니다 단순한 미로나 막대기를 새우는 문제조차 제대로 풀지 못했습니다. 이번엔 왜 이런 문제가 발생했는지 또 해결 방법은 무엇인지 알아보겠습니다.

이전에 말했듯 $\hat{Q}$ 은 Converges 하지 않는단 이유를 가지고 있었습니다.
* Correlations between samples(샘플간의 상관관계)
* Non-stationary targets(움직이는 타겟)

이러한 이유 때문에 신명망을 사용하면 차이가 발생합니다. 이는 학습이 잘 돼지 않는다는 의미입니다.


사실 Q-learning과 NN을 결합시키려는 시도는 매우 오래전부터 있었습니다. 하지만 저 2가지 문제를 해결해 이를 성공적으로 구현한 것은 2013년 DeepMind의 연구팀이었습니다. 이들은 Q-learning을 이용하여 Atari 게임을 플레이하는 AI를 만들었는데, 이것이 바로 DQN입니다.

* DQN paper: https://nature.com/articles/nature14236
한번쯤 읽어보시길 추천드립니다.

***

# 문제점
기존의 문제가 뭘까요?

* Correlations between samples(샘플간의 상관관계)
* Non-stationary targets(움직이는 타겟)

이겁니다. 이를 해결하기 위해 DeepMind 팀은 간단한 솔류션을 제시했습니다.

한번 생각해 봅시다. 미로를 탈출한다고 했을때, 혹은 이전처럼 막대기를 새운다 했을때 0.1초전과 지금의 상태는 많이 다를까요? 음 그렇진 않거 같습니다. 상당히 유사한 상태일 것입니다. action을 취할시 환경이 급격히 바뀌는게 아닌 조금씩 바뀌기 때문에, 받아오는 데이터들 혹은 sample 들의 연관성이 유사하단 점이 바로 문젭니다.

TODO: Linear regression 그래프 예제 필요

두번째는 타켓이 움직인다 라는 겁니다.

```math
\min_{\theta} \sum_{t=0}^T [\hat{Q}(s_t, a_t|\theta) - (r_t + \gamma \max_{a'}\hat{Q}(s_{t+1}, a'|\theta))]^2
```

이전 공식에서 주의해서 보아야 할것이 두부분 있습니다.

```math
\hat{Q}(s_t, a_t|\theta)
```

```math
(r_t + \gamma \max_{a'}\hat{Q}(s_{t+1}, a'|\theta))
```

이 두 Q값이 서로 같은 신경망을 통해 계산되기 때문에, 이 두 Q값이 서로 영향을 주게 됩니다. 이는 Non-stationary targets 라는 문제를 발생시킵니다.

$\hat{Q}(s_t, a_t|\theta)$ 부분에서 다음 상태에 가까워 지기 위해 네트워크를 업데이트 시키고 그 다음 계산인 $(r_t + \gamma \max_{a'}\hat{Q}(s_{t+1}, a'|\theta))$ 부분에서는 업데이트된 네트워크를 사용하고 다시 업데이트를 시킵니다. 확실히 문제가 발생할 수 밖에 없습니다.

***

# DQN's three solutions
이 문제를 해결하기 위해 DeepMind 팀은 3가지 솔루션을 제시했습니다.

1. Go deep: 더 깊은 신경망을 사용한다.
2. Capture and replay: Correlations between samples(샘플간의 상관관계)를 해결하기 위해 sample을 저장하고 랜덤하게 뽑아서 학습한다.
3. Separate networks: Non-stationary targets(움직이는 타겟)를 해결하기 위해 2개의 네트워크를 사용한다.

## Go deep
시원시원한 생각이죠, 이전엔 하나의 layer만 사용한걸 늘리면 되겠다는 생각입니다. 이는 더 복잡한 문제를 풀 수 있게 해줍니다.

* End-to-end learning of values Q(S, a) from pixels s
* Input state s is stack of raw pixels from last 4 frames
* Output is Q(s, a) for 18 joystick/button positions
* Reward is change in score for that frame

TODO: 위 내용 이해 필요, 설명 없었음

## Capture and replay
굉장히 중유한 솔루션 입니다. 너무 연관있는 데이터들로 학습을 시키니 학습이 잘 돼지 않았던 문제를 해결하기 위해 experience replay라는 방법을 사용합니다.

Agent 에게 action을 취하게 하는 루프를 돌면 이 action을 통해 상태를 받아오게 돼는데 이전엔 바로바로 학습을 시켰습니다. 이를 바꿔숴 중간에 버퍼에 저장을 한다음, 이 버퍼가 다 차면 혹은 일정한 시간이 지나면 해당 버퍼의 담겨진 데이터중 랜덤으로 하나 뽑아서 학습을 시킵니다.\

TODO: 사진


TODO: 의사 코드 사진 필요

* **Store transitions (s, a, r, s') in replay memory D**: action을 취한다음 D 라는 버퍼에다가 상태, 액션, 보상, 다음 상태를 저장합니다.
* **Sample random minibatch of transitions (s, a, r, s') from D**: D에서 랜덤하게 샘플을 뽑아 mini batch를 만들어 이걸 가지고 학습을 진행합니다.

아주 간단한 아이디어지만 왜 될까요?

눈치 채신분들도 많겠지만 이렇게 해봅시다.

버퍼에 어느정도 데이터가 쌓어셔 이걸 그래프로 표현해보겠습니다.

이런 그래프가 나왔을때 기존엔 편향된 데이터가 학습이 될수 있었지만 이들중 몇개만 랜덤으로 뽑아 학습을 하게 되면 우리가 원했던 기울기의 그래프가 나올 가능성이 훨씬 크지 않을까요? 아주 심플한 아이디어지만 실제로 매우 잘 작동합니다.

## Separate networks
이또한 매우 심플한 아이디어죠, 사용하는 NN가 같아 Non-stationary targets 문제가 발생했는데, 이를 해결하기 위해 2개의 네트워크를 사용합니다.

공식에 익숙해 졌을거라 생각하니 바로 공식부터 보여드리면 이렇습니다.

```math
\min_{\theta} \sum_{t=0}^T [\hat{Q}(s_t, a_t|\theta) - (r_t + \gamma \max_{a'}\hat{Q}(s_{t+1}, a'|\bar{\theta}))]^2
```

양쪽 Q의 $\theta$ 가 다르다는것을 바로 눈치채실수 있을겁니다. 이는 서로 다른 NN을 사용한다는 의밉니다.

이때 $\hat{Q}(s_t, a_t|\theta)$ 에서 사용돼는 $\theta$ 를 policy network라고 부르고, $(r_t + \gamma \max_{a'}\hat{Q}(s_{t+1}, a'|\bar{\theta}))$ 에서 사용되는 $\bar{\theta}$ 를 target network라고 부릅니다.

중요한점은 학습을 시킬때는 policy network 많을 업데이트 시킵니다. target network는 움직이지 않고 있다가 policy network가 일정한 시간이 지나거나 일정한 횟수만큼 업데이트 될때마다 policy network의 weight를 target network로 복사합니다.

TODO: 의사코드

정말 심플하지만 이또한 정말 잘 작동합니다.

***

# 요약
TODO: 의사코드와 요약