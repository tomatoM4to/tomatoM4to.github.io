# 문제점
사실 이때까지 설명한 내용에는 아주 중대한 결함이 하나 있습니다.

실제로 hidden layer가 있나 없나 실제 결과값은 동일합니다. 실제로 실험을 해보면 hidden layer가 있든 없든 결과가 거의 똑같이 나올겁니다.

비교해 보겠습니다.

TODO: 실험 결과 추가

***

# Activation function
이러한 문제를 해결하기 위해 Activation function이라는 개념이 도입되었습니다. 개념은 간단합니다. h1값들을 계산한 다음 바로 진행하는것이 아니라 Activation function을 통과시킨 다음에 실제 사용은 Activation function을 통과한 값으로 사용하는 것입니다.

이러한 활성화 함수중 가장 유명한 함수는 sigmoid 함수입니다. sigmoid 함수는 다음과 같습니다.

```math
f(x) = \frac{1}{1 + e^{-x}}
```

이 시그모이드 함수 x에다가 이전에 계산했던 값을 집어 넣어 나온 결과값을 사용하게 됩니다.

이 sigmoid 함수를 사용하면 값이 어떻게 변환하는지는 그래프를 보면 됩니다.

TODO: 함수 그래프 그림 추가

어떤 값을 넣든 0과 1사이의 값으로 변환됩니다. 이러한 특성을 이용해서 hidden layer를 통과한 값들을 0과 1사이의 값으로 변환시키고 이를 다시 사용하게 됩니다.

이거 말고도 여러 활성화 함수가 있습니다. -1부터 1까지 나오는 함수도 있구요

요즘 가장 많이 사용하는 함수는 ReLU 함수입니다. ReLU 함수는 다음과 같습니다.

TODO: RELU 이미지 추가

```math
f(x) = max(0, x)
```

RELU 함수는 0보다 작은 값은 0으로 만들고 0보다 큰 값은 그대로 사용하는 함수입니다.

## 사용하는 이유
활성화 함수가 뭐인진 알겠지만 정확히 왜 이것때문에 위의 문제가 해결되는지는 아직 설명하지 않았습니다.

활성화 함수를 사용하는 이유를 간단히 말하면 **비선형성을 추가하기 위해서**입니다.

반대로 활성함수 없이 하는 에측은 전부 **선형적인 예측**이 됩니다. 선형적인 예측은 한계가 있습니다. 예를 들어서 **XOR 문제를 해결할 수 없습니다.**

비쥬얼 적으로 표현하면 이렇게 됩니다.

TODO: 이미지 추가

결과적으로 비 선형적으로 예측하면 더 복잡한 상황에도 예측이 가능할것 같습니다. 결과적으로 그냥 그래프를 꼬으라는 의미입니다.

이러한 Activation function은 모든 레이어에 있으면 좋지만 출력층엔 없을 수도 있습니다. 최종 결과를 0과 1 사이로 압출 시키고 싶다면 sigmoid 함수를 사용할 수 있습니다.