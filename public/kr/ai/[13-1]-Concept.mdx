쇼킹한것은 아무런 규칙도 알려주지 않고 같은코드에 게임만 변경함

기존 Q-learning: 할때마다 업데이트

Experience Replay: 하나씩 뽑을순 있는데 보통 여러개 미니배치 사이즈만큼 뽑음

무작위로 추출: 상호작용하는 순서와는 상관없이, 추출이니 없어지는게 아님

빨간줄: 논문에서 이렇게 말했다 함

***

Experience Replay: Example

빨간 글: 발산 문제가 완화, 이런 자료 없다,,

***

target: 차이가 없게 만들기 위한 목적, 근데 이전까진 목적이 계속 변함, 뭔가 이상하다, 그래서 업데이트 안함

Target Network: 안정된 target 반환

직접 해보기: 이런 자료 없다..

증가치가 더욱 감소: 아직 발사을 완전히 막진 못하지만  거의 막음

***

에타가 0? 에러 추출예기, 에타가 0이면 1/N 이니 일반적인 경험 리플레이

근데 높은확률로 뽑는다 해도 의미가 있나? 음.. 별로?

매개변수가 업데이트 되더라도, 해당 경험에 대한 Error가 극적으로 감소하지않기 때문에 한번 큰 에러는 반복적으로 추출됨

중요한 경험만 보고자 했지만 일반적인 경험리플레이봐 못할 가능성이 큼

솔루션: 주어진 경험이 추출될 확률을 고려하여 매개변수가 업데이트 되는 강도를 조정

# CNN
아타리 게임  같은 경우 특성값을 뽑거나 에이전트를 수정하지 않음, 같은 에이전트에 게임만 바꿈

cnn: dqn의 필수 네트웤, 이미지를 처리하기 위한 네트웤, 연속된 어쩌구를 위한 네트웤 프레임 앞두가 별로안달라지니까

dqn은 이미지 통으로 했음

# Convolution
w 가 같은게 매우 큰 특징, 연속된 친구끼리 가중치 고려, 뭔가 연관이 있지 않을까?

한칸씩 움직이면서 똑같은 w를 곱한것

가중치는 어떻게보면 하나의 집합, 필터 같은건데? 여러개 쓸수도 있음

# Convolution Layer
딥러닝은 용어가 별것도 없는데 이상한것들 많이 썼지만 보면 별거 없음

# pooling layer
용어를 왜 pool 로 쓸까.. 햇갈리게 -교수-

사이즈 압축: zip 같은 알고리즘이 아니라 필요한것만 추리는것, 수학적으로 추리고 반으로 줄어듬

이론으로 100% 이해하지 말고 나중에 직접 구현해보자(4학년때 함)

***

컴퓨터가 다해주는데 뭘

그리고 이런 인공신경망은 텐서플로우,파이토치가 어떻게보면 규격임

재밌는 이야기: 텐서플롱가 kepra?로 진화했는데 파이토치 스러움

프로그래밍 언어도 서로 장점 뽑고 수렴진화함

상태 행동에 대해 w를 곱하기 땜문에 이전 어쩌구까지 구해지는게 펑션 어프로치메이트의 큰 특징

***

# 문제 발생
이번 시간에는 강화학습에서 가장 중요한 알고리즘이라 할수 있는 DQN에 대해 알아보겠습니다.

지난 시간에 Q-table로 Q-learning를 구현하고, 이 Q-table의 한계 때문에 Network를 이용하여 Q-learning을 구현하는 방법을 알아보았습니다. 하지만 코드를 실행하면 알수 있듯 그 결과가 매우 형편없었습니다 단순한 미로나 막대기를 새우는 문제조차 제대로 풀지 못했습니다. 이번엔 왜 이런 문제가 발생했는지 또 해결 방법은 무엇인지 알아보겠습니다.

이전에 말했듯 $\hat{Q}$ 은 Converges 하지 않는단 이유를 가지고 있었습니다.
* Correlations between samples(샘플간의 상관관계)
* Non-stationary targets(움직이는 타겟)

이러한 이유 때문에 신명망을 사용하면 차이가 발생합니다. 이는 학습이 잘 돼지 않는다는 의미입니다.


사실 Q-learning과 NN을 결합시키려는 시도는 매우 오래전부터 있었습니다. 하지만 저 2가지 문제를 해결해 이를 성공적으로 구현한 것은 2013년 DeepMind의 연구팀이었습니다. 이들은 Q-learning을 이용하여 Atari 게임을 플레이하는 AI를 만들었는데, 이것이 바로 DQN입니다.

* DQN paper: https://nature.com/articles/nature14236
한번쯤 읽어보시길 추천드립니다.

***

# 문제점
기존의 문제가 뭘까요?

* Correlations between samples(샘플간의 상관관계)
* Non-stationary targets(움직이는 타겟)

이겁니다. 이를 해결하기 위해 DeepMind 팀은 간단한 솔류션을 제시했습니다.

한번 생각해 봅시다. 미로를 탈출한다고 했을때, 혹은 이전처럼 막대기를 새운다 했을때 0.1초전과 지금의 상태는 많이 다를까요? 음 그렇진 않거 같습니다. 상당히 유사한 상태일 것입니다. action을 취할시 환경이 급격히 바뀌는게 아닌 조금씩 바뀌기 때문에, 받아오는 데이터들 혹은 sample 들의 연관성이 유사하단 점이 바로 문젭니다.

TODO: Linear regression 그래프 예제 필요

두번째는 타켓이 움직인다 라는 겁니다.

```math
\min_{\theta} \sum_{t=0}^T [\hat{Q}(s_t, a_t|\theta) - (r_t + \gamma \max_{a'}\hat{Q}(s_{t+1}, a'|\theta))]^2
```

이전 공식에서 주의해서 보아야 할것이 두부분 있습니다.

```math
\hat{Q}(s_t, a_t|\theta)
```

```math
(r_t + \gamma \max_{a'}\hat{Q}(s_{t+1}, a'|\theta))
```

이 두 Q값이 서로 같은 신경망을 통해 계산되기 때문에, 이 두 Q값이 서로 영향을 주게 됩니다. 이는 Non-stationary targets 라는 문제를 발생시킵니다.

$\hat{Q}(s_t, a_t|\theta)$ 부분에서 다음 상태에 가까워 지기 위해 네트워크를 업데이트 시키고 그 다음 계산인 $(r_t + \gamma \max_{a'}\hat{Q}(s_{t+1}, a'|\theta))$ 부분에서는 업데이트된 네트워크를 사용하고 다시 업데이트를 시킵니다. 확실히 문제가 발생할 수 밖에 없습니다.

***

# DQN's three solutions
이 문제를 해결하기 위해 DeepMind 팀은 3가지 솔루션을 제시했습니다.

1. Go deep: 더 깊은 신경망을 사용한다.
2. Capture and replay: Correlations between samples(샘플간의 상관관계)를 해결하기 위해 sample을 저장하고 랜덤하게 뽑아서 학습한다.
3. Separate networks: Non-stationary targets(움직이는 타겟)를 해결하기 위해 2개의 네트워크를 사용한다.

## Go deep
시원시원한 생각이죠, 이전엔 하나의 layer만 사용한걸 늘리면 되겠다는 생각입니다. 이는 더 복잡한 문제를 풀 수 있게 해줍니다.

* End-to-end learning of values Q(S, a) from pixels s
* Input state s is stack of raw pixels from last 4 frames
* Output is Q(s, a) for 18 joystick/button positions
* Reward is change in score for that frame

TODO: 위 내용 이해 필요, 설명 없었음

## Capture and replay
굉장히 중유한 솔루션 입니다. 너무 연관있는 데이터들로 학습을 시키니 학습이 잘 돼지 않았던 문제를 해결하기 위해 experience replay라는 방법을 사용합니다.

Agent 에게 action을 취하게 하는 루프를 돌면 이 action을 통해 상태를 받아오게 돼는데 이전엔 바로바로 학습을 시켰습니다. 이를 바꿔숴 중간에 버퍼에 저장을 한다음, 이 버퍼가 다 차면 혹은 일정한 시간이 지나면 해당 버퍼의 담겨진 데이터중 랜덤으로 하나 뽑아서 학습을 시킵니다.\

TODO: 사진


TODO: 의사 코드 사진 필요

* **Store transitions (s, a, r, s') in replay memory D**: action을 취한다음 D 라는 버퍼에다가 상태, 액션, 보상, 다음 상태를 저장합니다.
* **Sample random minibatch of transitions (s, a, r, s') from D**: D에서 랜덤하게 샘플을 뽑아 mini batch를 만들어 이걸 가지고 학습을 진행합니다.

아주 간단한 아이디어지만 왜 될까요?

눈치 채신분들도 많겠지만 이렇게 해봅시다.

버퍼에 어느정도 데이터가 쌓어셔 이걸 그래프로 표현해보겠습니다.

이런 그래프가 나왔을때 기존엔 편향된 데이터가 학습이 될수 있었지만 이들중 몇개만 랜덤으로 뽑아 학습을 하게 되면 우리가 원했던 기울기의 그래프가 나올 가능성이 훨씬 크지 않을까요? 아주 심플한 아이디어지만 실제로 매우 잘 작동합니다.

## Separate networks
이또한 매우 심플한 아이디어죠, 사용하는 NN가 같아 Non-stationary targets 문제가 발생했는데, 이를 해결하기 위해 2개의 네트워크를 사용합니다.

공식에 익숙해 졌을거라 생각하니 바로 공식부터 보여드리면 이렇습니다.

```math
\min_{\theta} \sum_{t=0}^T [\hat{Q}(s_t, a_t|\theta) - (r_t + \gamma \max_{a'}\hat{Q}(s_{t+1}, a'|\bar{\theta}))]^2
```

양쪽 Q의 $\theta$ 가 다르다는것을 바로 눈치채실수 있을겁니다. 이는 서로 다른 NN을 사용한다는 의밉니다.

이때 $\hat{Q}(s_t, a_t|\theta)$ 에서 사용돼는 $\theta$ 를 policy network라고 부르고, $(r_t + \gamma \max_{a'}\hat{Q}(s_{t+1}, a'|\bar{\theta}))$ 에서 사용되는 $\bar{\theta}$ 를 target network라고 부릅니다.

중요한점은 학습을 시킬때는 policy network 많을 업데이트 시킵니다. target network는 움직이지 않고 있다가 policy network가 일정한 시간이 지나거나 일정한 횟수만큼 업데이트 될때마다 policy network의 weight를 target network로 복사합니다.

TODO: 의사코드

정말 심플하지만 이또한 정말 잘 작동합니다.

***

# 요약
TODO: 의사코드와 요약