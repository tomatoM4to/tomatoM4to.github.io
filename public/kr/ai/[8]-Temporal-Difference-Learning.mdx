가장 많이 쓰이고 아마 실습때도 이걸 쓸수도 있다.

TPD가 몬테카를로보다 훨씬 많이 사용됀다. 몬테카를로는 극복할수 없는 한계가 있음

현실의 문제를 해결할때는 다 여거쓴다 생각해도 ok

DP 에는 환경의 모델을 정확히 알수 없을때 문제가 있음

그리고 한번 연산할때마다 모든 어쩌구를 다 업데이트 해야함

그래서 몬테카를로가 도입됌, 근데 Gt를 알수 있다는건 에피소드가 끝나야 가치함수가 업데이트 돼는데, 근데 에피소드가 미친듯이 길다면?

이러면 별거아닌 가치함수로 계속 이어짐

그리고 종료상태가 따로 없는 문제도 많음, 게임은 종료가 있지만 범용로봇 같은 경우는 끝이 없기에몬테카를로를 못씀

그리고 몬테카를로는 Increase... 어쩌구가 그냥 없음

***

# Value Function
TD 러닝의 가장 핵시민 부분임 마지막 줄을  가치고 업데이트함

# TDP
중요한점은 Gt를 기다릴 필요가 없기 때문에 안기다림, 다음 상태로 가자마자 가치함수?를 업데이트 할 수 있음

찝찝한 부분은 이전 몬테카를로의 Gt는 실제 값이지만 TDP에서의 V파이는 추정값임, 즉 참이 아님 근데 이게 Gt랑 똑같다고? 모 궁극적으론 똑같을수 있어도.. 학습 도중에도..?

추정치가지고 또다른 추정을 하는거임, 직관적으로 말이 안됌

## Convergence Guarantee
증명이 됐음, 놀랍게도..

그렇다면 MC Prediction과 TD Prediction중 어느것이 더 빠리 수렴할까? 한쪽은 실제 값으로 추정? 한쪽은 추정으로 추정

엄밀히 증명되진 않았는데 TDP가 더 빠름

***

# TDP VS MCP

## TDP
상태 B에 대한 가치함수의 추정치는 얼마일까?

* V(B): 6/8 = 0.75
* V(A) = [0 + V(B) / 1] = 0.75

TDP는 마르코프 성질을 따르면(다음상태가 오직 현상태와 연간돼어 있다면) 아주 잘 작동함

## MCP
확률분호 다 필요없고 한줄한줄 존재하는? 데이터에 맞도록 학습함

***
이상 Prediction은 설명 마침, 간단

***

# SARSA
이전 페이지에서 설명한것은 그-대로 가져와서 사용하는것

# Q-learning
무지성으로 max값을 고름, 결국 SARSA든 이거든 행동 하나를 선택한건데 선택하는 방식이 다름

근데 사람들이 생각함 행동 하나를  선택하는게 아니라 아예 전수탐색 비슷하게 해서 젤 ㄱㅊ은걸 고름  이것이 바로 `Expected Sarsa` 임

# Expected SARSA
연산은 많아지지면 하나를 선택한것보다 분산이 잘라짐, 그리고 스탭사이즈 알파가 크더라도 좋은 성능을 보장, 왜냐면 전수탐색 하니까

# Backup Diagrams
Q-learning에서 이상한 부채꼴? 모양은 이중에서 max 고른다는 의미임

Expected sarsa는 전수탐색을 의미하고

***

다음주 월요일에도 수업하긴 함, 이대 팀과제 공지(2주후 시험)
