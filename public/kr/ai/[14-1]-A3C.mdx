DQN 에선 행동을 0, 1, 2, 3, ...

해놨는데 이건 0.0 ~ 1.0

그냥 통짜로 액션 선택

# REINFORCE
서튼이란 사람이 만든 알고리즘

목적: 에피소드 내에서 얻을 수 있는 ...

총 보상을 고려? 언제 총 보상을 더하야 하나? 이전까지 그런것 한적 없음

그래서 $G_t$ 로 바꾸기 위해 시그마 안쪽으로 넣어줌, t가 없어서 그냥 바꾸지 못함, 근데 게임 같은건 플레이 하면 되니까 타입스탭 같은거 신경 안써도 되서 더 효율적으로 바꿀수 있음


몬테카를로는 에피소드가 끝나야 학습을 할 수 있음

# Variance vs bias
강화학습 기준으로 설명

몬테카를로는 우측위라 할수 있음, Q-learning 같은 템포럴 어저구는 좌측 아래

둘중 어느게 좋나? 그럴수 없음, 트레이드 오프 관계임 한쪽이 좋아지면 한쪽이 나빠짐

# Speed and Direction in Update Rule

상태에 의존적이라면 $b(S_t)$ 는 만족

# 공식

첫번째 줄 왼쪽에서 *1 트릭 사용, 모든 확률 1 로 변횐


빠르게 메게변수 세타가 학습됨: 이 식이 매우 중요함

gt - 어쩌구 이게 중요 -: 상태 가치 함수!

상태에만 의존적인 대표적인 함수: 상태 가치 함수


***

adventure function? 아 그냥 Q-V 구나~

Cov2d 여러개를 통해뭔가 좋은값이 됐으면, 이걸 다른 또하나의 네트워크에 연결될텐데 이걸 2개 연결하잔 아이디어임, Q 네트웤 연결, W 네트웤 연결


Conv2d 에 이미지를 넣으면 매개변수가 바뀜, 좋은 결과를 내놓기 위해 바뀜

어드벤티지 펑션 안쓰는데 왜 이름을 이렇게 쓰지?

어싱크 방식은 A3C 방식에만 쓰는게 아닌 Q-learning 에서도 Sarsa 도 됨, 애초에 논문 이름도 A3C 가 아님

이 어싱크 방식이기 때문에 생기는 재밌는 현상이 있음, 이 여러 Agent 는 멀티프로세싱이나 멀티스레딩 함

어싱크 방식이면,어떤 agent는 빨리 끝날수 있음, 이때 모아놓은 Gradient를 가직 업데이트 시킴?

어싱크의 의미: 서로 Agent의 매개변수가 다름, 매개변수를 가지고 정책을 근사하기 때문에 서로의 정책이 다른거임, 물론 당연히 근사는 있겠지만

서로 다른 정책으로 환경과 상호작용하기 때문에 다양한 경험들이 생성됨, 탐험의 강화

기본적으로 Poilicy Gradent는 입실론 그리디를 쓰지 않기 때문에 탐험이 안될수 있음

Replay memory를 쓰지 않기 때문에 메모리도 적게듬

기존 Agent 하나만 쓸땐 하나에 의존성이 크지만 여러개로 업데이트 하니 시간적 의존성이 완화됨, 학습 속도 증가

탐험이 강화되니 최적을 더 잘찾고 학습 속도도 더 빠름, 더 중요한건 A3C 에서만 쓸수 있는게 아님, Agent를 비동기적으로 실행하는것 뿐

업데이트 공식에 감마가 빠짐, A3C 에선 시간 개념이 없음, 예를들어 타임스탭이 20이면,Gradient 20개 쌓이면 보내 이거임, 그럼 예는 21부터 시작, 그냥 중간중간마다 보내는거임, 에피소드 끝나고 업데이트가 아님

근데 사라믈이 A3C 때문에 싸움, 매개변수 업데이트 하는 방식이 이상하지만 탐험이 좋음

어싱크가 아닌 A2C 아이디어가 생김, 그냥 동기방식일 뿐, 탐험이 부족해도 매개변수 업데이트 방식이 괜찮음

그냥 각 Agent가 전부 끝나야 메인이 업데이트 됨

이 단계 다음부턴 고인물 싸움임, 복잡해짐 이 이후에 나온 알고리즘이매우 많이 나옴

수학보단 코딩으로 배우는게 훨씬 쉬움;; 교수님도 1판 처음 볼때 벨만 이콰입션에서 그냥 덮고 코딩부터 함

교과서 1000페이지 분량의 학습임 이게..

수학공식은 중요하기 보단 익숙해지게