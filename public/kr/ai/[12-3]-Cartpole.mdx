https://dryjelly.tistory.com/139

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym

# 환경 설정
env = gym.make('CartPole-v0')

# 하이퍼파라미터
learning_rate = 1e-1
input_size = env.observation_space.shape[0]
output_size = env.action_space.n
max_episodes = 2000
dis = 0.9

# Q-Network 모델 정의
class QNetwork(nn.Module):
    def __init__(self, input_size, output_size):
        super(QNetwork, self).__init__()
        self.fc = nn.Linear(input_size, output_size)
        nn.init.xavier_uniform_(self.fc.weight)

    def forward(self, x):
        return self.fc(x)

# 모델, 옵티마이저 초기화
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = QNetwork(input_size, output_size).to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.MSELoss()

step_history = []

for episode in range(max_episodes):
    e = 1. / ((episode / 10) + 1)
    step_count = 0
    state, _ = env.reset()
    done = truncated = False

    while not (done or truncated):
        step_count += 1
        x = torch.FloatTensor(state).unsqueeze(0).to(device)

        # Q값 예측
        with torch.no_grad():
            Q = model(x)

        # e-greedy 정책
        if np.random.rand(1) < e:
            action = env.action_space.sample()
        else:
            action = Q.argmax().item()

        # 환경과 상호작용
        next_state, reward, done, truncated, _ = env.step(action)

        # Q 값 업데이트
        Q_target = Q.clone().detach()
        if done:
            Q_target[0][action] = -100
        else:
            x_next = torch.FloatTensor(next_state).unsqueeze(0).to(device)
            with torch.no_grad():
                Q_next = model(x_next)
            Q_target[0][action] = reward + dis * Q_next.max().item()

        # 모델 학습
        optimizer.zero_grad()
        Q_pred = model(x)
        loss = criterion(Q_pred, Q_target)
        loss.backward()
        optimizer.step()

        state = next_state

    step_history.append(step_count)
    if episode % 100 == 0:
        print(f"Episode: {episode}  steps: {step_count}")

    if len(step_history) > 10 and np.mean(step_history[-10:]) > 500:
        break

# 학습된 모델 테스트
observation = env.reset()[0]
reward_sum = 0
while True:
    x = torch.FloatTensor(observation).unsqueeze(0).to(device)
    with torch.no_grad():
        Q = model(x)
    action = Q.argmax().item()

    observation, reward, done, _, _ = env.step(action)
    reward_sum += reward
    if done:
        print(f"Total score: {reward_sum}")
        break
```