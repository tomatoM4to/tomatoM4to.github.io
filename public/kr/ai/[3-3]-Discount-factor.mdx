## Discount factor
위 예제에서 어느정도 구현이 됐었습니다. Discount factor은 결국 더 효율적이게 path를 찾기 위해 고안된 방법입니다. 코드가 아니라 아까 배웠던 이론을 생각해 봅시다. 우린 이때까지 다음상태의 최대값을 현재상태에 업데이트 했습니다. 근데 이건 뭔가 이상합니다. 상하좌우로 움직일때 더 좋고 안좋고가 없이 무조건 같은 값으로 업데이트를 한다고 이론적으로 학습했습니다. Discount factor은 이러한 문제를 해결하기 위해 고안된 방법입니다.

Discount factor 은 기호로 $\gamma$로 표현합니다. 이는 0과 1사이의 값을 가집니다. 이는 상태를 업데이트 할때 다음 상태의 최대값 $\gamma$을 곱해줍니다. 이는 다음 상태의 최대값을 현재 상태에 업데이트 할때 그 값이 얼마나 중요한지를 나타냅니다. 이를 통해 더 효율적인 path를 찾을수 있습니다.

처음엔 그대로 복사를 합니다. 하지만 그 다음 업데이트 할때는 $\gamma$를 곱해 업데이트 해라! 라는 겁니다.

한번 위 4x4 예제로 살펴보겠습니다.
|  |  |  |  |
|---|---|---|---|
| [0, $R\gamma^4$, 0, $R\gamma^2$] | [0, 0, 0, $R\gamma$] | [0, 0, 0, $R$] | $R$ |
| [0, 0, 0, $R\gamma^3$] | [0, 0, 0, $R\gamma^2$] | [0, 0, 0, $R\gamma$] | [$R$, 0, 0, 0] |
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] |
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] |

결과적으로 어떻게 될까요? 첫 시작점은 (0, 0)에서 가장 큰값인 $R\gamma^2$ 값을 가진 방향으로 나아가 결국 최적의 경로를 찾게 될것입니다.

### Discount factor 값에 따른 결과
한번 $\gamma$값이 작다고 가정해 보겠습니다. 그렇다면 위 그림에서 $R\gamma$으로 가야 $R$ 상태에 도달하지만, $R\gamma$이 작다면 $R$으로 가는것이 어려울수 있습니다. 하지만 $R$상태에선 아! 이방향으로 가면 보상이 기다리는구나! 라면서 확실히 이동할겁니다. 지금 당장 $R\gamma$ 로 가도.. 보상은 없겠지.. 한번더 움직여야 하잖아? 라고 생각하는것과 비슷합니다. 이걸 흔히 현재지향적인 행동이라고 합니다. 미래에 큰 보상이 있더라도 지금 당장 현재를 더 중요하게 생각하는겁니다.

반대로 $\gamma$값이 크다면? $R\gamma$으로 가는것을 더 중요하게 생각할겁니다. 현실로 대입해보면 더 가야할순 있을거 같은데 그래도 거의 다왔다! 라고 생각하고 그 방향으로 가는것과 비슷합니다. 호들갑을 떨면서 가는것이라고 생각하면 됩니다. 이러한 행동을 미래지향적인 행동이라고 합니다. 지금 현재 받을 보상보다 미래에 받을 보상을 더 중요하게 생각하는 효과가 생깁니다.