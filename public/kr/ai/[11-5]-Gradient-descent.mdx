# 경사하강법

결론적으로 w값을 찾아야 하는데 그냥 때려맞추면 안될거 같습니다. 최적의 w값을 찾기 위한 방식인 경사하강법에 대해 알아보겠습니다.

이전까지 공부한 내용은 NN을 타고 예측값을 뽑아내는 것까지 했습니다. 이번엔 이렇게 예측된 값과 실제 값의 차이를 줄이기 위해 w값을 조정하는 방법에 대해 알아보겠습니다.

아주 간단히 생각하기 위해 지금은 w1 하나만 생각해 보겠습니다. w1엔 초기엔 랜덤한 숫자가 들어가 있을 겁니다. 이제 이 w값을 0부터 100까지 한번씩 대입을 해봤다 가정해 보겠습니다. w값에 의한 총손실 E에 대한 그래프 입니다.

TODO: 이미지

w값이 변할때마다 E값이 변하는것을 볼수 있을겁니다. 그렇다면 어느 w값을 찾아야 할까요? 좌측의 기울기가 0인 부분 저부분이 최하단인거 같습니다. 그래서 저 부분을 찾아야 할것 같습니다. 그렇다면 이걸 어떻게 찾을까요?

처음값은 랜덤하게 시작한가 했습니다. 그렇기 때문에 초기 값은 그냥 아무데나 점을 찍은 부분이라 할수 있을겁니다. 예를들어 3이라 가정해 보겠습니다. 그렇다면 이 예시에선 2로 가야 할까요 4로 가야 할까요? 딱봐도 2로 조정해야 할겁니다. 하지만 이 예시는 어디까지노 공부를 위한 예시지 실제 그래프가 존재하진 않을겁니다. 그래서 이걸 어떻게 찾을까요?

이때 Gradient Descent라는 방법을 사용합니다. 이 방법은 현재 w값에서 왼쪽으로 이동할지 오른쪽으로 이동할지에 대한 판단 기준이라 봐도 무방합니다. 경사를 타가 하강하면 됩니다.

이 방법은 <R>현재 w값에서의 접선의 기울기를 w에서 빼주면 됩니다.</R>

예를들어 현재 예제에서 3일때의 기울기가 1이라 해보겠습니다. 이땐 3 - 1 해서 2로 이동하게 됩니다.

그렇다면 만약 기울기가 음수일 경우는 어떻게 될까요? 0.5로 가정해보겠습니다. 이번에도 기울기가 -1입니다. 이땐 0.5 - -1 해서 1.5로 이동하게 됩니다. 결과적으로 w값이 오른쪽으로 움질일 겁니다.

식으로 표현하자면 이렇게 표현 할수 있습니다.
```math
w = w - \frac{\partial E}{\partial w}
```

오른쪽 수식은 면미분 기호인데 이를 문장으로 표현하면 **w가 총손실 E에 얼마나 큰 영향을 미치는가**로 표현할수 있습니다. 이렇게 새로은 w값이 수정돼게 돼서 새로은 딥러능 NN이 만들어 지게 될겁니다. 이것을 loss가 줄어들지 않을때까지 반복하시면 됩니다.

지금까지 배운 딥러닝의 개념을 살펴보겠습니다.

기계에게 학습을 시킨다 = 손실을 최소화 하는 w값을 찾게 시킨다 = 그 방법은 경사 하강법을 사용한다.

## 문제점
하지만 이러한 그래프가 그려진다 가정해 보겠습니다.

TODO: 그래프

만약 최소의 loss값이 아닌데 기울기가 0이라면 어떻게 될까요? 이때는 더이상 학습이 이루어 지지 않을수 있습니다. 이러면 학습이 제대로 이루어 질수 없습니다. 이런 문제를 해결하기 위해 **학습률**이라는 개념이 도입되었습니다.

이러한 에러를 trial and error 라고도 합니다.

이전 Q-learning에서 배웠듯 learning rate를 곱해 빼주면 됩니다.

```math
w = w - \alpha \frac{\partial E}{\partial w}
```

이전과 마찬가지로 정해진건 없고 실험적으로 찾아야 합니다.

learning late를 통해 w값을 업데이트 하면 빼줄때 더 크게 뿌재던가 더 작게 빼주게 됩니다. 이렇게 하면 지금 가고있는 최저점을 넘어 실제 최저점에 가까워 질수도 있게 됩니다.

이러한 learning rate를 일정한 숫자로만 주게 되면 복잡한 문제의 경우 학습이 안일어나는 경우도 있습니다. 그래서 학습이 일어 날때마다 가변적으로 최적화 해주는 수학 알고리즘이 몇개 있습니다.

전문용어로 learning rate optimizer라고 합니다.

이러한 것들이 있습니다.

* Momentum: 이전에 w값이 많이 뛰었다면 이번에도 많이 뛰게 하는 방법, 가속도 같은 개념입니다.
* Adagrad: 자주 변하는 w는 작게 변하게 하는 방법, 자주 변하면 크게
* RMSprop: Adagrad 의 제곱
* AdaDelta: AdaGrad 인데 너무 작아져서 학습이 안일어나는 문제를 해결한 방법
* Adam: RMSprop + Momentum

정해진건 없고 실험적으로 사용해야 합니다. 만약 모르겠다면 그냥 Adam을 사용하는것도 방법입니다.

지금까지 2차원 그래프를 살펴봤지만 사실 실제 w값들은 수백개, 수백만개이기 때문에 그래프도 수백만차원이 될겁니다. 이걸 일일이 그래프를 그릴순 없겠죠, 애초에 상상하는것도 인간의 범주를 벋어날겁니다. 그래서 경사하강법이라 했지만 실상은 **w변할때 E는 얼마나 변하는가** 를 계산한 방법이 경사하강법입니다.

***


# 경사하강법
이론상 기울기가 0까지 가겠지만 가끔 발산할수도 있음

**미분 가능해야함** 경사 하가법 안쓰고 머신러닝을 구현할수도 있음?

$w_0$ 이건 더미상태임, 더미지만 이또한 상태로볼수 있음, 수학적으로 표현하면 $w_1 * 1$

$w_0 * 1 + w_1 * s_1$ 이건 1차원

그리디 어드벤처에서 특정좌표를 특정지어도 됨, 이걸 특성값이라 부름, ㅅㅂ  상태 그대로 쓰는 경우는 거의 없고 상황에 맞게 수정한다네

***

# 테이블 vs 펑션
그래서 함수가 더 좋은가? 테이블이 더 좋음, 하지만 할수가 없어서 문제지

# Polynomials
기존의 $x_0 * 1 + x_1 * 1$ 이건 서로 독립인데 가끔 서로 독립이 아니여야하는 경우도 있음 $x_0 * 1 + x_1 * 1 + w_s * s$??

2차원을 9차원으로 바꾸든 지지고 볶든 ㄱㅊ

푸리에 변환: 모든 선형 함수는 sin, cos 같은 주기함수들의 조합이다?

## Corse Coding
생각보다 효율적, 원이 아니라 삼각형으로 하든 맘대로

1또는 0이라 하는 이진 특성값은 계산이 효율적, 자돼든 말든

## Tile Coding
Corse Coding의 확장판

0도부터90도를 1도부터 쭉 나누는 개념이랑 똑같은 개념임

간단한데 매우 좋다, 하지만 잘 알려진 문제에 한해서, 아타리 게임같은 엄청 복잡한 문제에선 좋다라고  할순 없음

도메인지식: 있어보이게 말했지만 ㅅ실 이렇게 하면 될거 같은데? 같은 직관임

특성값이 중요, 사실 기계학습 알고리즘 별로 안중요하고 특성값이 중요함, 이미지 인식? 딥러닝 안서도 잘 됨

