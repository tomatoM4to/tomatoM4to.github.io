# 학습 방법
다이나믹 프로그래밍 =! 강화학습이란걸 알아야함, 따로따로 발발전하다 나나중에 합친 사람이 대단한거지

프리딕션 == 애벌루에이션, 똑같은의미임

S0 -> A0: 여기서파이(S0) 수행 후-> R0

애피소드라는걸 다 알수 있단거는, 지금까지 아는대 한에선수익을 전부 알수있다

# First visitVS. Every visit
미로찾기에서 만약내가 왔던길을 다시 가거 다시 반복하는 x1 -> x2 -> x1 -> x2..

이런식으로 됄때 어느걸 선택할가에 대한 것

***

몬테 카를로는 수렴할때까지 하지 않않음, 그래서 의사코드에서 while 빼고 그냥 마노이 돌돌려도됌

환환경이 없기 때문문에 강화학습에선 행동 가치함수만 사용하게 됀다.

몬테카를로에선 우리가 아는 부분에서만 가능하고, 모르는애들한테는 모모름, 환경을 아예 모르기 때문에

모르는 애들들한테 어떤 정책? 행행동을 수행할지지에 대해선 모험과 착취가 있겠는데

입실론 Greedy라고더 괜찮은 방법이 있음

몬테카를로에선 현재의 정책이 과거의 정책보다 더 좋다는걸 알수 이어야한다.

입실론 그리디를 했을때, 과거의 정책볻 현재가 더 좋다는걸 증명, 알수 있을까?

결국 최적의 정책으 찾을수 있는지?

시시그마 파이((a'|s) -e/||A|)/1-e= 1

온폴리시 몬테카카를로의 문제는 e/|A| 확률로만 탐험을 하게 됀다, 한마디로 자기가 아는 조그만 지식 내에서 몬갈 하는데 결국 탐험이 부족함

Off Policy가 강화학습에서 대부분 사용됌, 근데 좀 느리긴 함

예를들어 과제기간 3일남았는데 Off policy 쓰면 과제 못함, 학습시간이 3일, 2일이면 On policy 써야 함

DP랑 강화학습은 사실 완전히 다른 방향으로 발전했고 일부분 비슷한게 있어서 배운것 뿐이지 사실 그그냥  다른거임



***

# Off-Policy Monye-Calo Methods
최적의 정책을 찾는 문제의 딜레마가 있음, 최적의 행동에 대한 가치함수를 학습을 해야함, V*를 알아야 함, 근데 이걸 하려면 최적이 아닌 행동을 해야함.

On-Policy는 그래서 최적의 행동에 대한 가치 함수를 학습하는 대신..(ppt 내용용

Behavior Policy는 에피소드를 다음과 같이 생성: 배울 필요 없음, 그냥 아무거나 막 생성함 다 같은확률로

감마(s) 는 현재 시간에 대한 타임스탬프/ V파이(s) 가 아니라 사실 Vb(s) 다?

핵심: 파랑줄

***

## Importance Sampling
Gt: 랜덤 긷댓값, ㄱ냥 랜덤값?

## Estimtor
사실 이러한 과저이 그렇게 간단하진 않음

T(t) 에피소드가 언제끝났냐에대한 예기임

공식상으론 맞는데 분산이 커지는걸 막아야함 -> 분모를 증가시켜야함

[다음 ppt장]그래서 Off Policy 몬테카를를로 방식을 살거면 무지성으로 이걸 쓰면 잘 됀다.


왜 지금까지 V파이를 계산했나요 Q파이를 계계산하지, 더 짧으니까.. 비슷하게 생겼으니까.. V를 Q로, a 추가가하면 끝이니까

## Peudocode

## Importance Sampling
감마: 할인율을 고려 안한 문문제가 발생해서 고려해야함

여기서 수식중에 - 여야 하는게 +로 써져있는 오류가 있음

오더네리 펄 디시션 어쩌구[마지막 ppt 내용용

증명명돼지 않고.. 그냥 잘 돼더라

옾폴리시 몬테 카를를로는 아직연구가  많이 필요하다 한다.

실제론 MC 보단 TD를 많이 씀, 아마 과제도 TD를 할할거임

