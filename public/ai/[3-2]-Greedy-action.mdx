하지만 이러한 문제가 있습니다. 이렇게 경로를 찾아나가는 과정을 거치다 보면, 도착은 할수 있겠지만 이 경로가 최적의 경로가 아닐 확률이 매우 높습니다. 또한 최적이 아닌 경로를 계속 따라가게 됩니다. 해당 코드의 count 변수의 값을 보면 경로를 찾은 뒤에도 계속 같은 경로로 이동하는 모습을 볼수 있습니다. 이러한 문제를 해결하기 위해 Q-learning에선 $\epsilon$-greedy action을 사용합니다.

# $\epsilon$-greedy action
greedy 방식에선 같은 경로를 계속 따라가게 되는 문제가 있습니다. 이를 해결하기 위해 탐험(Exploration) 이라는 개념을 도입한 방법입니다. 탐험이란 간단히 말해, 더 좋은길이 있을지도 모르니까 가보는 것입니다. 이제 탐험을 하기 위해 $\epsilon$-greedy action을 배워봅시다.

$\epsilon$이 무엇이냐면 0과 1 사이의 값을 의미합니다. 대충 0.1이라 가정해 보겠습니다. 이때 $\epsilon$-greedy action은 90%의 확률로 가장 큰 값을 가진 방향으로 이동하고, 10%의 확률로 랜덤하게 이동하는 것입니다. 이렇게 하면 랜덤하게 이동하면서 더 좋은 길이 있을지도 모르니까 가보는 것입니다.

만약 $\epsilon$ 이 1이라면? 100%의 확률로 랜덤하게 이동하게 됩니다. 이렇게 돼면, 기존에 찾았던 경로를 기반으로 더 좋은길을 유도 해줘야 하는데 유도해주지 못하는 상황이 발생합니다. 이렇듯 너무 높은 $\epsilon$값은 문제가 될수 있습니다. 반대로 너무 낮은 $\epsilon$값은 탐험을 하지 않아 최적의 경로를 찾지 못할수 있습니다. 이러한 $\epsilon$값을 조절하는 것이 중요합니다.

$\epsilon$ 을 이해했으니 이제 관련 용어를 알아보겠습니다. Exploration과 Exploitation입니다. Exploration은 탐험을 의미하고, Exploitation은 현재 가장 좋은 방향을 선택하는 것을 의미합니다. Exploitation은 Q값을 이용해 그리디하게 움직인단 의미이기도 합니다.

역시 Exploration과 Exploitation은은 trade-off 관계입니다. Exploration을 많이 하면 더 좋은 길을 찾을수 있지만, 그만큼 시간이 오래 걸립니다. 반대로 Exploitation을 많이 하면 빠르게 최적의 경로를 찾을수 있지만, 그만큼 더 좋은 길을 찾지 못할수 있습니다. 이러한 trade-off 관계를 잘 조절하는 것이 중요합니다.

Exploration의 두가지 장점
* 새로운 path 찾기 가능
* 새로운 goal 찾기 가능
    * 강화학습에선 map의 상태를 알지 못한다 했습니다. 기존엔 간단한 예시를 위해 한곳에 goal을 두었지만 실제로는 여러곳에 더 좋은 goal이 있을수 있습니다. 이때 탐험을 통해 새로운 goal을 찾을수 있습니다.

하지만 너무 탐험만 한다면 판때기를 이용하지 못합니다.

## Decaying $\epsilon$-greedy action
한번 생각해 봅시다. 그 어떤 정보도 없다면, 탐험을 하는게 좋을까요 아니면 Exploitation을 하는게 좋을까요? 보통 탐험을 하는것이 좋습니다. 그러나 탐험을 너무 많이 한다면, 최적의 경로를 찾지 못할수 있습니다. 하지만 이 두개는 trade-off 관계이디고 합니다. 이 전에는 $\epsilon$을 고정된 값으로 사용했지만, 이번에는 $\epsilon$을 점점 줄여가는 방법을 사용해보겠습니다. 이를 Decaying $\epsilon$이라고 합니다. 단순히 말해 처음엔 탐험을 많이 하다 나중엔 최적화에 더 많은 비중을 두는 것입니다.

<R>이떄 줄여나가는 기준은 episode를 기준으로 합니다.</R> 이 방식은 Q-learning에서 많이 사용되는 방법입니다. 그러므로써 trade-off 관계를 잘 조절할수 있습니다.

### 구현
```python
import random
import copy

def decaying_epsilon_greedy_q_learning():
    # 6x6 맵 초기화
    map = [[[0, 0, 0, 0] for _ in range(6)] for _ in range(6)]

    # 두 개의 도착 지점 설정 (첫 번째: [3,3]=10, 두 번째: [5,5]=20)
    map[3][3] = [10, 10, 10, 10]  # 첫 번째 도착지점
    map[5][5] = [20, 20, 20, 20]  # 두 번째 도착지점 (더 큰 보상)

    # Epsilon 파라미터
    epsilon_start = 0.9
    epsilon_end = 0.01
    epsilon_decay = 0.995

    count = 0
    for episode in range(1000):
        state = [0, 0]  # 시작 위치
        cp_map = copy.deepcopy(map)

        # 현재 에피소드의 입실론 값 계산
        epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))

        # 도착지점에 도달할 때까지 반복
        while state != [3,3] and state != [5,5]:
            loc = map[state[0]][state[1]]

            # Epsilon-greedy 행동 선택
            if random.random() < epsilon:
                action = random.randint(0, 3)
            else:
                action = loc.index(max(loc))

            prev_state = copy.deepcopy(state)

            # 행동 실행
            if action == 0 and state[0] > 0:    # 상
                state[0] -= 1
            elif action == 1 and state[0] < 5:  # 하 (범위를 5로 변경)
                state[0] += 1
            elif action == 2 and state[1] > 0:  # 좌
                state[1] -= 1
            elif action == 3 and state[1] < 5:  # 우 (범위를 5로 변경)
                state[1] += 1

            # 경계 처리
            state[0] = max(0, min(state[0], 5))
            state[1] = max(0, min(state[1], 5))

            # Q값 업데이트
            next_q_values = map[state[0]][state[1]]
            if max(next_q_values) > 0:
                map[prev_state[0]][prev_state[1]][action] = max(next_q_values) - 1

            # 도착지점 도달시
            if state == [3, 3]:
                map[prev_state[0]][prev_state[1]][action] = 9  # 첫 번째 도착지점 직전
                break
            elif state == [5, 5]:
                map[prev_state[0]][prev_state[1]][action] = 19  # 두 번째 도착지점 직전
                break

        # 맵이 변화하지 않을시 카운트
        if map == cp_map:
            count += 1

    return map, count

# 실행
q_map, count = decaying_epsilon_greedy_q_learning()

# 결과 출력
print("Final Q-values:")
for row in q_map:
    print(row)
print("\nCount:", count)

# 최적 경로 출력
def print_optimal_path():
    state = [0, 0]
    path = [(0, 0)]

    # 두 도착지점 중 하나에 도달할 때까지
    while state != [3,3] and state != [5,5]:
        loc = q_map[state[0]][state[1]]
        action = loc.index(max(loc))

        if action == 0:    # 상
            state[0] -= 1
        elif action == 1:  # 하
            state[0] += 1
        elif action == 2:  # 좌
            state[1] -= 1
        elif action == 3:  # 우
            state[1] += 1

        state[0] = max(0, min(state[0], 5))
        state[1] = max(0, min(state[1], 5))
        path.append((state[0], state[1]))

    print("\nOptimal path:", path)
    print("Final destination:", state)

print_optimal_path()
```
이전관 다르게 6x6으로 확장되었고, 두개의 도착지점이 추가되었습니다. 이때 첫번째 도착지점은 10점, 두번째 도착지점은 20점을 가지고 있습니다. 이때 두번째 도착지점이 더 높은 점수를 가지고 있습니다. 그리고 경로를 추적하는 코드가 추가되었습니다.