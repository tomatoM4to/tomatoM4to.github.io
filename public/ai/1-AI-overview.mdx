# Reinforcement Learning
배고픈 고양이를 버튼을 눌러야 문이 열리는 특별한 상자에 가두고, 상자 밖에는 먹이를 두었다고 사정해 봅시다. 만약 고양이가 우연히 버튼을 누르게 되면 상자 밖으로 빠져나와 사료를 먹을수 있을겁니다. 이를 반복하게 되면, 고양이는 <R>버튼을 누르는 것이 긍정적인 결과를 낸다</R> 를 학습하고 점점 더 상자에서 빠져나오는 시간이 짧아지게 될겁니다. 이것은, 버튼을 누르는 행동과 긍적적인 결과간의 관계가 강화(Reinforcement)되었다 라고 할수 있습니다. 이것이 바로 강화학습의 기본적인 개념입니다.

이를 위에서 배웠던 용어로 다시 정리해 보겠습니다.
* **에이전트(Agent)**: 고양이
* **환경(Environment)**: 고양이를 제외한 모든것
* **상태(State)**: 고양이가 상자 안에 있는 상황, 상자 밖에 있는 상황 등 현재 환경의 상태
* **행동(Action)**: 버튼을 누르거나, 움직이거나 하는 등 에이전트가 선택할 수 있는 모든 행동들
* **정책(Policy)**: 특정 상태에서 어떤 행동을 선택할지 결정하는 전략 (예: 배고픈 상태에서 버튼을 누르는 행동을 선택)
* **보상(Reward)**: 행동의 결과로 얻는 피드백 (예: 먹이를 먹었을 때의 만족감)

추가적으로 강화학습의의 중요한 핵심적인 특징중 하나는, **버튼을 누르면 왜 문이 열리는가?** 와 같은 원리를 이해할 필요가 없습니다. 대신 **이 상황(상태)에서 이 행동을 했더니 좋은 결과가 났다**라는 사실만 알면 됩니다.


아래 예시로 보여드릴 딥바인드에서 만든 게임 AI는 화면, 점수, 오른쪽 왼쪽밖에 모른다고 합니다. 이 AI는 화면에서 어떤 행동을 해야 좋은 결과를 얻는지를 학습하게 됩니다. 아래 예시에선 왼쪽, 오른쪽 밖에 없는 간단한 상황이지만, 운전시 핸들을 어떻게 돌려야 하는지 같은 각도 같은 연속적인 이러한 경우는 훨씬 어렵기 때문에 100000번, 20000번 시도해 우연히 성공하듯 학습하게 됩니다. 추가적으로 알파고 제로 또한 바둑이 뭔지 모르는 상태에서 학습을 시작합니다.

## Characteristics: Optimalization
강화학습은 최대의 보상을 얻을수 있는 최적의 정책을 학습하는 것이 목표 입니다.
예를들어, 체스에서는 승리하는 것이 최대의 보상이 될겁니다. 혹은 최단거리로 미로를 빠져 나가는 경로를 찾는 경우도 있을수 있거요.

주요한 특징은, 현재 선택한 행동에


## 특징: 최적화
기본적으로 강화학습은 최적화를 푸는 문제

중요한거 어떤 가치가 있는지 명시적으로 표표현된다.

가와학습의 모델은 옭고 그름을 알지 못하고 어떤 보상상이 주어지는지다.

## 순차적
직관적으로 당연

## Delayed Consequences

체스를 예로들면 지금의 수가 당장은 손해더라도 끝까지 갔을때때의 보상이 큰 행도을 학습(승리)

## Exploration
손 했을때 손을 물었을때, 손으 올렸다면 머기가 주어졌단 사실을 모름(대부분)

***

# 언제 쓸까 Where Reinforcement Learning

***

# 핵심 컨셉 [사실 앞에 있는건 중요하지 않다]

1. 상태 하나하나 시시간이 존재한다.
2. s를 입력으로 삼아 a(행동)를 출력력으로 내는건데, P(a | s)
3. 그 행동에 대한 결과를 준다. Rt1 + 1, 에이전트에게
4. 보상이 주주어진다면 환경의 상태는 변화한다.(바둑돌을 두면 환경이 변한다.)


 어떤 관측한 상태로 먼가 행동을 하면 보상이 주어지고(양수, 음수 다 가능) 상태를 변화하고 업데이트

 # 에이전트

 # 환경
 이전에 에이전트를 제외한 모든것이라 했지만, 환경을 모두 아는것은 불가능능하므로
실제 환경과 실제ㅏㄴ경은 다르다.

# 행행동


# 보상
결국 숫자다, 기본적으로 환경을 정확히 모르기 때문에 명확히 보상을 지급하지 않는는다.

그래서 기본적으로 보상을 정정의한다.

예를들어, 체스말을 잃었을때 -1, 안잃었으면 0, 얻었으면 +1

예를들어,트럭 시뮬 경우, 만약 시시간당 -1을 주어진다면 빠르르게 하는데에 집중해 모든 오브젝트에 박아대면서 주차할 수 있다.

어떤 예에선 게임 AI가 -1을 받지 않기 위해 승리(+1) 보다 게임을 멈추는 방법(0) 을 학습했다.

# 정책
결국 함수

# return
Rt + rRt + ...

모두 가지고 있따.

# 가치 함수

정책이 주어 져을때 어떤 상태 s에 대해