쇼킹한것은 아무런 규칙도 알려주지 않고 같은코드에 게임만 변경함

기존 Q-learning: 할때마다 업데이트

Experience Replay: 하나씩 뽑을순 있는데 보통 여러개 미니배치 사이즈만큼 뽑음

무작위로 추출: 상호작용하는 순서와는 상관없이, 추출이니 없어지는게 아님

빨간줄: 논문에서 이렇게 말했다 함

***

Experience Replay: Example

빨간 글: 발산 문제가 완화, 이런 자료 없다,,

***

target: 차이가 없게 만들기 위한 목적, 근데 이전까진 목적이 계속 변함, 뭔가 이상하다, 그래서 업데이트 안함

Target Network: 안정된 target 반환

직접 해보기: 이런 자료 없다..

증가치가 더욱 감소: 아직 발사을 완전히 막진 못하지만  거의 막음

***

에타가 0? 에러 추출예기, 에타가 0이면 1/N 이니 일반적인 경험 리플레이

근데 높은확률로 뽑는다 해도 의미가 있나? 음.. 별로?

매개변수가 업데이트 되더라도, 해당 경험에 대한 Error가 극적으로 감소하지않기 때문에 한번 큰 에러는 반복적으로 추출됨

중요한 경험만 보고자 했지만 일반적인 경험리플레이봐 못할 가능성이 큼

솔루션: 주어진 경험이 추출될 확률을 고려하여 매개변수가 업데이트 되는 강도를 조정

# CNN
아타리 게임  같은 경우 특성값을 뽑거나 에이전트를 수정하지 않음, 같은 에이전트에 게임만 바꿈

cnn: dqn의 필수 네트웤, 이미지를 처리하기 위한 네트웤, 연속된 어쩌구를 위한 네트웤 프레임 앞두가 별로안달라지니까

dqn은 이미지 통으로 했음

# Convolution
w 가 같은게 매우 큰 특징, 연속된 친구끼리 가중치 고려, 뭔가 연관이 있지 않을까?

한칸씩 움직이면서 똑같은 w를 곱한것

가중치는 어떻게보면 하나의 집합, 필터 같은건데? 여러개 쓸수도 있음

# Convolution Layer
딥러닝은 용어가 별것도 없는데 이상한것들 많이 썼지만 보면 별거 없음

# pooling layer
용어를 왜 pool 로 쓸까.. 햇갈리게 -교수-

사이즈 압축: zip 같은 알고리즘이 아니라 필요한것만 추리는것, 수학적으로 추리고 반으로 줄어듬

이론으로 100% 이해하지 말고 나중에 직접 구현해보자(4학년때 함)

***

컴퓨터가 다해주는데 뭘

그리고 이런 인공신경망은 텐서플로우,파이토치가 어떻게보면 규격임

재밌는 이야기: 텐서플롱가 kepra?로 진화했는데 파이토치 스러움

프로그래밍 언어도 서로 장점 뽑고 수렴진화함

상태 행동에 대해 w를 곱하기 땜문에 이전 어쩌구까지 구해지는게 펑션 어프로치메이트의 큰 특징