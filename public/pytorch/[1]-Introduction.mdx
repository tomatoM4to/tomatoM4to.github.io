# Pytorch
파이토치에 대해 알아보는 시간을 가져보겠 습니다. 이번 장에선 간단히 파이토치가 무엇인지, 파이토치가 어떤 역사를 가직 있는지, 그리고 마지막으로 간단한 모델을 만들어보는 시간을 가져보겠습니다.

## 역사
먼저 페이스북이 초기 루아(Lua) 언어로 개발한 토치(Torch) 라는 프레임워크가 있었습니다. 모 하지만 다들 루아 라는 언어를 모르죠? 페이스북이 토치를 파이썬 으로 새로 개발한 것이 파이토치입니다. 파이토치는 2017년에 처음 공개되었으며, 현재까지도 계속해서 업데이트 되고 있습니다.

초기 Torch는 Numpy 라이브러리 처럼 과학 연산을 위한 라이브러리로 시작되었 습니다. 이후 병렬처리를 위해 GPU를 이용한 텐서 조작 및 동적 신경망 구축이 가능하도록 딥러닝 프레임워크로 발전 시켰습니다.

파이썬의 철학에 잘 맞게 만들어 졌고, 유연하면서 가속화된 계산 속도를 제공합니다.

## 모듈 구조
바텀업 방식으로 보겠습니다.

당연하겠지만 로우 레벨에선 C나 CUDA 같은 하드웨어에 가까운 저수준 의 언어로 작성이 되어 있습니다. 그 위에는 C++로 작성된 ATen이라는 라이브러리가 있습니다. ATen은 텐서 연산을 위한 라이브러리로, 어떻게보면 엔진이라 할수 있습니다.

top 레벨엔 파이썬 API가 있습니다. 이로서 파이토치는 C++, C, CYDA로 작성된 코드들이 파이썬으로 래핑되어 있는 형태로 제공된다는것을 알수 있습니다. 그래서 가끔 어떤 분들이 왜 트린 파이썬으로 인공지능 신경망을 만드는거지? 에 대한 의문이 조금은 해결되지 않을까 싶습니다.

그리고 이러한 API는 최상단에서 볼수 있는것처럼 , `torch`, `torch.autograd`, `torch.nn`, `torch.multiprocessing`, `torch.utils` 이러한 모듈들로 제공됩니다.

## 구성 요소
앞서 언급한 모듈들을 간단히 살펴보겠습니다. 좀 있다 예제를 통해 살펴보겠습니다.

* `torch`: 메인 네임스페이스로서 텐서등 다양한 수학 함수가 포함되어 있습니다.
* `torch.autograd`: 자동 미분을 위한 함수들이 포함되어 있습니다.
* `torch.nn`: 신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의되어 있습니다.
* `torch.optim`: 확률적 경사 하강법(Stochastic Gradient Descent, SGD)를 중심으로 한 파라미터 최적화 알고리즘이 구현되어 있습니다.
* `torch.multiprocessing`: 병렬처리 기능을 제공하는 라이브러리 입니다.
* `torch.utils`: 데이터 조작 등 유틸리티 기능이 구현되어 있습니다.
* `torch.onnx`: ONNX(Open Neural Network Exchange)의 포맷으로 모델을 익스포트(export)할 때 사용하는 패키지입니다. 파이토치로 모델을 구성하고 학습을 진행 시켰지만 다른 프레임워크에서 사용하고 싶을때 사용합니다.

## 텐서(Tensor)
파이토치를 잘 쓰기 위해선 텐서 라는 개념을 먼저 이해 해야합니다. 텐서라는 개념은 수학에서 많이 사용돼는 개념인데요, 일단 간단히 말하면 **신경망을 이용해 어떠한 데이터를 처리할때 이 데이터를 처리하는 가장 기본적인 구조** 라고 할수 있습니다.

좀더 러프하게 표현 하자면 데이터를 담기위한 다차원 배열이라고 할수 있습니다. Numpy에 익숙한 분이시라면 텐서는 Numpy의 ndarray와 유사한 구조라 생각하셔도 괜찮습니다. 여러 타입을 저장할순 있지만 일반적으로 수치형 데이터를 저장하게 됩니다.

파이토치는 이러한 텐서를 그냥 다루는것이 아니라 GPU를 사용해 텐서 조작을 가속화 할수 있는 기능이 있습니다.

그림을 통해 다차원 텐서를 이해할 필요가 있는데요 차례대로 보겠습니다.

**0D Tensor**
* 0D Tensor: Scalar, 차원이 없는 텐서는 스칼라 라고 부릅니다.
* Rank: 축이라고 부릅니다. 축이 없습니다.
* Shape: 텐서의 모양을 나타내는것이며, 모양이 없기 때문에 이또한 없습니다.

**1D Tensor**
* 벡터 라고 부를수 있습니다. 그래서 Rank가 1이 됩니다. Shape는 (x,) 이렇게 표현됩니다. 위 그림에선 3개 있으니 (3,) 이 됩니다.

**2D Tensor**
* Matrix 라고 부릅니다. Rank가 2가 됩니다. Shape는 (x, y) 이렇게 표현됩니다. 위 그림에선 3x3 이니 (3, 3) 이 됩니다.

**3D Tensor**
* 3D Tensor: Rank가 3이 됩니다. Shape는 (x, y, z) 이렇게 표현됩니다. 위 그림에선 3x3x3 이니 (3, 3, 3) 이 됩니다. 마치 큐브 같죠?

4차원 텐서는 3차원 텐서의 모임, 5차원 텐서는 4차원 텐서들의 모임... 쭉 이어나갑니다. 계속 이러한 방식으로 7차원 8차원 쭉 늘어날수 있게 됩니다.

결론적으로 텐서는 다차원의 개념을 담고있고 다차원의 데이터 표현을 위해 사용됩니다.










