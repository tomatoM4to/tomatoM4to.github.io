# Loss function
모델을 정의해서 구성했습니다. 이러한 모델을 구성할땐 들어가야할 Parameter 값들이 있습니다. 그중 하나인 Loss function에 대해 알아보겠습니다.

실제 모델이 예측한 값과 실제 값(정답) 사이의 오차를 측정하는 함수 입니다.

학습이 진행되면서 해당 과정이 얼마나 잘 되고 있는지 나타내는 지표와 같은 역할을 합니다.

모델이 훈련되는 동안 최소화될 값으로 주어진 문제에 대한 성공 지표 볼수도 있죠

손실 함수에 따른 결과를 통해 학습 파라미터를 조정합니다.

최적화 이론에서 최소화 하고자 함수가 바로 손실 함수입니다.

미분 가능한 함수를 사용해야 합니다.

파이토치의 주요 손실 함수는 다음과 같습니다.
* `torch.nn.BCELoss` : 이진 분류를 위해 사용합니다.
* `torch.nn.CrossEntropyLoss` : 다중 Class 분류를 위해 사용합니다.
* `torch.nn.MSELoss` : 회귀 문제를 위해 사용합니다.

손실 함수는 `torch.nn.functional` 모듈을 통해 사용할 수 있습니다.

```python
import torch
import torch.nn as nn

critertion = nn.CrossEntropyLoss()
```
***

# Optimizer
손실 함수를 기반으로 모델이 어떻게 업데이트 되어야 하는지 결정합니다.

optimizer는 `step()` 을 통해 전달받은 파라미터를 모델 업데이트 모든 옵티마지저는 기본으로 `torch.optim.Optimizer` 클래스를 사용합니다.

`zero_grad()` 함수를 통해 기울기를 옵티마이저에 사용된 파라미터들의 기울기를 0으로 초기화할수 있습니다.

`torch.optim.lr_scheduler` 를 통해 학습률을 조절할 수 있습니다.

TODO: 옵티마티저 리스트 확실하지 않음

주요 옵티마이저엔 다음과 같습니다.
* `torch.optim.SGD` : 확률적 경사 하강법
* `torch.optim.Adam` : Adam 옵티마이저
* `torch.optim.Adadelta` : Adadelta 옵티마이저
* `torch.optim.Adagrad` : Adagrad 옵티마이저
* `torch.optim.AdamW` : AdamW 옵티마이저
* `torch.optim.Adamax` : Adamax 옵티마이저
* `torch.optim.ASGD` : ASGD 옵티마이저
* `torch.optim.RMSprop` : RMSprop 옵티마이저
* `torch.optim.Rprop` : Rprop 옵티마이저

***

# Learning Rate Scheduler
학습시 특정 조건에 따라 학습률을 조절하여 최적화를 진행하는 방법입니다.

일정획수 이상이 되면 학습률을 감소(decay) 시키거나 전역 최소점(global minimum) 근처에 가면 학습률을 줄이는 등 여러 방법이 있습니다.

파이토치의 학습률 스케줄러의 종류는 다음과 같이 있습니다.

TODO: 리스트 작성해야함


***

# Metrics
모델의 학습과 테스트 단계를 평가하기 위해 사용되는 함수입니다.

잘 돼고 있는지 안돼고 있는지, 정확도는 몇인지 개선은 돼고 있는지에대해 눈으로 확인할 수 있습니다.

metrics를 사용하기 위해선 먼저 `torchmetrics` 라이브러리를 설치해야 합니다.

```bash
pip install torchmetrics
```

**EXAMPLE**
```python
import torchmetrics
```

TODO: 코드 작성해야함