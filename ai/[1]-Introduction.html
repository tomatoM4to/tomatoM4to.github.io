<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/7626ed2c039b2726-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="./img/RL-workflow.png"/><link rel="stylesheet" href="/_next/static/css/df1c3f4ec0b23699.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-8cb709924e8d3f28.js"/><script src="/_next/static/chunks/4bd1b696-7f4092adee896cfb.js" async=""></script><script src="/_next/static/chunks/517-eb49c20223f87188.js" async=""></script><script src="/_next/static/chunks/main-app-cdef3b2a1179051d.js" async=""></script><script src="/_next/static/chunks/839-ee0b8cfd2b2dd0ab.js" async=""></script><script src="/_next/static/chunks/app/%5Btheme%5D/%5Bpost%5D/page-65ffec522be16945.js" async=""></script><script src="/_next/static/chunks/0e762574-45ffabdaeed21b00.js" async=""></script><script src="/_next/static/chunks/app/layout-f3645d2bed64bd74.js" async=""></script><script src="/_next/static/chunks/5e22fd23-a3f66ab797e86fa4.js" async=""></script><script src="/_next/static/chunks/578c2090-175257547869ead6.js" async=""></script><script src="/_next/static/chunks/977-0a004c1c6b7bbfa2.js" async=""></script><script src="/_next/static/chunks/app/%5Btheme%5D/layout-f16cf79ce296f3c6.js" async=""></script><meta name="next-size-adjust"/><title>tomatom4to&#x27;s Computer Science Blog</title><meta name="description" content="Comprehensive computer science knowledge covering OS, Database, AI, Networks, Linux, and Docker. Learn computer science concepts with clear explanations and practical examples."/><meta name="author" content="tomatom4to"/><meta name="keywords" content="Computer Science,Operating Systems,Database,AI,Network,Linux,Docker,Programming,Software Development"/><meta name="creator" content="tomatom4to"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://tomatom4to.github.io"/><meta name="format-detection" content="telephone=no, address=no, email=no"/><meta property="og:title" content="tomatom4to&#x27;s Computer Science Blog"/><meta property="og:description" content="Your gateway to comprehensive computer science knowledge and practical programming skills"/><meta property="og:url" content="https://tomatom4to.github.io"/><meta property="og:site_name" content="tomatom4to&#x27;s CS Blog"/><meta property="og:locale" content="ko_KR"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="tomatom4to&#x27;s Computer Science Blog"/><meta name="twitter:description" content="Your gateway to comprehensive computer science knowledge and practical programming skills"/><link rel="icon" href="/icon.png?09be0199f64930e1" type="image/png" sizes="500x500"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_9b822d
                min-h-screen
                bg-slate-50 text-slate-900 dark:bg-slate-900 dark:text-slate-50"><div class="grid grid-cols-[24rem_1fr] auto-rows-auto"><nav class="
        __className_92d895
        z-20
        text-xl h-12 md:h-14 md:text-2xl
        bg-slate-50/30 dark:bg-slate-900/30 border-b-slate-300 dark:border-b-slate-600
        backdrop-blur-md
        w-full
        flex
        items-center
        justify-between
        pl-5
        pr-5
        mb-20
        border-b-2
        border-b-slate-300
        fixed"><a href="/">tomatoM4to&#x27;s blog</a><div class="hidden lg:flex items-center"><input type="text" class="w-36 h-7 rounded-full border-2 border-black pl-2" placeholder="search"/><div class="bg-slate-300 h-10 w-0.5 ml-2"></div><a class="p-2 rounded-full hover:bg-slate-300 dark:hover:bg-slate-600 transition-colors duration-300" href="https://github.com/tomatoM4to/tomatoM4to.github.io"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" class="text-2xl cursor-pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M7.976 0A7.977 7.977 0 0 0 0 7.976c0 3.522 2.3 6.507 5.431 7.584.392.049.538-.196.538-.392v-1.37c-2.201.49-2.69-1.076-2.69-1.076-.343-.93-.881-1.175-.881-1.175-.734-.489.048-.489.048-.489.783.049 1.224.832 1.224.832.734 1.223 1.859.88 2.3.685.048-.538.293-.88.489-1.076-1.762-.196-3.621-.881-3.621-3.964 0-.88.293-1.566.832-2.153-.05-.147-.343-.978.098-2.055 0 0 .685-.196 2.201.832.636-.196 1.322-.245 2.007-.245s1.37.098 2.006.245c1.517-1.027 2.202-.832 2.202-.832.44 1.077.146 1.908.097 2.104a3.16 3.16 0 0 1 .832 2.153c0 3.083-1.86 3.719-3.62 3.915.293.244.538.733.538 1.467v2.202c0 .196.146.44.538.392A7.984 7.984 0 0 0 16 7.976C15.951 3.572 12.38 0 7.976 0z"></path></svg></a><button class="p-2 rounded-full hover:bg-slate-300 dark:hover:bg-slate-600 transition-colors duration-300"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="text-2xl cursor-pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill="none" stroke-width="2" d="M12,23 C18.0751322,23 23,18.0751322 23,12 C23,5.92486775 18.0751322,1 12,1 C5.92486775,1 1,5.92486775 1,12 C1,18.0751322 5.92486775,23 12,23 Z M12,23 C15,23 16,18 16,12 C16,6 15,1 12,1 C9,1 8,6 8,12 C8,18 9,23 12,23 Z M2,16 L22,16 M2,8 L22,8"></path></svg></button></div></nav><main class="col-span-2"><div class="flex"><aside class="lg:hidden"><button class="
            flex
            flex-col
            gap-1
            justify-center
            items-center
            rounded-lg
            fixed
            right-2
            top-2
            active:outline-none
            p-2
            transition-all
            hover:bg-slate-300 dark:hover:bg-slate-600
            w-8 h-8 md:w-10 md:h-10
            z-50
            "><span class="w-6 h-0.5 bg-slate-900 dark:bg-slate-50 rounded transform transition-transform duration-300 ease-in-out "></span><span class="w-6 h-0.5 bg-slate-900 dark:bg-slate-50 rounded transition-opacity duration-300 ease-in-out opacity-100"></span><span class="w-6 h-0.5 bg-slate-900 dark:bg-slate-50 rounded transform transition-transform duration-300 ease-in-out "></span></button><div class="
                bg-slate-50 text-slate-900 dark:bg-slate-900 dark:text-slate-50
                fixed
                top-0
                right-0
                flex
                flex-col
                px-5
                h-full
                w-7/12
                shadow-lg
                transform
                transition-transform
                duration-300
                ease-in-out
                translate-x-full
                overflow-y-auto
                z-40"><nav class="h-12 md:h-14 flex items-center border-b-2 bg-slate-50/30 dark:bg-slate-900/30 border-b-slate-300 dark:border-b-slate-600"><a class="p-2 rounded-full duration-300 hover:bg-slate-300 dark:hover:bg-slate-600" href="https://github.com/tomatoM4to/tomatoM4to.github.io"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" class="text-xl md:text-2xl cursor-pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M7.976 0A7.977 7.977 0 0 0 0 7.976c0 3.522 2.3 6.507 5.431 7.584.392.049.538-.196.538-.392v-1.37c-2.201.49-2.69-1.076-2.69-1.076-.343-.93-.881-1.175-.881-1.175-.734-.489.048-.489.048-.489.783.049 1.224.832 1.224.832.734 1.223 1.859.88 2.3.685.048-.538.293-.88.489-1.076-1.762-.196-3.621-.881-3.621-3.964 0-.88.293-1.566.832-2.153-.05-.147-.343-.978.098-2.055 0 0 .685-.196 2.201.832.636-.196 1.322-.245 2.007-.245s1.37.098 2.006.245c1.517-1.027 2.202-.832 2.202-.832.44 1.077.146 1.908.097 2.104a3.16 3.16 0 0 1 .832 2.153c0 3.083-1.86 3.719-3.62 3.915.293.244.538.733.538 1.467v2.202c0 .196.146.44.538.392A7.984 7.984 0 0 0 16 7.976C15.951 3.572 12.38 0 7.976 0z"></path></svg></a></nav><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[1]-Introduction"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Introduction</a><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Basis math</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[2-1]-Math-with-RL">Math with RL</a></div></div></div></div><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Q learning</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[3-1]-Basic-Concept">Basic Concept</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[3-2]-Greedy-action">Greedy action</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[3-3]-Discount-factor">Discount factor</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[3-4]-Learning-rate">Learning rate</a></div></div></div></div><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[4]-Markov-process"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Markov process</a><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[5]-Bellman-Optimality-Equation"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Bellman Optimality Equation</a><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[6]-DP"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>DP</a><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[7]-Monte-Carlo"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Monte Carlo</a><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[8]-Temporal-Difference-Learning"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Temporal Difference Learning</a><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[9]-nSTEP-Bootstrapping"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>nSTEP Bootstrapping</a><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Planning and Learning</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[10-1]-Function-Approximation">Function Approximation</a></div></div></div></div><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Deep learning</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[11-1]-Concept">Concept</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[11-2]-Newerl-Network">Newerl Network</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[11-3]-Loss-function">Loss function</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[11-4]-Activation-function">Activation function</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[11-5]-Gradient-descent">Gradient descent</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[11-6]-Back-Propagation">Back Propagation</a></div></div></div></div><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Q Network</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[12-1]-Overview">Overview</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[12-2]-Frozen-Lake">Frozen Lake</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[12-3]-Cartpole">Cartpole</a></div></div></div></div><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>DQN</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[13-1]-Concept">Concept</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[13-2]-Cartpole">Cartpole</a></div></div></div></div><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Policy gradient</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[14-1]-A3C">A3C</a></div></div></div></div></div><div class="
        w-screen
        h-screen
        fixed
        left-0
        top-0
        transition-opacity
        duration-300
        bg-slate-50/70 dark:bg-slate-900/70
        opacity-0 pointer-events-none
        z-30"></div></aside><aside class="
                hidden lg:flex w-64 2xl:w-96
                border-r-slate-300 dark:border-r-slate-600
                flex-col
                h-screen
                border-r-2
                p-1
                pl-5
                pt-14
                fixed
                overflow-y-auto
                overscroll-contain
                z-auto
                "><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[1]-Introduction"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Introduction</a><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Basis math</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[2-1]-Math-with-RL">Math with RL</a></div></div></div></div><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Q learning</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[3-1]-Basic-Concept">Basic Concept</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[3-2]-Greedy-action">Greedy action</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[3-3]-Discount-factor">Discount factor</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[3-4]-Learning-rate">Learning rate</a></div></div></div></div><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[4]-Markov-process"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Markov process</a><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[5]-Bellman-Optimality-Equation"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Bellman Optimality Equation</a><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[6]-DP"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>DP</a><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[7]-Monte-Carlo"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Monte Carlo</a><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[8]-Temporal-Difference-Learning"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Temporal Difference Learning</a><a class="
                flex
                items-center
                px-1 py-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
                text-sm sm:text-base
                " href="./[9]-nSTEP-Bootstrapping"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>nSTEP Bootstrapping</a><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Planning and Learning</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[10-1]-Function-Approximation">Function Approximation</a></div></div></div></div><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Deep learning</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[11-1]-Concept">Concept</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[11-2]-Newerl-Network">Newerl Network</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[11-3]-Loss-function">Loss function</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[11-4]-Activation-function">Activation function</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[11-5]-Gradient-descent">Gradient descent</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[11-6]-Back-Propagation">Back Propagation</a></div></div></div></div><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Q Network</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[12-1]-Overview">Overview</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[12-2]-Frozen-Lake">Frozen Lake</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[12-3]-Cartpole">Cartpole</a></div></div></div></div><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>DQN</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[13-1]-Concept">Concept</a><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[13-2]-Cartpole">Cartpole</a></div></div></div></div><div class="flex flex-col text-sm sm:text-base"><button class="
                    flex
                    justify-between
                    items-center
                    text-left
                    px-1 py-2
                    hover:bg-slate-300 dark:hover:bg-slate-600
                    focus:outline-none
                    rounded-lg"><div class="flex items-center"><div class="
                w-1.5 h-1.5 sm:w-2 sm:h-2
                bg-slate-900 dark:bg-slate-50
                rounded-full
                mr-3
            "></div>Policy gradient</div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="rotate-0" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></button><div class="overflow-hidden max-h-0"><div class="p-2"><div class="flex flex-col"><a class="
                        p-2
                        border-l-2
                        
    border-l-slate-300 dark:border-l-slate-600
    hover:text-blue-600 dark:hover:text-cyan-400
    hover:border-blue-600 dark:hover:border-cyan-400" href="./[14-1]-A3C">A3C</a></div></div></div></div></aside><div class="lg:ml-64 2xl:ml-96 mt-32 mb-32 flex-1 flex flex-col items-center overflow-x-hidden"><div class="w-11/12 lg:w-2/3 2xl:w-1/2 markdown-body"><h1 class=" text-3xl md:text-4xl font-extrabold my-6 text-transparent bg-clip-text bg-gradient-to-r from-blue-500 via-purple-500 to-pink-500 dark:from-blue-400 dark:via-purple-400 dark:to-pink-400">Reinforcement Learning</h1>
<p>배고픈 고양이를 버튼을 눌러야 문이 열리는 특별한 상자에 가두고, 상자 밖에는 먹이를 두었다고 사정해 봅시다. 만약 고양이가 우연히 버튼을 누르게 되면 상자 밖으로 빠져나와 사료를 먹을수 있을겁니다. 이를 반복하게 되면, 고양이는 <span class=" text-rose-600 font-bold text-lg hover:text-rose-700 dark:text-rose-400 dark:hover:text-rose-500 transition-colors duration-200">버튼을 누르는 것이 긍정적인 결과를 낸다</span> 를 학습하고 점점 더 상자에서 빠져나오는 시간이 짧아지게 될겁니다. 이것은, 버튼을 누르는 행동과 긍적적인 결과간의 관계가 강화(Reinforcement)되었다 라고 할수 있습니다. 이것이 바로 강화학습의 기본적인 개념입니다.</p>
<p>이를 위에서 배웠던 용어로 다시 정리해 보겠습니다.</p>
<ul class="list-disc ml-12 space-y-2 my-3">
<li><strong>에이전트(Agent)</strong>: 고양이</li>
<li><strong>환경(Environment)</strong>: 고양이를 제외한 모든것</li>
<li><strong>상태(State)</strong>: 고양이가 상자 안에 있는 상황, 상자 밖에 있는 상황 등 현재 환경의 상태</li>
<li><strong>행동(Action)</strong>: 버튼을 누르거나, 움직이거나 하는 등 에이전트가 선택할 수 있는 모든 행동들</li>
<li><strong>정책(Policy)</strong>: 특정 상태에서 어떤 행동을 선택할지 결정하는 전략 (예: 배고픈 상태에서 버튼을 누르는 행동을 선택)</li>
<li><strong>보상(Reward)</strong>: 행동의 결과로 얻는 피드백 (예: 먹이를 먹었을 때의 만족감)</li>
</ul>
<p>추가적으로 강화학습의의 중요한 핵심적인 특징중 하나는, <strong>버튼을 누르면 왜 문이 열리는가?</strong> 와 같은 원리를 이해할 필요가 없습니다. 대신 <strong>이 상황(상태)에서 이 행동을 했더니 좋은 결과가 났다</strong>라는 사실만 알면 됩니다.</p>
<p>아래 예시로 보여드릴 딥바인드에서 만든 게임 AI는 화면, 점수, 오른쪽 왼쪽밖에 모른다고 합니다. 이 AI는 화면에서 어떤 행동을 해야 좋은 결과를 얻는지를 학습하게 됩니다. 아래 예시에선 왼쪽, 오른쪽 밖에 없는 간단한 상황이지만, 운전시 핸들을 어떻게 돌려야 하는지 같은 각도 같은 연속적인 이러한 경우는 훨씬 어렵기 때문에 100000번, 20000번 시도해 우연히 성공하듯 학습하게 됩니다. 추가적으로 알파고 제로 또한 바둑이 뭔지 모르는 상태에서 학습을 시작합니다.</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">Characteristics: Optimalization</h2>
<p>강화학습은 최대의 보상을 얻을수 있는 최적의 정책을 학습하는 것이 목표 입니다.
예를들어, 체스에서는 승리하는 것이 최대의 보상이 될겁니다. 혹은 최단거리로 미로를 빠져 나가는 경로를 찾는 경우도 있을수 있거요.</p>
<p>주요한 특징은, 현재 선택한 행동에</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">특징: 최적화</h2>
<p>기본적으로 강화학습은 최적화를 푸는 문제</p>
<p>중요한거 어떤 가치가 있는지 명시적으로 표표현된다.</p>
<p>가와학습의 모델은 옭고 그름을 알지 못하고 어떤 보상상이 주어지는지다.</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">순차적</h2>
<p>직관적으로 당연</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">Delayed Consequences</h2>
<p>체스를 예로들면 지금의 수가 당장은 손해더라도 끝까지 갔을때때의 보상이 큰 행도을 학습(승리)</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">Exploration</h2>
<p>손 했을때 손을 물었을때, 손으 올렸다면 머기가 주어졌단 사실을 모름(대부분)</p>
<hr/>
<h1 class=" text-3xl md:text-4xl font-extrabold my-6 text-transparent bg-clip-text bg-gradient-to-r from-blue-500 via-purple-500 to-pink-500 dark:from-blue-400 dark:via-purple-400 dark:to-pink-400">WhereReinforcementLearning?</h1>
<ul class="list-disc ml-12 space-y-2 my-3">
<li>
<p>최적의정책을인간이알수없는경우</p>
<ul class="list-disc ml-12 space-y-2 my-3">
<li>예.인간의능력을뛰어넘는최적의정책을찾기(AlphaGoZero등)</li>
<li>예.기존에알려지지않은분야에서최적의정책을찾기(핵융합로플라즈마 제어 등)</li>
</ul>
</li>
<li>
<p>현재시점에서선택된행동의(최종적인)결과를나중에라도알수있는 경우</p>
<ul class="list-disc ml-12 space-y-2 my-3">
<li>예.AtariBreakout:점수</li>
<li>예.체스나바둑:승패여부</li>
</ul>
</li>
</ul>
<hr/>
<h1 class=" text-3xl md:text-4xl font-extrabold my-6 text-transparent bg-clip-text bg-gradient-to-r from-blue-500 via-purple-500 to-pink-500 dark:from-blue-400 dark:via-purple-400 dark:to-pink-400">Workflow</h1>
<p>Workflow를 보기 전 알아두어야 할점은 상태 하나하나 시간이 존재한다는것입니다.</p>
<p><img src="./img/RL-workflow.png" alt="Reinforcement Learning Workflow"/></p>
<p>순서대로 살펴보겠습니다.</p>
<ol class="list-decimal ml-12 space-y-2 my-3">
<li>강화 학습 Agent는 환경의 상태를 인지 합니다.<!-- -->
<ul class="list-disc ml-12 space-y-2 my-3">
<li><strong>Environment -&gt; Agent:</strong> <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow><annotation encoding="application/x-tex">S_{t} = s</annotation></semantics></math></span></li>
</ul>
</li>
<li>정책을 바탕으로 인지된 상태에 대한 행동을 선택<!-- -->
<ul class="list-disc ml-12 space-y-2 my-3">
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi(a | s)</annotation></semantics></math></span> -&gt; <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">A_{t} = a</annotation></semantics></math></span></li>
</ul>
</li>
<li>환경은 선택된 행동에 대해 보상을 에이전트에게 제공하고, 새로운 상태로 변경, 마치 바둑돌을 두면 환경이 변화 하는것과 같습니다.<!-- -->
<ul class="list-disc ml-12 space-y-2 my-3">
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">R_{t + 1} = r</annotation></semantics></math></span>, <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">S_{t + 1} = s&#x27;</annotation></semantics></math></span></li>
</ul>
</li>
<li>상태-행동-보상을 참고하여, 현재 정책의 가치를 평가하고 더 나은 정책을 학습<!-- -->
<ul class="list-disc ml-12 space-y-2 my-3">
<li><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>π</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">v_{\pi}(s)</annotation></semantics></math></span> / <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mi>π</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q_{\pi}(s, a)</annotation></semantics></math></span></li>
</ul>
</li>
</ol>
<p>요약: 어떤 관측한 상태에서 어떠한 행동을 하면 보상이 주어지고(양수, 음수 다 가능), 상태를 변화하고 업데이트</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">에이전트</h2>
<p>환경과 상호작용하면서 주어진 목적을 달성할 수 있는 최적의 정책을 학습하는 주체 입니다. 학습된 정책을 바탕으로 주어진 상태에 대한 행동을 선택하고, 보상을 받습니다.</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">환경</h2>
<p>Agent와 상호작용 하되, Agent에 포함되지 않는 모든것을 의미합니다. Agent가 취하는 행동의 결과인 보상을 제공하며, 자신의 상태를 Agent에게 드러냅니다.</p>
<p>현실의 문제에서 환경을 전부 파악하는 경우는 거의 불가능에 가까우므로, Agent에게 드러내는 환경은 실제 환경과 다를수 있습니다.</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">상태</h2>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow><annotation encoding="application/x-tex">S_{t} = s
</annotation></semantics></math></span>
<p>Agent가 관측할 수 있는 정보를 바탕으로 인지된 환경의 상태를 의미합니다. 실제 환경의 상태와 Agent에 의해 인지된 상태는 (보통) 같지 않으나, 많은 경우 같다고 가정합니다.</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">행동</h2>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">A_{t} = a
</annotation></semantics></math></span>
<p>학습된 정책에 기반하여, 주어진 상태에서 선택한 의사결정을 의미합니다. Agent가 환경에게 전달하는 유일한 정보입니다.</p>
<p>행동은 이산적인 경우와 연속적인 경우로 나뉩니다. 이산적인 경우는 Breakout의 좌우 이동, 연속적인 경우는 앵그리버드의 발사 각도 조절이 있습니다.</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">보상</h2>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">R_{t + 1} = r
</annotation></semantics></math></span>
<p>Agent가 환경에게 받는 피드백을 의미합니다. Agent가 선택한 행동에 대한 결과로 주어지며, 보상은 양수, 음수, 0 등 Scalar Value로 주어집니다.</p>
<p>예를들어, 체스말을 잃었을때 -1, 안잃었으면 0, 얻었으면 +1 인 경우가 있습니다.</p>
<p>또 ,트럭 시뮬레이션의 경우 시간당 -1을 주어진다면 빠르게 하는데에 집중해 모든 오브젝트에 박아대면서 주차할 수 있습니다.</p>
<p>어떤 예에선 게임 AI가 -1을 받지 않기 위해 승리(+1) 보다 게임을 멈추는 방법(0) 을 학습했다.</p>
<p>이러한 적절한 보상의 설게가 RL의 주요한 난제 입니다.</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">정책</h2>
<p>상태를 입력으로, 출력을 행동으로 하는 함수로, 주어진 상태에 대한 Agent의 행동을 결정합니다.</p>
<h3 class=" text-xl md:text-2xl font-bold my-4 text-green-600 hover:text-green-800 dark:text-green-400 dark:hover:text-green-500 transition-colors duration-200">결정론적 정책(Deterministic Policy)</h3>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>a</mi><mo>=</mo><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a = \pi(s)
</annotation></semantics></math></span>
<p>결정적 정책에선 동일한 상태에 대해 동일한 행동을 하는 정책을 의미합니다.</p>
<h3 class=" text-xl md:text-2xl font-bold my-4 text-green-600 hover:text-green-800 dark:text-green-400 dark:hover:text-green-500 transition-colors duration-200">확률적 정책(Stochastic Policy)</h3>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi(a | s)
</annotation></semantics></math></span>
<p>상태에 대해 행동을할 확률을 출력으로 내는 정책을 의미합니다. 이는 동일한 상태에 대해 여러 행동을 선택할 수 있습니다.</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">수익(return)</h2>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mi>t</mi></msub><mo>+</mo><mi>γ</mi><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><msup><mi>γ</mi><mn>3</mn></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>3</mn></mrow></msub><mo>+</mo><mo>⋯</mo></mrow><annotation encoding="application/x-tex">G_{t} = R_{t} + {\gamma}R_{t + 1} + {\gamma}^2R_{t + 2} + {\gamma}^3R_{t + 3} + \cdots
</annotation></semantics></math></span>
<p>현재 시점부터 미래까지 받을 수 있는 보상의 누적합을 의미합니다. <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span>는 0 ~ 1값을 가지며 할인율을 의미합니다. 미래의 보상이 현재의 보상보다 얼마나 중요한지를 결정합니다. 현재 시점에 가까울 수록 보상의 가치가 크게 됩니다.</p>
<p>강화학습의 Agent는 Return을 최대화 하는것을 목적으로 학습합니다. 정확히는 Expected Return을 최대화 하는것을 목적으로 합니다.</p>
<p>왜 Explected Return 이냐면, state와 action이 random variable이기 때문에, 어떤 행동을 했을때 어떤 보상또한 ranom variable이기 때문입니다.</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">가치 함수(Value Function)</h2>
<p>정책의 가치를 평가하는 함수를 의미하고 2가지 종류가 있습니다. 가치란 특정 상태에서 얻을 수 있는 수익의 기대값을 의미합니다.</p>
<h3 class=" text-xl md:text-2xl font-bold my-4 text-green-600 hover:text-green-800 dark:text-green-400 dark:hover:text-green-500 transition-colors duration-200">상태 가치 함수(State Value Function)</h3>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>v</mi><mi>π</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>E</mi><mi>π</mi></msub><mo stretchy="false">[</mo><msub><mi>G</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">v_{\pi}(s) = E_{\pi}[G_{t} | S_{t} = s]
</annotation></semantics></math></span>
<p>정책이 주어 졌을때, 주어진 상태에서 얻을 수 있는 수익의 기대값을 의미합니다.</p>
<h3 class=" text-xl md:text-2xl font-bold my-4 text-green-600 hover:text-green-800 dark:text-green-400 dark:hover:text-green-500 transition-colors duration-200">행동 가치 함수(Action Value Function)</h3>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>q</mi><mi>π</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>E</mi><mi>π</mi></msub><mo stretchy="false">[</mo><msub><mi>G</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">q_{\pi}(s, a) = E_{\pi}[G_{t} | S_{t} = s, A_{t} = a]
</annotation></semantics></math></span>
<p>정책이 주어졌을때, 주어진 상태와 행동에서 얻을 수 있는 수익의 기대값을 의미합니다.</p>
<h2 class=" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200">모델(Model)</h2>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>r</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>r</mi><mi mathvariant="normal">∣</mi><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(s&#x27;, r | s, a) = P(S_{t + 1} = s&#x27;, R_{t + 1} = r | S_{t} = s, A_{t} = a)
</annotation></semantics></math></span>
<p>이전에 실제 환경과 Agent에게 드러난 환경이 다르다고 했습니다. Model은 주어진 상태에서 Agent가 내린 행동에 대해, 환경의 다음 반응(보상 및 다음 상태)를 예층하는 함수 입니다.</p>
<p>오직 관측된 정보를 바탕으로 추정하며, 실제 환경과 다를 수 있습니다. 또 Model은 RL Agent를 학습시키는 데 필수적인 요소는 아닙니다.</p></div><div class="
            flex
            flex-col sm:flex-row
            justify-between
            items-center
            w-full
            mt-8
            pt-4
            border-t-slate-300 dark:border-t-slate-600
            border-t-2
            w-11/12 lg:w-2/3 2xl:w-1/2
        "><div class="basis-1/2"></div><a class="
                w-full
                flex
                gap-2
                justify-end
                items-center
                font-medium
                py-2 sm:py-4
                px-2
                rounded-lg
                hover:bg-slate-300 dark:hover:bg-slate-600
            " href="[2-1]-Math-with-RL"><span>Math with RL</span><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z"></path></svg></a></div></div></div></main></div><script src="/_next/static/chunks/webpack-8cb709924e8d3f28.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"4:\"$Sreact.fragment\"\n5:I[4839,[\"839\",\"static/chunks/839-ee0b8cfd2b2dd0ab.js\",\"271\",\"static/chunks/app/%5Btheme%5D/%5Bpost%5D/page-65ffec522be16945.js\"],\"\"]\n6:I[2404,[\"87\",\"static/chunks/0e762574-45ffabdaeed21b00.js\",\"839\",\"static/chunks/839-ee0b8cfd2b2dd0ab.js\",\"177\",\"static/chunks/app/layout-f3645d2bed64bd74.js\"],\"DarkModeButton\"]\n7:I[5244,[],\"\"]\n8:I[3866,[],\"\"]\nb:I[6213,[],\"OutletBoundary\"]\nd:I[6213,[],\"MetadataBoundary\"]\nf:I[6213,[],\"ViewportBoundary\"]\n11:I[4835,[],\"\"]\n1:HL[\"/_next/static/media/7626ed2c039b2726-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/_next/static/css/df1c3f4ec0b23699.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"-Vkd4D1vg3cg8oXzspUAn\",\"p\":\"\",\"c\":[\"\",\"ai\",\"%5B1%5D-Introduction\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"theme\",\"ai\",\"d\"],{\"children\":[[\"post\",\"%5B1%5D-Introduction\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$4\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/df1c3f4ec0b23699.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"ko\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_9b822d\\n                min-h-screen\\n                bg-slate-50 text-slate-900 dark:bg-slate-900 dark:text-slate-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-[24rem_1fr] auto-rows-auto\",\"children\":[[\"$\",\"nav\",null,{\"className\":\"\\n        __className_92d895\\n        z-20\\n        text-xl h-12 md:h-14 md:text-2xl\\n        bg-slate-50/30 dark:bg-slate-900/30 border-b-slate-300 dark:border-b-slate-600\\n        backdrop-blur-md\\n        w-full\\n        flex\\n        items-center\\n        justify-between\\n        pl-5\\n        pr-5\\n        mb-20\\n        border-b-2\\n        border-b-slate-300\\n        fixed\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/\",\"children\":\"tomatoM4to's blog\"}],[\"$\",\"div\",null,{\"className\":\"hidden lg:flex items-center\",\"children\":[[\"$\",\"input\",null,{\"type\":\"text\",\"className\":\"w-36 h-7 rounded-full border-2 border-black pl-2\",\"placeholder\":\"search\"}],[\"$\",\"div\",null,{\"className\":\"bg-slate-300 h-10 w-0.5 ml-2\"}],[\"$\",\"$L5\",null,{\"href\":\"https://github.com/tomatoM4to/tomatoM4to.github.io\",\"className\":\"p-2 rounded-full hover:bg-slate-300 dark:hover:bg-slate-600 transition-colors duration-300\",\"children\":[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 16 16\",\"className\":\"text-2xl cursor-pointer\",\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\",\"d\":\"M7.976 0A7.977 7.977 0 0 0 0 7.976c0 3.522 2.3 6.507 5.431 7.584.392.049.538-.196.538-.392v-1.37c-2.201.49-2.69-1.076-2.69-1.076-.343-.93-.881-1.175-.881-1.175-.734-.489.048-.489.048-.489.783.049 1.224.832 1.224.832.734 1.223 1.859.88 2.3.685.048-.538.293-.88.489-1.076-1.762-.196-3.621-.881-3.621-3.964 0-.88.293-1.566.832-2.153-.05-.147-.343-.978.098-2.055 0 0 .685-.196 2.201.832.636-.196 1.322-.245 2.007-.245s1.37.098 2.006.245c1.517-1.027 2.202-.832 2.202-.832.44 1.077.146 1.908.097 2.104a3.16 3.16 0 0 1 .832 2.153c0 3.083-1.86 3.719-3.62 3.915.293.244.538.733.538 1.467v2.202c0 .196.146.44.538.392A7.984 7.984 0 0 0 16 7.976C15.951 3.572 12.38 0 7.976 0z\",\"children\":[]}]]],\"style\":{\"color\":\"$undefined\"},\"height\":\"1em\",\"width\":\"1em\",\"xmlns\":\"http://www.w3.org/2000/svg\"}]}],[\"$\",\"button\",null,{\"className\":\"p-2 rounded-full hover:bg-slate-300 dark:hover:bg-slate-600 transition-colors duration-300\",\"children\":[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 24 24\",\"className\":\"text-2xl cursor-pointer\",\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"fill\":\"none\",\"strokeWidth\":\"2\",\"d\":\"M12,23 C18.0751322,23 23,18.0751322 23,12 C23,5.92486775 18.0751322,1 12,1 C5.92486775,1 1,5.92486775 1,12 C1,18.0751322 5.92486775,23 12,23 Z M12,23 C15,23 16,18 16,12 C16,6 15,1 12,1 C9,1 8,6 8,12 C8,18 9,23 12,23 Z M2,16 L22,16 M2,8 L22,8\",\"children\":[]}]]],\"style\":{\"color\":\"$undefined\"},\"height\":\"1em\",\"width\":\"1em\",\"xmlns\":\"http://www.w3.org/2000/svg\"}]}],[\"$\",\"$L6\",null,{}]]}]]}],[\"$\",\"main\",null,{\"className\":\"col-span-2\",\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}]]}]}]}]]}],{\"children\":[[\"theme\",\"ai\",\"d\"],[\"$\",\"$4\",\"c\",{\"children\":[null,\"$L9\"]}],{\"children\":[[\"post\",\"%5B1%5D-Introduction\",\"d\"],[\"$\",\"$4\",\"c\",{\"children\":[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$0:f:0:1:2:children:0\",\"children\",\"$0:f:0:1:2:children:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$4\",\"c\",{\"children\":[\"$La\",null,[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}]]}],{},null]},null]},null]},null],[\"$\",\"$4\",\"h\",{\"children\":[null,[\"$\",\"$4\",\"AArsekj5lx7oh-nIlsT8p\",{\"children\":[[\"$\",\"$Ld\",null,{\"children\":\"$Le\"}],[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\"}]]}]]}]]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\ne:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"tomatom4to's Computer Science Blog\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Comprehensive computer science knowledge covering OS, Database, AI, Networks, Linux, and Docker. Learn computer science concepts with clear explanations and practical examples.\"}],[\"$\",\"meta\",\"3\",{\"name\":\"author\",\"content\":\"tomatom4to\"}],[\"$\",\"meta\",\"4\",{\"name\":\"keywords\",\"content\":\"Computer Science,Operating Systems,Database,AI,Network,Linux,Docker,Programming,Software Development\"}],[\"$\",\"meta\",\"5\",{\"name\":\"creator\",\"content\":\"tomatom4to\"}],[\"$\",\"meta\",\"6\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"7\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"8\",{\"rel\":\"canonical\",\"href\":\"https://tomatom4to.github.io\"}],[\"$\",\"meta\",\"9\",{\"name\":\"format-detection\",\"content\":\"telephone=no, address=no, email=no\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:title\",\"content\":\"tomatom4to's Computer Science Blog\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:description\",\"content\":\"Your gateway to comprehensive computer science knowledge and practical programming skills\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:url\",\"content\":\"https://tomatom4to.github.io\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:site_name\",\"content\":\"tomatom4to's CS Blog\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:locale\",\"content\":\"ko_KR\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"16\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:title\",\"content\":\"tomatom4to's Computer Science Blog\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:description\",\"content\":\"Your gateway to comprehensive computer science knowledge and practical programming skills\"}],[\"$\",\"link\",\"19\",{\"rel\":\"icon\",\"href\":\"/icon.png?09be0199f64930e1\",\"type\":\"image/png\",\"sizes\":\"500x500\"}]]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"12:I[9051,[\"87\",\"static/chunks/0e762574-45ffabdaeed21b00.js\",\"206\",\"static/chunks/5e22fd23-a3f66ab797e86fa4.js\",\"949\",\"static/chunks/578c2090-175257547869ead6.js\",\"839\",\"static/chunks/839-ee0b8cfd2b2dd0ab.js\",\"977\",\"static/chunks/977-0a004c1c6b7bbfa2.js\",\"859\",\"static/chunks/app/%5Btheme%5D/layout-f16cf79ce296f3c6.js\"],\"Hamburger\"]\n13:I[4783,[\"87\",\"static/chunks/0e762574-45ffabdaeed21b00.js\",\"206\",\"static/chunks/5e22fd23-a3f66ab797e86fa4.js\",\"949\",\"static/chunks/578c2090-175257547869ead6.js\",\"839\",\"static/chunks/839-ee0b8cfd2b2dd0ab.js\",\"977\",\"static/chunks/977-0a004c1c6b7bbfa2.js\",\"859\",\"static/chunks/app/%5Btheme%5D/layout-f16cf79ce296f3c6.js\"],\"NonAccordionLink\"]\n14:I[4783,[\"87\",\"static/chunks/0e762574-45ffabdaeed21b00.js\",\"206\",\"static/chunks/5e22fd23-a3f66ab797e86fa4.js\",\"949\",\"static/chunks/578c2090-175257547869ead6.js\",\"839\",\"static/chunks/839-ee0b8cfd2b2dd0ab.js\",\"977\",\"static/chunks/977-0a004c1c6b7bbfa2.js\",\"859\",\"static/chunks/app/%5Btheme%5D/layout-f16cf79ce296f3c6.js\"],\"Accordion\"]\n15:I[4783,[\"87\",\"static/chunks/0e762574-45ffabdaeed21b00.js\",\"206\",\"static/chunks/5e22fd23-a3f66ab797e86fa4.js\",\"949\",\"static/chunks/578c2090-175257547869ead6.js\",\"839\",\"static/chunks/839-ee0b8cfd2b2dd0ab.js\",\"977\",\"static/chunks/977-0a004c1c6b7bbfa2.js\",\"859\",\"static/chunks/app/%5Btheme%5D/layout-f16cf79ce296f3c6.js\"],\"AccordionItem\"]\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"div\",null,{\"className\":\"flex\",\"children\":[[\"$\",\"$L12\",null,{\"res\":[{\"includeHyphen\":false,\"firstOrder\":1,\"secondOrder\":-1,\"URL\":\"[1]-Introduction\",\"title\":\"Introduction\",\"contentList\":[]},{\"includeHyphen\":false,\"firstOrder\":2,\"secondOrder\":-1,\"URL\":\"[2]-Basis-math\",\"title\":\"Basis math\",\"contentList\":[{\"includeHyphen\":true,\"firstOrder\":2,\"secondOrder\":1,\"URL\":\"[2-1]-Math-with-RL\",\"title\":\"Math with RL\",\"contentList\":[]}]},{\"includeHyphen\":false,\"firstOrder\":3,\"secondOrder\":-1,\"URL\":\"[3]-Q-learning\",\"title\":\"Q learning\",\"contentList\":[{\"includeHyphen\":true,\"firstOrder\":3,\"secondOrder\":1,\"URL\":\"[3-1]-Basic-Concept\",\"title\":\"Basic Concept\",\"contentList\":[]},{\"includeHyphen\":true,\"firstOrder\":3,\"secondOrder\":2,\"URL\":\"[3-2]-Greedy-action\",\"title\":\"Greedy action\",\"contentList\":[]},{\"includeHyphen\":true,\"firstOrder\":3,\"secondOrder\":3,\"URL\":\"[3-3]-Discount-factor\",\"title\":\"Discount factor\",\"contentList\":[]},{\"includeHyphen\":true,\"firstOrder\":3,\"secondOrder\":4,\"URL\":\"[3-4]-Learning-rate\",\"title\":\"Learning rate\",\"contentList\":[]}]},{\"includeHyphen\":false,\"firstOrder\":4,\"secondOrder\":-1,\"URL\":\"[4]-Markov-process\",\"title\":\"Markov process\",\"contentList\":[]},{\"includeHyphen\":false,\"firstOrder\":5,\"secondOrder\":-1,\"URL\":\"[5]-Bellman-Optimality-Equation\",\"title\":\"Bellman Optimality Equation\",\"contentList\":[]},{\"includeHyphen\":false,\"firstOrder\":6,\"secondOrder\":-1,\"URL\":\"[6]-DP\",\"title\":\"DP\",\"contentList\":[]},{\"includeHyphen\":false,\"firstOrder\":7,\"secondOrder\":-1,\"URL\":\"[7]-Monte-Carlo\",\"title\":\"Monte Carlo\",\"contentList\":[]},{\"includeHyphen\":false,\"firstOrder\":8,\"secondOrder\":-1,\"URL\":\"[8]-Temporal-Difference-Learning\",\"title\":\"Temporal Difference Learning\",\"contentList\":[]},{\"includeHyphen\":false,\"firstOrder\":9,\"secondOrder\":-1,\"URL\":\"[9]-nSTEP-Bootstrapping\",\"title\":\"nSTEP Bootstrapping\",\"contentList\":[]},{\"includeHyphen\":false,\"firstOrder\":10,\"secondOrder\":-1,\"URL\":\"[10]-Planning-and-Learning\",\"title\":\"Planning and Learning\",\"contentList\":[{\"includeHyphen\":true,\"firstOrder\":10,\"secondOrder\":1,\"URL\":\"[10-1]-Function-Approximation\",\"title\":\"Function Approximation\",\"contentList\":[]}]},{\"includeHyphen\":false,\"firstOrder\":11,\"secondOrder\":-1,\"URL\":\"[11]-Deep-learning\",\"title\":\"Deep learning\",\"contentList\":[{\"includeHyphen\":true,\"firstOrder\":11,\"secondOrder\":1,\"URL\":\"[11-1]-Concept\",\"title\":\"Concept\",\"contentList\":[]},{\"includeHyphen\":true,\"firstOrder\":11,\"secondOrder\":2,\"URL\":\"[11-2]-Newerl-Network\",\"title\":\"Newerl Network\",\"contentList\":[]},{\"includeHyphen\":true,\"firstOrder\":11,\"secondOrder\":3,\"URL\":\"[11-3]-Loss-function\",\"title\":\"Loss function\",\"contentList\":[]},{\"includeHyphen\":true,\"firstOrder\":11,\"secondOrder\":4,\"URL\":\"[11-4]-Activation-function\",\"title\":\"Activation function\",\"contentList\":[]},{\"includeHyphen\":true,\"firstOrder\":11,\"secondOrder\":5,\"URL\":\"[11-5]-Gradient-descent\",\"title\":\"Gradient descent\",\"contentList\":[]},{\"includeHyphen\":true,\"firstOrder\":11,\"secondOrder\":6,\"URL\":\"[11-6]-Back-Propagation\",\"title\":\"Back Propagation\",\"contentList\":[]}]},{\"includeHyphen\":false,\"firstOrder\":12,\"secondOrder\":-1,\"URL\":\"[12]-Q-Network\",\"title\":\"Q Network\",\"contentList\":[{\"includeHyphen\":true,\"firstOrder\":12,\"secondOrder\":1,\"URL\":\"[12-1]-Overview\",\"title\":\"Overview\",\"contentList\":[]},{\"includeHyphen\":true,\"firstOrder\":12,\"secondOrder\":2,\"URL\":\"[12-2]-Frozen-Lake\",\"title\":\"Frozen Lake\",\"contentList\":[]},{\"includeHyphen\":true,\"firstOrder\":12,\"secondOrder\":3,\"URL\":\"[12-3]-Cartpole\",\"title\":\"Cartpole\",\"contentList\":[]}]},{\"includeHyphen\":false,\"firstOrder\":13,\"secondOrder\":-1,\"URL\":\"[13]-DQN\",\"title\":\"DQN\",\"contentList\":[{\"includeHyphen\":true,\"firstOrder\":13,\"secondOrder\":1,\"URL\":\"[13-1]-Concept\",\"title\":\"Concept\",\"contentList\":[]},{\"includeHyphen\":true,\"firstOrder\":13,\"secondOrder\":2,\"URL\":\"[13-2]-Cartpole\",\"title\":\"Cartpole\",\"contentList\":[]}]},{\"includeHyphen\":false,\"firstOrder\":14,\"secondOrder\":-1,\"URL\":\"[14]-Policy-gradient\",\"title\":\"Policy gradient\",\"contentList\":[{\"includeHyphen\":true,\"firstOrder\":14,\"secondOrder\":1,\"URL\":\"[14-1]-A3C\",\"title\":\"A3C\",\"contentList\":[]}]}]}],[\"$\",\"aside\",null,{\"className\":\"\\n                hidden lg:flex w-64 2xl:w-96\\n                border-r-slate-300 dark:border-r-slate-600\\n                flex-col\\n                h-screen\\n                border-r-2\\n                p-1\\n                pl-5\\n                pt-14\\n                fixed\\n                overflow-y-auto\\n                overscroll-contain\\n                z-auto\\n                \",\"children\":[[\"$\",\"$L13\",\"0\",{\"href\":\"./[1]-Introduction\",\"label\":\"Introduction\"}],[\"$\",\"$L14\",\"1\",{\"label\":\"Basis math\",\"children\":[\"$\",\"$L15\",null,{\"contentList\":\"$9:props:children:0:props:res:1:contentList\"}]}],[\"$\",\"$L14\",\"2\",{\"label\":\"Q learning\",\"children\":[\"$\",\"$L15\",null,{\"contentList\":\"$9:props:children:0:props:res:2:contentList\"}]}],[\"$\",\"$L13\",\"3\",{\"href\":\"./[4]-Markov-process\",\"label\":\"Markov process\"}],[\"$\",\"$L13\",\"4\",{\"href\":\"./[5]-Bellman-Optimality-Equation\",\"label\":\"Bellman Optimality Equation\"}],[\"$\",\"$L13\",\"5\",{\"href\":\"./[6]-DP\",\"label\":\"DP\"}],[\"$\",\"$L13\",\"6\",{\"href\":\"./[7]-Monte-Carlo\",\"label\":\"Monte Carlo\"}],[\"$\",\"$L13\",\"7\",{\"href\":\"./[8]-Temporal-Difference-Learning\",\"label\":\"Temporal Difference Learning\"}],[\"$\",\"$L13\",\"8\",{\"href\":\"./[9]-nSTEP-Bootstrapping\",\"label\":\"nSTEP Bootstrapping\"}],[\"$\",\"$L14\",\"9\",{\"label\":\"Planning and Learning\",\"children\":[\"$\",\"$L15\",null,{\"contentList\":\"$9:props:children:0:props:res:9:contentList\"}]}],[\"$\",\"$L14\",\"10\",{\"label\":\"Deep learning\",\"children\":[\"$\",\"$L15\",null,{\"contentList\":\"$9:props:children:0:props:res:10:contentList\"}]}],[\"$\",\"$L14\",\"11\",{\"label\":\"Q Network\",\"children\":[\"$\",\"$L15\",null,{\"contentList\":\"$9:props:children:0:props:res:11:contentList\"}]}],[\"$\",\"$L14\",\"12\",{\"label\":\"DQN\",\"children\":[\"$\",\"$L15\",null,{\"contentList\":\"$9:props:children:0:props:res:12:contentList\"}]}],[\"$\",\"$L14\",\"13\",{\"label\":\"Policy gradient\",\"children\":[\"$\",\"$L15\",null,{\"contentList\":\"$9:props:children:0:props:res:13:contentList\"}]}]]}],[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$0:f:0:1:2:children:0\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]]}]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"div\",null,{\"className\":\"lg:ml-64 2xl:ml-96 mt-32 mb-32 flex-1 flex flex-col items-center overflow-x-hidden\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-11/12 lg:w-2/3 2xl:w-1/2 markdown-body\",\"children\":[[\"$\",\"h1\",null,{\"children\":\"Reinforcement Learning\",\"className\":\" text-3xl md:text-4xl font-extrabold my-6 text-transparent bg-clip-text bg-gradient-to-r from-blue-500 via-purple-500 to-pink-500 dark:from-blue-400 dark:via-purple-400 dark:to-pink-400\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"배고픈 고양이를 버튼을 눌러야 문이 열리는 특별한 상자에 가두고, 상자 밖에는 먹이를 두었다고 사정해 봅시다. 만약 고양이가 우연히 버튼을 누르게 되면 상자 밖으로 빠져나와 사료를 먹을수 있을겁니다. 이를 반복하게 되면, 고양이는 \",[\"$\",\"span\",null,{\"className\":\" text-rose-600 font-bold text-lg hover:text-rose-700 dark:text-rose-400 dark:hover:text-rose-500 transition-colors duration-200\",\"children\":\"버튼을 누르는 것이 긍정적인 결과를 낸다\"}],\" 를 학습하고 점점 더 상자에서 빠져나오는 시간이 짧아지게 될겁니다. 이것은, 버튼을 누르는 행동과 긍적적인 결과간의 관계가 강화(Reinforcement)되었다 라고 할수 있습니다. 이것이 바로 강화학습의 기본적인 개념입니다.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"이를 위에서 배웠던 용어로 다시 정리해 보겠습니다.\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"에이전트(Agent)\"}],\": 고양이\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"환경(Environment)\"}],\": 고양이를 제외한 모든것\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"상태(State)\"}],\": 고양이가 상자 안에 있는 상황, 상자 밖에 있는 상황 등 현재 환경의 상태\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"행동(Action)\"}],\": 버튼을 누르거나, 움직이거나 하는 등 에이전트가 선택할 수 있는 모든 행동들\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"정책(Policy)\"}],\": 특정 상태에서 어떤 행동을 선택할지 결정하는 전략 (예: 배고픈 상태에서 버튼을 누르는 행동을 선택)\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"보상(Reward)\"}],\": 행동의 결과로 얻는 피드백 (예: 먹이를 먹었을 때의 만족감)\"]}],\"\\n\"],\"className\":\"list-disc ml-12 space-y-2 my-3\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"추가적으로 강화학습의의 중요한 핵심적인 특징중 하나는, \",[\"$\",\"strong\",null,{\"children\":\"버튼을 누르면 왜 문이 열리는가?\"}],\" 와 같은 원리를 이해할 필요가 없습니다. 대신 \",[\"$\",\"strong\",null,{\"children\":\"이 상황(상태)에서 이 행동을 했더니 좋은 결과가 났다\"}],\"라는 사실만 알면 됩니다.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"아래 예시로 보여드릴 딥바인드에서 만든 게임 AI는 화면, 점수, 오른쪽 왼쪽밖에 모른다고 합니다. 이 AI는 화면에서 어떤 행동을 해야 좋은 결과를 얻는지를 학습하게 됩니다. 아래 예시에선 왼쪽, 오른쪽 밖에 없는 간단한 상황이지만, 운전시 핸들을 어떻게 돌려야 하는지 같은 각도 같은 연속적인 이러한 경우는 훨씬 어렵기 때문에 100000번, 20000번 시도해 우연히 성공하듯 학습하게 됩니다. 추가적으로 알파고 제로 또한 바둑이 뭔지 모르는 상태에서 학습을 시작합니다.\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Characteristics: Optimalization\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"강화학습은 최대의 보상을 얻을수 있는 최적의 정책을 학습하는 것이 목표 입니다.\\n예를들어, 체스에서는 승리하는 것이 최대의 보상이 될겁니다. 혹은 최단거리로 미로를 빠져 나가는 경로를 찾는 경우도 있을수 있거요.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"주요한 특징은, 현재 선택한 행동에\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"특징: 최적화\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"기본적으로 강화학습은 최적화를 푸는 문제\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"중요한거 어떤 가치가 있는지 명시적으로 표표현된다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"가와학습의 모델은 옭고 그름을 알지 못하고 어떤 보상상이 주어지는지다.\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"순차적\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"직관적으로 당연\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Delayed Consequences\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"체스를 예로들면 지금의 수가 당장은 손해더라도 끝까지 갔을때때의 보상이 큰 행도을 학습(승리)\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Exploration\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"손 했을때 손을 물었을때, 손으 올렸다면 머기가 주어졌단 사실을 모름(대부분)\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h1\",null,{\"children\":\"WhereReinforcementLearning?\",\"className\":\" text-3xl md:text-4xl font-extrabold my-6 text-transparent bg-clip-text bg-gradient-to-r from-blue-500 via-purple-500 to-pink-500 dark:from-blue-400 dark:via-purple-400 dark:to-pink-400\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"최적의정책을인간이알수없는경우\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"예.인간의능력을뛰어넘는최적의정책을찾기(AlphaGoZero등)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"예.기존에알려지지않은분야에서최적의정책을찾기(핵융합로플라즈마 제어 등)\"}],\"\\n\"],\"className\":\"list-disc ml-12 space-y-2 my-3\"}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"현재시점에서선택된행동의(최종적인)결과를나중에라도알수있는 경우\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"예.AtariBreakout:점수\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"예.체스나바둑:승패여부\"}],\"\\n\"],\"className\":\"list-disc ml-12 space-y-2 my-3\"}],\"\\n\"]}],\"\\n\"],\"className\":\"list-disc ml-12 space-y-2 my-3\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h1\",null,{\"children\":\"Workflow\",\"className\":\" text-3xl md:text-4xl font-extrabold my-6 text-transparent bg-clip-text bg-gradient-to-r from-blue-500 via-purple-500 to-pink-500 dark:from-blue-400 dark:via-purple-400 dark:to-pink-400\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Workflow를 보기 전 알아두어야 할점은 상태 하나하나 시간이 존재한다는것입니다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"./img/RL-workflow.png\",\"alt\":\"Reinforcement Learning Workflow\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"순서대로 살펴보겠습니다.\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"강화 학습 Agent는 환경의 상태를 인지 합니다.\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Environment -\u003e Agent:\"}],\" \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"S\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"s\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"S_{t} = s\"}]]}]}]}]]}],\"\\n\"],\"className\":\"list-disc ml-12 space-y-2 my-3\"}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"정책을 바탕으로 인지된 상태에 대한 행동을 선택\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"π\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"a\"}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"∣\"}],[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\pi(a | s)\"}]]}]}]}],\" -\u003e \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"A\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"a\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"A_{t} = a\"}]]}]}]}]]}],\"\\n\"],\"className\":\"list-disc ml-12 space-y-2 my-3\"}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"환경은 선택된 행동에 대해 보상을 에이전트에게 제공하고, 새로운 상태로 변경, 마치 바둑돌을 두면 환경이 변화 하는것과 같습니다.\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"R\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"t\"}],[\"$\",\"mo\",null,{\"children\":\"+\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}]]}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"r\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"R_{t + 1} = r\"}]]}]}]}],\", \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"S\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"t\"}],[\"$\",\"mo\",null,{\"children\":\"+\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}]]}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"mathvariant\":\"normal\",\"lspace\":\"0em\",\"rspace\":\"0em\",\"children\":\"′\"}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"S_{t + 1} = s'\"}]]}]}]}]]}],\"\\n\"],\"className\":\"list-disc ml-12 space-y-2 my-3\"}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"상태-행동-보상을 참고하여, 현재 정책의 가치를 평가하고 더 나은 정책을 학습\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"v\"}],[\"$\",\"mi\",null,{\"children\":\"π\"}]]}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"v_{\\\\pi}(s)\"}]]}]}]}],\" / \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"q\"}],[\"$\",\"mi\",null,{\"children\":\"π\"}]]}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"a\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"q_{\\\\pi}(s, a)\"}]]}]}]}]]}],\"\\n\"],\"className\":\"list-disc ml-12 space-y-2 my-3\"}],\"\\n\"]}],\"\\n\"],\"className\":\"list-decimal ml-12 space-y-2 my-3\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"요약: 어떤 관측한 상태에서 어떠한 행동을 하면 보상이 주어지고(양수, 음수 다 가능), 상태를 변화하고 업데이트\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"에이전트\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"환경과 상호작용하면서 주어진 목적을 달성할 수 있는 최적의 정책을 학습하는 주체 입니다. 학습된 정책을 바탕으로 주어진 상태에 대한 행동을 선택하고, 보상을 받습니다.\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"환경\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Agent와 상호작용 하되, Agent에 포함되지 않는 모든것을 의미합니다. Agent가 취하는 행동의 결과인 보상을 제공하며, 자신의 상태를 Agent에게 드러냅니다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"현실의 문제에서 환경을 전부 파악하는 경우는 거의 불가능에 가까우므로, Agent에게 드러내는 환경은 실제 환경과 다를수 있습니다.\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"상태\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"display\":\"block\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"S\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"s\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"S_{t} = s\\n\"}]]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Agent가 관측할 수 있는 정보를 바탕으로 인지된 환경의 상태를 의미합니다. 실제 환경의 상태와 Agent에 의해 인지된 상태는 (보통) 같지 않으나, 많은 경우 같다고 가정합니다.\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"행동\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"display\":\"block\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"A\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"a\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"A_{t} = a\\n\"}]]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"학습된 정책에 기반하여, 주어진 상태에서 선택한 의사결정을 의미합니다. Agent가 환경에게 전달하는 유일한 정보입니다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"행동은 이산적인 경우와 연속적인 경우로 나뉩니다. 이산적인 경우는 Breakout의 좌우 이동, 연속적인 경우는 앵그리버드의 발사 각도 조절이 있습니다.\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"보상\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"display\":\"block\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"R\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"t\"}],[\"$\",\"mo\",null,{\"children\":\"+\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}]]}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"r\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"R_{t + 1} = r\\n\"}]]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Agent가 환경에게 받는 피드백을 의미합니다. Agent가 선택한 행동에 대한 결과로 주어지며, 보상은 양수, 음수, 0 등 Scalar Value로 주어집니다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"예를들어, 체스말을 잃었을때 -1, 안잃었으면 0, 얻었으면 +1 인 경우가 있습니다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"또 ,트럭 시뮬레이션의 경우 시간당 -1을 주어진다면 빠르게 하는데에 집중해 모든 오브젝트에 박아대면서 주차할 수 있습니다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"어떤 예에선 게임 AI가 -1을 받지 않기 위해 승리(+1) 보다 게임을 멈추는 방법(0) 을 학습했다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"이러한 적절한 보상의 설게가 RL의 주요한 난제 입니다.\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"정책\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"상태를 입력으로, 출력을 행동으로 하는 함수로, 주어진 상태에 대한 Agent의 행동을 결정합니다.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"결정론적 정책(Deterministic Policy)\",\"className\":\" text-xl md:text-2xl font-bold my-4 text-green-600 hover:text-green-800 dark:text-green-400 dark:hover:text-green-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"display\":\"block\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"a\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"π\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"a = \\\\pi(s)\\n\"}]]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"결정적 정책에선 동일한 상태에 대해 동일한 행동을 하는 정책을 의미합니다.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"확률적 정책(Stochastic Policy)\",\"className\":\" text-xl md:text-2xl font-bold my-4 text-green-600 hover:text-green-800 dark:text-green-400 dark:hover:text-green-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"display\":\"block\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"π\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"a\"}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"∣\"}],[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\pi(a | s)\\n\"}]]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"상태에 대해 행동을할 확률을 출력으로 내는 정책을 의미합니다. 이는 동일한 상태에 대해 여러 행동을 선택할 수 있습니다.\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"수익(return)\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"display\":\"block\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"G\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"R\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}]]}],[\"$\",\"mo\",null,{\"children\":\"+\"}],[\"$\",\"mi\",null,{\"children\":\"γ\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"R\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"t\"}],[\"$\",\"mo\",null,{\"children\":\"+\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}]]}]]}],[\"$\",\"mo\",null,{\"children\":\"+\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"γ\"}],[\"$\",\"mn\",null,{\"children\":\"2\"}]]}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"R\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"t\"}],[\"$\",\"mo\",null,{\"children\":\"+\"}],[\"$\",\"mn\",null,{\"children\":\"2\"}]]}]]}],[\"$\",\"mo\",null,{\"children\":\"+\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"γ\"}],[\"$\",\"mn\",null,{\"children\":\"3\"}]]}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"R\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"t\"}],[\"$\",\"mo\",null,{\"children\":\"+\"}],[\"$\",\"mn\",null,{\"children\":\"3\"}]]}]]}],[\"$\",\"mo\",null,{\"children\":\"+\"}],[\"$\",\"mo\",null,{\"children\":\"⋯\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"G_{t} = R_{t} + {\\\\gamma}R_{t + 1} + {\\\\gamma}^2R_{t + 2} + {\\\\gamma}^3R_{t + 3} + \\\\cdots\\n\"}]]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"현재 시점부터 미래까지 받을 수 있는 보상의 누적합을 의미합니다. \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"γ\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\gamma\"}]]}]}]}],\"는 0 ~ 1값을 가지며 할인율을 의미합니다. 미래의 보상이 현재의 보상보다 얼마나 중요한지를 결정합니다. 현재 시점에 가까울 수록 보상의 가치가 크게 됩니다.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"강화학습의 Agent는 Return을 최대화 하는것을 목적으로 학습합니다. 정확히는 Expected Return을 최대화 하는것을 목적으로 합니다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"왜 Explected Return 이냐면, state와 action이 random variable이기 때문에, 어떤 행동을 했을때 어떤 보상또한 ranom variable이기 때문입니다.\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"가치 함수(Value Function)\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"정책의 가치를 평가하는 함수를 의미하고 2가지 종류가 있습니다. 가치란 특정 상태에서 얻을 수 있는 수익의 기대값을 의미합니다.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"상태 가치 함수(State Value Function)\",\"className\":\" text-xl md:text-2xl font-bold my-4 text-green-600 hover:text-green-800 dark:text-green-400 dark:hover:text-green-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"display\":\"block\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"v\"}],[\"$\",\"mi\",null,{\"children\":\"π\"}]]}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"E\"}],[\"$\",\"mi\",null,{\"children\":\"π\"}]]}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"[\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"G\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}]]}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"∣\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"S\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"]\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"v_{\\\\pi}(s) = E_{\\\\pi}[G_{t} | S_{t} = s]\\n\"}]]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"정책이 주어 졌을때, 주어진 상태에서 얻을 수 있는 수익의 기대값을 의미합니다.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"행동 가치 함수(Action Value Function)\",\"className\":\" text-xl md:text-2xl font-bold my-4 text-green-600 hover:text-green-800 dark:text-green-400 dark:hover:text-green-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"display\":\"block\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"q\"}],[\"$\",\"mi\",null,{\"children\":\"π\"}]]}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"a\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"E\"}],[\"$\",\"mi\",null,{\"children\":\"π\"}]]}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"[\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"G\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}]]}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"∣\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"S\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"A\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"a\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"]\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"q_{\\\\pi}(s, a) = E_{\\\\pi}[G_{t} | S_{t} = s, A_{t} = a]\\n\"}]]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"정책이 주어졌을때, 주어진 상태와 행동에서 얻을 수 있는 수익의 기대값을 의미합니다.\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"모델(Model)\",\"className\":\" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"display\":\"block\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"P\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"mathvariant\":\"normal\",\"lspace\":\"0em\",\"rspace\":\"0em\",\"children\":\"′\"}]]}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"r\"}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"∣\"}],[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"a\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"P\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"S\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"t\"}],[\"$\",\"mo\",null,{\"children\":\"+\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}]]}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"mathvariant\":\"normal\",\"lspace\":\"0em\",\"rspace\":\"0em\",\"children\":\"′\"}]]}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"R\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"t\"}],[\"$\",\"mo\",null,{\"children\":\"+\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}]]}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"r\"}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"∣\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"S\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"s\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"A\"}],[\"$\",\"mi\",null,{\"children\":\"t\"}]]}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"a\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"P(s', r | s, a) = P(S_{t + 1} = s', R_{t + 1} = r | S_{t} = s, A_{t} = a)\\n\"}]]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"이전에 실제 환경과 Agent에게 드러난 환경이 다르다고 했습니다. Model은 주어진 상태에서 Agent가 내린 행동에 대해, 환경의 다음 반응(보상 및 다음 상태)를 예층하는 함수 입니다.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"오직 관측된 정보를 바탕으로 추정하며, 실제 환경과 다를 수 있습니다. 또 Model은 RL Agent를 학습시키는 데 필수적인 요소는 아닙니다.\"}]]}],[\"$\",\"div\",null,{\"className\":\"\\n            flex\\n            flex-col sm:flex-row\\n            justify-between\\n            items-center\\n            w-full\\n            mt-8\\n            pt-4\\n            border-t-slate-300 dark:border-t-slate-600\\n            border-t-2\\n            w-11/12 lg:w-2/3 2xl:w-1/2\\n        \",\"children\":[[\"$\",\"div\",null,{\"className\":\"basis-1/2\"}],[\"$\",\"$L5\",null,{\"href\":\"[2-1]-Math-with-RL\",\"className\":\"\\n                w-full\\n                flex\\n                gap-2\\n                justify-end\\n                items-center\\n                font-medium\\n                py-2 sm:py-4\\n                px-2\\n                rounded-lg\\n                hover:bg-slate-300 dark:hover:bg-slate-600\\n            \",\"children\":[false,[\"$\",\"span\",null,{\"children\":\"Math with RL\"}],[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 512 512\",\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"d\":\"M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z\",\"children\":[]}]]],\"className\":\"$undefined\",\"style\":{\"color\":\"$undefined\"},\"height\":\"1em\",\"width\":\"1em\",\"xmlns\":\"http://www.w3.org/2000/svg\"}]]}]]}]]}]\n"])</script></body></html>