3:I[9275,[],""]
6:I[1343,[],""]
8:I[231,["217","static/chunks/578c2090-c1b891f3b6c746fd.js","675","static/chunks/b563f954-758761ebc4ecc2e7.js","699","static/chunks/8e1d74a4-44e18cb83de8b273.js","779","static/chunks/0e762574-0fedad6633d82a8a.js","231","static/chunks/231-87925b9c7247c60f.js","874","static/chunks/app/%5Bsubject%5D/layout-39e3ee08b07e17b8.js"],""]
4:["subject","ai","d"]
5:["post","%5B3%5D-Q-learning","d"]
0:["Ggd4W6vMAAu-nUgX-lKdS",[[["",{"children":[["subject","ai","d"],{"children":[["post","%5B3%5D-Q-learning","d"],{"children":["__PAGE__?{\"subject\":\"ai\",\"post\":\"[3]-Q-learning\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":[["subject","ai","d"],{"children":[["post","%5B3%5D-Q-learning","d"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children","$5","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/dd89e605550f760e.css","precedence":"next","crossOrigin":"$undefined"}]]}],null]},["$L7",null],null]},[["$","html",null,{"lang":"ko","children":["$","body",null,{"className":"__className_1e1d11 min-h-screen","children":["$","div",null,{"className":"grid grid-cols-[24rem_1fr] auto-rows-auto","children":[["$","nav",null,{"className":"bg-white w-full h-14 flex items-center justify-between pl-5 pr-5 mb-20 border-b-2 border-b-slate-300 text-2xl fixed","children":[["$","$L8",null,{"href":"/","children":"tomatoM4to's blog"}],["$","div",null,{"className":"hidden lg:flex items-center","children":[["$","input",null,{"type":"text","className":"w-36 h-7 rounded-full border-2 border-black pl-2","placeholder":"search"}],["$","div",null,{"className":"bg-slate-300 h-10 w-0.5 ml-2"}],["$","$L8",null,{"href":"https://github.com/tomatoM4to/tomatoM4to.github.io","className":"p-2 rounded-full hover:bg-gray-300 transition-colors duration-300","children":["$","svg",null,{"stroke":"currentColor","fill":"currentColor","strokeWidth":"0","viewBox":"0 0 16 16","className":"text-2xl cursor-pointer","children":["$undefined",[["$","path","0",{"fillRule":"evenodd","clipRule":"evenodd","d":"M7.976 0A7.977 7.977 0 0 0 0 7.976c0 3.522 2.3 6.507 5.431 7.584.392.049.538-.196.538-.392v-1.37c-2.201.49-2.69-1.076-2.69-1.076-.343-.93-.881-1.175-.881-1.175-.734-.489.048-.489.048-.489.783.049 1.224.832 1.224.832.734 1.223 1.859.88 2.3.685.048-.538.293-.88.489-1.076-1.762-.196-3.621-.881-3.621-3.964 0-.88.293-1.566.832-2.153-.05-.147-.343-.978.098-2.055 0 0 .685-.196 2.201.832.636-.196 1.322-.245 2.007-.245s1.37.098 2.006.245c1.517-1.027 2.202-.832 2.202-.832.44 1.077.146 1.908.097 2.104a3.16 3.16 0 0 1 .832 2.153c0 3.083-1.86 3.719-3.62 3.915.293.244.538.733.538 1.467v2.202c0 .196.146.44.538.392A7.984 7.984 0 0 0 16 7.976C15.951 3.572 12.38 0 7.976 0z","children":[]}]]],"style":{"color":"$undefined"},"height":"1em","width":"1em","xmlns":"http://www.w3.org/2000/svg"}]}],["$","button",null,{"className":"p-2 rounded-full hover:bg-gray-300 transition-colors duration-300","children":["$","svg",null,{"stroke":"currentColor","fill":"currentColor","strokeWidth":"0","viewBox":"0 0 24 24","className":"text-2xl cursor-pointer","children":["$undefined",[["$","path","0",{"fill":"none","strokeWidth":"2","d":"M12,23 C18.0751322,23 23,18.0751322 23,12 C23,5.92486775 18.0751322,1 12,1 C5.92486775,1 1,5.92486775 1,12 C1,18.0751322 5.92486775,23 12,23 Z M12,23 C15,23 16,18 16,12 C16,6 15,1 12,1 C9,1 8,6 8,12 C8,18 9,23 12,23 Z M2,16 L22,16 M2,8 L22,8","children":[]}]]],"style":{"color":"$undefined"},"height":"1em","width":"1em","xmlns":"http://www.w3.org/2000/svg"}]}],["$","button",null,{"className":"p-2 rounded-full hover:bg-gray-300 transition-colors duration-300","children":["$","svg",null,{"stroke":"currentColor","fill":"currentColor","strokeWidth":"0","viewBox":"0 0 16 16","className":"text-2xl cursor-pointer","children":["$undefined",[["$","path","0",{"d":"M6 .278a.77.77 0 0 1 .08.858 7.2 7.2 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277q.792-.001 1.533-.16a.79.79 0 0 1 .81.316.73.73 0 0 1-.031.893A8.35 8.35 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.75.75 0 0 1 6 .278","children":[]}],["$","path","1",{"d":"M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.73 1.73 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.73 1.73 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.73 1.73 0 0 0 1.097-1.097zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.16 1.16 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.16 1.16 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732z","children":[]}]]],"style":{"color":"$undefined"},"height":"1em","width":"1em","xmlns":"http://www.w3.org/2000/svg"}]}]]}]]}],["$","main",null,{"className":"col-span-2","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/ea9287ddd32ae283.css","precedence":"next","crossOrigin":"$undefined"}]]}]}]]}]}]}],null],null],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/40f13b8fccf4d106.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/10abd2b3b9bafb32.css","precedence":"next","crossOrigin":"$undefined"}]],"$L9"]]]]
9:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1, maximum-scale=1, user-scalable=yes"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"tomatom4to's Computer Science Blog"}],["$","meta","3",{"name":"description","content":"Comprehensive computer science knowledge covering OS, Database, AI, Networks, Linux, and Docker. Learn computer science concepts with clear explanations and practical examples."}],["$","meta","4",{"name":"author","content":"tomatom4to"}],["$","meta","5",{"name":"keywords","content":"Computer Science,Operating Systems,Database,AI,Network,Linux,Docker,Programming,Software Development"}],["$","meta","6",{"name":"creator","content":"tomatom4to"}],["$","meta","7",{"name":"robots","content":"index, follow"}],["$","meta","8",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","9",{"rel":"canonical","href":"https://tomatom4to.github.io"}],["$","meta","10",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","11",{"property":"og:title","content":"tomatom4to's Computer Science Blog"}],["$","meta","12",{"property":"og:description","content":"Your gateway to comprehensive computer science knowledge and practical programming skills"}],["$","meta","13",{"property":"og:url","content":"https://tomatom4to.github.io"}],["$","meta","14",{"property":"og:site_name","content":"tomatom4to's CS Blog"}],["$","meta","15",{"property":"og:locale","content":"ko_KR"}],["$","meta","16",{"property":"og:type","content":"website"}],["$","meta","17",{"name":"twitter:card","content":"summary"}],["$","meta","18",{"name":"twitter:title","content":"tomatom4to's Computer Science Blog"}],["$","meta","19",{"name":"twitter:description","content":"Your gateway to comprehensive computer science knowledge and practical programming skills"}],["$","link","20",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","meta","21",{"name":"next-size-adjust"}]]
1:null
a:I[1815,["217","static/chunks/578c2090-c1b891f3b6c746fd.js","675","static/chunks/b563f954-758761ebc4ecc2e7.js","699","static/chunks/8e1d74a4-44e18cb83de8b273.js","779","static/chunks/0e762574-0fedad6633d82a8a.js","231","static/chunks/231-87925b9c7247c60f.js","874","static/chunks/app/%5Bsubject%5D/layout-39e3ee08b07e17b8.js"],"Hamburger"]
7:["$","div",null,{"className":"flex","children":[["$","$La",null,{"res":[{"isOutLine":false,"firstOrder":1,"secondOrder":-1,"order":"1","title":"RL overview","originalName":"[1]-RL-overview"},{"isOutLine":false,"firstOrder":2,"secondOrder":-1,"order":"2","title":"Basis math","originalName":"[2]-Basis-math"},{"isOutLine":true,"firstOrder":2,"secondOrder":1,"order":"2-1","title":"Math with RL","originalName":"[2-1]-Math-with-RL"},{"isOutLine":false,"firstOrder":3,"secondOrder":-1,"order":"3","title":"Q learning","originalName":"[3]-Q-learning"},{"isOutLine":false,"firstOrder":4,"secondOrder":-1,"order":"4","title":"Markov process","originalName":"[4]-Markov-process"},{"isOutLine":false,"firstOrder":5,"secondOrder":-1,"order":"5","title":"Bellman Optimality Equation","originalName":"[5]-Bellman-Optimality-Equation"},{"isOutLine":false,"firstOrder":6,"secondOrder":-1,"order":"6","title":"DP","originalName":"[6]-DP"},{"isOutLine":false,"firstOrder":7,"secondOrder":-1,"order":"7","title":"Monte Carlo","originalName":"[7]-Monte-Carlo"},{"isOutLine":false,"firstOrder":8,"secondOrder":-1,"order":"8","title":"Temporal Difference Learning","originalName":"[8]-Temporal-Difference-Learning"},{"isOutLine":false,"firstOrder":9,"secondOrder":-1,"order":"9","title":"nSTEP Bootstrapping","originalName":"[9]-nSTEP-Bootstrapping"},{"isOutLine":false,"firstOrder":10,"secondOrder":-1,"order":"10","title":"Planning and Learning","originalName":"[10]-Planning-and-Learning"},{"isOutLine":true,"firstOrder":10,"secondOrder":1,"order":"10-1","title":"Function Approximation","originalName":"[10-1]-Function-Approximation"},{"isOutLine":false,"firstOrder":11,"secondOrder":-1,"order":"11","title":"Deep learning overview","originalName":"[11]-Deep-learning-overview"},{"isOutLine":true,"firstOrder":11,"secondOrder":1,"order":"11-1","title":"Gradient descent","originalName":"[11-1]-Gradient-descent"}],"params":{"subject":"ai"}}],["$","aside",null,{"className":"hidden lg:flex w-64 2xl:w-96 flex-col h-screen border-r-2 border-gray-300 mt-14 p-1 pl-5 fixed overflow-y-auto overscroll-contain","children":[["$","$L8",null,{"href":"/ai/[1]-RL-overview","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["1",". ","RL overview"]}],["$","$L8",null,{"href":"/ai/[2]-Basis-math","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["2",". ","Basis math"]}],["$","$L8",null,{"href":"/ai/[2-1]-Math-with-RL","className":"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["2-1",". ","Math with RL"]}],["$","$L8",null,{"href":"/ai/[3]-Q-learning","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["3",". ","Q learning"]}],["$","$L8",null,{"href":"/ai/[4]-Markov-process","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["4",". ","Markov process"]}],["$","$L8",null,{"href":"/ai/[5]-Bellman-Optimality-Equation","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["5",". ","Bellman Optimality Equation"]}],["$","$L8",null,{"href":"/ai/[6]-DP","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["6",". ","DP"]}],["$","$L8",null,{"href":"/ai/[7]-Monte-Carlo","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["7",". ","Monte Carlo"]}],["$","$L8",null,{"href":"/ai/[8]-Temporal-Difference-Learning","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["8",". ","Temporal Difference Learning"]}],["$","$L8",null,{"href":"/ai/[9]-nSTEP-Bootstrapping","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["9",". ","nSTEP Bootstrapping"]}],["$","$L8",null,{"href":"/ai/[10]-Planning-and-Learning","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["10",". ","Planning and Learning"]}],["$","$L8",null,{"href":"/ai/[10-1]-Function-Approximation","className":"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["10-1",". ","Function Approximation"]}],["$","$L8",null,{"href":"/ai/[11]-Deep-learning-overview","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["11",". ","Deep learning overview"]}],["$","$L8",null,{"href":"/ai/[11-1]-Gradient-descent","className":"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["11-1",". ","Gradient descent"]}]]}],["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}]]}]
b:T8f2,import random
import copy

def greedy_q_learning():
    # 4x4 맵 초기화, 각 위치마다 4개의 행동(상,하,좌,우)에 대한 Q값
    map = [[[0, 0, 0, 0] for _ in range(4)] for _ in range(4)]

    # 목표 지점의 Q값을 99로 설정
    map[3][3] = [99, 99, 99, 99]
    count = 0
    for episode in range(1000):
        state = [0, 0]  # 시작 위치
        cp_map = copy.deepcopy(map)  # 맵 변화 감지를 위한 복사

        while state != [3, 3]:  # 목표 도달까지 반복
            loc = map[state[0]][state[1]]

            # 현재 위치에서의 행동 선택
            if loc.count(0) == 4:  # 모든 Q값이 0이면 랜덤 선택
                action = random.randint(0, 3)
            else:  # 가장 큰 Q값을 가진 행동 선택
                action = loc.index(max(loc))

            # 이전 상태 저장
            prev_state = copy.deepcopy(state)

            # 선택한 행동으로 이동
            if action == 0 and state[0] > 0:  # 상
                state[0] -= 1
            elif action == 1 and state[0] < 3:  # 하
                state[0] += 1
            elif action == 2 and state[1] > 0:  # 좌
                state[1] -= 1
            elif action == 3 and state[1] < 3:  # 우
                state[1] += 1

            # 맵 밖으로 나가는 경우 처리
            if state[0] < 0:
                state[0] = 0
            elif state[0] > 3:
                state[0] = 3
            if state[1] < 0:
                state[1] = 0
            elif state[1] > 3:
                state[1] = 3

            # Q값 업데이트: Q값을 그대로 전파 함으로써, 한번 설정된 경로를 게속 따라가게 됩니다
            next_q_values = map[state[0]][state[1]]
            if max(next_q_values) > 0:
                map[prev_state[0]][prev_state[1]][action] = max(next_q_values)

            # 목표 도달 시
            if state == [3, 3]:
                map[prev_state[0]][prev_state[1]][action] = 99
                break

        # 맵이 변화하지 않을시 카운트
        if map == cp_map:
            count += 1
    return map, count

# 실행
q_map, count = greedy_q_learning()

# 결과 출력
print("Final Q-values:")
for row in q_map:
    print(row)

print("Count:", count)
c:Tc9b,import random
import copy

def decaying_epsilon_greedy_q_learning():
    # 6x6 맵 초기화
    map = [[[0, 0, 0, 0] for _ in range(6)] for _ in range(6)]

    # 두 개의 도착 지점 설정 (첫 번째: [3,3]=10, 두 번째: [5,5]=20)
    map[3][3] = [10, 10, 10, 10]  # 첫 번째 도착지점
    map[5][5] = [20, 20, 20, 20]  # 두 번째 도착지점 (더 큰 보상)

    # Epsilon 파라미터
    epsilon_start = 0.9
    epsilon_end = 0.01
    epsilon_decay = 0.995

    count = 0
    for episode in range(1000):
        state = [0, 0]  # 시작 위치
        cp_map = copy.deepcopy(map)

        # 현재 에피소드의 입실론 값 계산
        epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))

        # 도착지점에 도달할 때까지 반복
        while state != [3,3] and state != [5,5]:
            loc = map[state[0]][state[1]]

            # Epsilon-greedy 행동 선택
            if random.random() < epsilon:
                action = random.randint(0, 3)
            else:
                action = loc.index(max(loc))

            prev_state = copy.deepcopy(state)

            # 행동 실행
            if action == 0 and state[0] > 0:    # 상
                state[0] -= 1
            elif action == 1 and state[0] < 5:  # 하 (범위를 5로 변경)
                state[0] += 1
            elif action == 2 and state[1] > 0:  # 좌
                state[1] -= 1
            elif action == 3 and state[1] < 5:  # 우 (범위를 5로 변경)
                state[1] += 1

            # 경계 처리
            state[0] = max(0, min(state[0], 5))
            state[1] = max(0, min(state[1], 5))

            # Q값 업데이트
            next_q_values = map[state[0]][state[1]]
            if max(next_q_values) > 0:
                map[prev_state[0]][prev_state[1]][action] = max(next_q_values) - 1

            # 도착지점 도달시
            if state == [3, 3]:
                map[prev_state[0]][prev_state[1]][action] = 9  # 첫 번째 도착지점 직전
                break
            elif state == [5, 5]:
                map[prev_state[0]][prev_state[1]][action] = 19  # 두 번째 도착지점 직전
                break

        # 맵이 변화하지 않을시 카운트
        if map == cp_map:
            count += 1

    return map, count

# 실행
q_map, count = decaying_epsilon_greedy_q_learning()

# 결과 출력
print("Final Q-values:")
for row in q_map:
    print(row)
print("\nCount:", count)

# 최적 경로 출력
def print_optimal_path():
    state = [0, 0]
    path = [(0, 0)]

    # 두 도착지점 중 하나에 도달할 때까지
    while state != [3,3] and state != [5,5]:
        loc = q_map[state[0]][state[1]]
        action = loc.index(max(loc))

        if action == 0:    # 상
            state[0] -= 1
        elif action == 1:  # 하
            state[0] += 1
        elif action == 2:  # 좌
            state[1] -= 1
        elif action == 3:  # 우
            state[1] += 1

        state[0] = max(0, min(state[0], 5))
        state[1] = max(0, min(state[1], 5))
        path.append((state[0], state[1]))

    print("\nOptimal path:", path)
    print("Final destination:", state)

print_optimal_path()
2:["$","div",null,{"className":"lg:ml-64 2xl:ml-96 mt-32 mb-32 flex-1 flex flex-col items-center overflow-x-hidden","children":["$","div",null,{"className":"w-11/12 md:w-3/4 lg:w-2/3 2xl:w-1/2 markdown-body","children":[["$","h1",null,{"children":"Q-learning"}],"\n",["$","p",null,{"children":["먼저 Q-learning라는 알고리즘이 어떻게 동작하는지 살펴보면서 강화학습이란것이 어떤식으로 작동하는지 살펴보겠 습니다. ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mn",null,{"children":"4"}],["$","mo",null,{"children":"∗"}],["$","mn",null,{"children":"4"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"4*4"}]]}]}]}]," 격자 지도가 있다 가정해 보겠습니다. ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"("}],["$","mn",null,{"children":"0"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"0"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"(0, 0)"}]]}]}]}]," 에서 출발해 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"("}],["$","mn",null,{"children":"3"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"3"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"(3, 3)"}]]}]}]}],"에 도착해야 한다 가정해 보겠습니다."]}],"\n",["$","p",null,{"children":["이 지도를 처음부터 전부 알고 있다면 한번에 가장 빠른 길을 찾을 수 있겠지만, RL에선 보통 이 지도를 모른다고 가정합니다. 때문에, 바로 최적의 길을 찾는것이 아니라 아래에도 가보고 위에도 가보고 하는 과정을 거치다가 우연히 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"("}],["$","mn",null,{"children":"3"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"3"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"(3, 3)"}]]}]}]}],"에 도착하게 됩니다. 그러면서 이 경로를 점점 최적의 경로로 만들어 나가는 것이 RL의 핵심입니다. 핵심적인 부분은 지금 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"("}],["$","mn",null,{"children":"3"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"3"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"(3, 3)"}]]}]}]}]," 에 도착했다 라는 사실많을 알게 되는겁니다. 이렇게 하면 첫번째 episode가 끝나게 됩니다."]}],"\n",["$","p",null,{"children":"그리고 2번째, 3번째 episode를 거치면서 점점 최적의 경로를 찾아나가게 됩니다. 이렇게 최적의 경로를 찾아나가는 과정을 Q-learning이라고 합니다. Q-learning은 이러한 과정을 통해 최적의 경로를 찾아나가는 알고리즘입니다."}],"\n",["$","h2",null,{"children":"Greedy action"}],"\n",["$","p",null,{"children":"이러한 과정을 구현하기 위해 Q-learning에선 Greedy-action라는 행동을 합니다. 직역하면 탐욕적인 행동이라 할수 있는데요, 움직이는 각 경로마다 점수를 매기고 해당 점수가 가장 큰 방향으로 움직이는 것입니다."}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":["초기에 상하좌우로 이동할수 있는 격자지도는 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"["}],["$","mn",null,{"children":"0"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"0"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"0"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"0"}],["$","mo",null,{"stretchy":"false","children":"]"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"[0, 0, 0, 0]"}]]}]}]}]," 으로 초기화 되어 있습니다. 각각 상하좌우에 대한 값입니다. 이때는 전부 점수가 같은 상황이니 Random하게 행동하게 됩니다. 각 방향에 대해 0.25의 확률로 움직이게 됩니다."]}],"\n"],"className":"list-decimal ml-4"}],"\n",["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{}],["$","th",null,{}],["$","th",null,{}],["$","th",null,{}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"R = 1"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}]]}]]}]]}],"\n",["$","ol",null,{"start":"2","children":["\n",["$","li",null,{"children":["그러다가 우연히 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"("}],["$","mn",null,{"children":"3"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"3"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"(3, 3)"}]]}]}]}],"에 도착하게 된다면, Q-learning는 이곳에 무엇인가 있어! 라면서 헨젤과 그레텔처럼 무엇인가를 남기고 episode를 끝내게 됩니다. 이때는 위로 가는 방향에 대해 점수를 R로 매기게 됩니다. R은 Reward의 약자로 정수의 형태로 존재합니다. 그 이외인 좌, 우, 아래 방향에 대해는 아직 0점입니다."]}],"\n"],"className":"list-decimal ml-4"}],"\n",["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{}],["$","th",null,{}],["$","th",null,{}],["$","th",null,{}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"R = 1"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[R, 0, 0, 0]"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}]]}]]}]]}],"\n",["$","ol",null,{"start":"3","children":["\n",["$","li",null,{"children":["2번째 episode를 시작합니다. 이번에도 아직 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"("}],["$","mn",null,{"children":"3"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"1"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"(3, 1)"}]]}]}]}]," 에서 위로가는 방향에 대한 점수많이 R이고 그 이외의 나머지는 아직 0인 상태입니다. 그러다가 우연히 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"("}],["$","mn",null,{"children":"2"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"1"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"(2, 1)"}]]}]}]}]," 번째 좌표에 들려 또 우연히 오른쪽으로 이동했다 가정해 보겠습니다. 이렇게 되면 Q-learning는 0이 아닌 더 큰 값을 발견하게 됩니다. 이때 Q-learning는 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"("}],["$","mn",null,{"children":"3"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"1"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"(3, 1)"}]]}]}]}],"의 가장 큰값을 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"("}],["$","mn",null,{"children":"2"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"1"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"(2, 1)"}]]}]}]}],"에 매기게 됩니다. 이때는 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mo",null,{"stretchy":"false","children":"("}],["$","mn",null,{"children":"2"}],["$","mo",null,{"separator":"true","children":","}],["$","mn",null,{"children":"1"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"(2, 1)"}]]}]}]}],"에서 오른쪽으로 가는 방향에 대해 점수를 R로 매기게 됩니다. Q-learning의 중요한 특징을 여기서 관찰할수 있는데요, Q-learning은 이동을 하면서 바로바로 업데이트 합니다. 그렇기 때문에 에피소드가 끝나기 전에도 업데이트를 수행 합니다. 오른쪽으로 이동을 함과 동시에 오른쪽으로 갔을때의 형가를 실시간으로 업데이트 합니다. 이렇든 이 판을 만드는 과정이 Q-learning이고 이러한 값을 Q-value라고 합니다."]}],"\n"],"className":"list-decimal ml-4"}],"\n",["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{}],["$","th",null,{}],["$","th",null,{}],["$","th",null,{}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"R = 1"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, R]"}],["$","td",null,{"children":"[R, 0, 0, 0]"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}]]}]]}]]}],"\n",["$","ol",null,{"start":"4","children":["\n",["$","li",null,{"children":"이런식으로 episode를 지속한다면 최종적으로 이러한 방향이 나오게 됩니다."}],"\n"],"className":"list-decimal ml-4"}],"\n",["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{}],["$","th",null,{}],["$","th",null,{}],["$","th",null,{}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"[0, R, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"R = 1"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, R]"}],["$","td",null,{"children":"[0, 0, 0, R]"}],["$","td",null,{"children":"[0, 0, 0, R]"}],["$","td",null,{"children":"[R, 0, 0, 0]"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}]]}]]}]]}],"\n",["$","h3",null,{"children":"구현"}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"overflow-x-auto bg-gray-200 rounded-md px-1 language-python","children":"$b"}],"className":"bg-gray-200 rounded-md px-1 py-2"}],"\n",["$","hr",null,{"className":"border-t-4 border-gray-300 mt-10 mb-10","children":"$undefined"}],"\n",["$","p",null,{"children":["하지만 이러한 문제가 있습니다. 이렇게 경로를 찾아나가는 과정을 거치다 보면, 도착은 할수 있겠지만 이 경로가 최적의 경로가 아닐 확률이 매우 높습니다. 또한 최적이 아닌 경로를 계속 따라가게 됩니다. 해당 코드의 count 변수의 값을 보면 경로를 찾은 뒤에도 계속 같은 경로로 이동하는 모습을 볼수 있습니다. 이러한 문제를 해결하기 위해 Q-learning에선 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"-greedy action을 사용합니다."]}],"\n",["$","h1",null,{"children":[["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"-greedy action"]}],"\n",["$","p",null,{"children":["greedy 방식에선 같은 경로를 계속 따라가게 되는 문제가 있습니다. 이를 해결하기 위해 탐험(Exploration) 이라는 개념을 도입한 방법입니다. 탐험이란 간단히 말해, 더 좋은길이 있을지도 모르니까 가보는 것입니다. 이제 탐험을 하기 위해 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"-greedy action을 배워봅시다."]}],"\n",["$","p",null,{"children":[["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"이 무엇이냐면 0과 1 사이의 값을 의미합니다. 대충 0.1이라 가정해 보겠습니다. 이때 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"-greedy action은 90%의 확률로 가장 큰 값을 가진 방향으로 이동하고, 10%의 확률로 랜덤하게 이동하는 것입니다. 이렇게 하면 랜덤하게 이동하면서 더 좋은 길이 있을지도 모르니까 가보는 것입니다."]}],"\n",["$","p",null,{"children":["만약 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}]," 이 1이라면? 100%의 확률로 랜덤하게 이동하게 됩니다. 이렇게 돼면, 기존에 찾았던 경로를 기반으로 더 좋은길을 유도 해줘야 하는데 유도해주지 못하는 상황이 발생합니다. 이렇듯 너무 높은 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"값은 문제가 될수 있습니다. 반대로 너무 낮은 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"값은 탐험을 하지 않아 최적의 경로를 찾지 못할수 있습니다. 이러한 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"값을 조절하는 것이 중요합니다."]}],"\n",["$","p",null,{"children":[["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}]," 을 이해했으니 이제 관련 용어를 알아보겠습니다. Exploration과 Exploitation입니다. Exploration은 탐험을 의미하고, Exploitation은 현재 가장 좋은 방향을 선택하는 것을 의미합니다. Exploitation은 Q값을 이용해 그리디하게 움직인단 의미이기도 합니다."]}],"\n",["$","p",null,{"children":"역시 Exploration과 Exploitation은은 trade-off 관계입니다. Exploration을 많이 하면 더 좋은 길을 찾을수 있지만, 그만큼 시간이 오래 걸립니다. 반대로 Exploitation을 많이 하면 빠르게 최적의 경로를 찾을수 있지만, 그만큼 더 좋은 길을 찾지 못할수 있습니다. 이러한 trade-off 관계를 잘 조절하는 것이 중요합니다."}],"\n",["$","p",null,{"children":"Exploration의 두가지 장점"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"새로운 path 찾기 가능"}],"\n",["$","li",null,{"children":["새로운 goal 찾기 가능","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"강화학습에선 map의 상태를 알지 못한다 했습니다. 기존엔 간단한 예시를 위해 한곳에 goal을 두었지만 실제로는 여러곳에 더 좋은 goal이 있을수 있습니다. 이때 탐험을 통해 새로운 goal을 찾을수 있습니다."}],"\n"],"className":"list-disc ml-4"}],"\n"]}],"\n"],"className":"list-disc ml-4"}],"\n",["$","p",null,{"children":"하지만 너무 탐험만 한다면 판때기를 이용하지 못합니다."}],"\n",["$","h2",null,{"children":["Decaying ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"-greedy action"]}],"\n",["$","p",null,{"children":["한번 생각해 봅시다. 그 어떤 정보도 없다면, 탐험을 하는게 좋을까요 아니면 Exploitation을 하는게 좋을까요? 보통 탐험을 하는것이 좋습니다. 그러나 탐험을 너무 많이 한다면, 최적의 경로를 찾지 못할수 있습니다. 하지만 이 두개는 trade-off 관계이디고 합니다. 이 전에는 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"을 고정된 값으로 사용했지만, 이번에는 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"을 점점 줄여가는 방법을 사용해보겠습니다. 이를 Decaying ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"이라고 합니다. 단순히 말해 처음엔 탐험을 많이 하다 나중엔 최적화에 더 많은 비중을 두는 것입니다."]}],"\n",["$","p",null,{"children":[["$","span",null,{"style":{"color":"red"},"children":"이떄 줄여나가는 기준은 episode를 기준으로 합니다."}]," 이 방식은 Q-learning에서 많이 사용되는 방법입니다. 그러므로써 trade-off 관계를 잘 조절할수 있습니다."]}],"\n",["$","h3",null,{"children":"구현"}],"\n",["$","pre",null,{"children":["$","code",null,{"className":"overflow-x-auto bg-gray-200 rounded-md px-1 language-python","children":"$c"}],"className":"bg-gray-200 rounded-md px-1 py-2"}],"\n",["$","p",null,{"children":"이전관 다르게 6x6으로 확장되었고, 두개의 도착지점이 추가되었습니다. 이때 첫번째 도착지점은 10점, 두번째 도착지점은 20점을 가지고 있습니다. 이때 두번째 도착지점이 더 높은 점수를 가지고 있습니다. 그리고 경로를 추적하는 코드가 추가되었습니다."}],"\n",["$","h2",null,{"children":"Discount factor"}],"\n",["$","p",null,{"children":"위 예제에서 어느정도 구현이 됐었습니다. Discount factor은 결국 더 효율적이게 path를 찾기 위해 고안된 방법입니다. 코드가 아니라 아까 배웠던 이론을 생각해 봅시다. 우린 이때까지 다음상태의 최대값을 현재상태에 업데이트 했습니다. 근데 이건 뭔가 이상합니다. 상하좌우로 움직일때 더 좋고 안좋고가 없이 무조건 같은 값으로 업데이트를 한다고 이론적으로 학습했습니다. Discount factor은 이러한 문제를 해결하기 위해 고안된 방법입니다."}],"\n",["$","p",null,{"children":["Discount factor 은 기호로 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"γ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\gamma"}]]}]}]}],"로 표현합니다. 이는 0과 1사이의 값을 가집니다. 이는 상태를 업데이트 할때 다음 상태의 최대값 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"γ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\gamma"}]]}]}]}],"을 곱해줍니다. 이는 다음 상태의 최대값을 현재 상태에 업데이트 할때 그 값이 얼마나 중요한지를 나타냅니다. 이를 통해 더 효율적인 path를 찾을수 있습니다."]}],"\n",["$","p",null,{"children":["처음엔 그대로 복사를 합니다. 하지만 그 다음 업데이트 할때는 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"γ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\gamma"}]]}]}]}],"를 곱해 업데이트 해라! 라는 겁니다."]}],"\n",["$","p",null,{"children":"한번 위 4x4 예제로 살펴보겠습니다."}],"\n",["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{}],["$","th",null,{}],["$","th",null,{}],["$","th",null,{}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["[0, ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"R"}],["$","msup",null,{"children":[["$","mi",null,{"children":"γ"}],["$","mn",null,{"children":"4"}]]}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R\\gamma^4"}]]}]}]}],", 0, ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"R"}],["$","msup",null,{"children":[["$","mi",null,{"children":"γ"}],["$","mn",null,{"children":"2"}]]}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R\\gamma^2"}]]}]}]}],"]"]}],["$","td",null,{"children":["[0, 0, 0, ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"R"}],["$","mi",null,{"children":"γ"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R\\gamma"}]]}]}]}],"]"]}],["$","td",null,{"children":["[0, 0, 0, ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"R"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R"}]]}]}]}],"]"]}],["$","td",null,{"children":["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"R"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R"}]]}]}]}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["[0, 0, 0, ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"R"}],["$","msup",null,{"children":[["$","mi",null,{"children":"γ"}],["$","mn",null,{"children":"3"}]]}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R\\gamma^3"}]]}]}]}],"]"]}],["$","td",null,{"children":["[0, 0, 0, ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"R"}],["$","msup",null,{"children":[["$","mi",null,{"children":"γ"}],["$","mn",null,{"children":"2"}]]}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R\\gamma^2"}]]}]}]}],"]"]}],["$","td",null,{"children":["[0, 0, 0, ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"R"}],["$","mi",null,{"children":"γ"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R\\gamma"}]]}]}]}],"]"]}],["$","td",null,{"children":["[",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"R"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R"}]]}]}]}],", 0, 0, 0]"]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}],["$","td",null,{"children":"[0, 0, 0, 0]"}]]}]]}]]}],"\n",["$","p",null,{"children":["결과적으로 어떻게 될까요? 첫 시작점은 (0, 0)에서 가장 큰값인 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"R"}],["$","msup",null,{"children":[["$","mi",null,{"children":"γ"}],["$","mn",null,{"children":"2"}]]}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R\\gamma^2"}]]}]}]}]," 값을 가진 방향으로 나아가 결국 최적의 경로를 찾게 될것입니다."]}],"\n",["$","h3",null,{"children":"Discount factor 값에 따른 결과"}],"\n",["$","p",null,{"children":["한번 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"γ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\gamma"}]]}]}]}],"값이 작다고 가정해 보겠습니다. 그렇다면 위 그림에서 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"R"}],["$","mi",null,{"children":"γ"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R\\gamma"}]]}]}]}],"으로 가야 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"R"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R"}]]}]}]}]," 상태에 도달하지만, ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"R"}],["$","mi",null,{"children":"γ"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R\\gamma"}]]}]}]}],"이 작다면 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"R"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R"}]]}]}]}],"으로 가는것이 어려울수 있습니다. 하지만 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"R"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R"}]]}]}]}],"상태에선 아! 이방향으로 가면 보상이 기다리는구나! 라면서 확실히 이동할겁니다. 지금 당장 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"R"}],["$","mi",null,{"children":"γ"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R\\gamma"}]]}]}]}]," 로 가도.. 보상은 없겠지.. 한번더 움직여야 하잖아? 라고 생각하는것과 비슷합니다. 이걸 흔히 현재지향적인 행동이라고 합니다. 미래에 큰 보상이 있더라도 지금 당장 현재를 더 중요하게 생각하는겁니다."]}],"\n",["$","p",null,{"children":["반대로 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"γ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\gamma"}]]}]}]}],"값이 크다면? ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"R"}],["$","mi",null,{"children":"γ"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R\\gamma"}]]}]}]}],"으로 가는것을 더 중요하게 생각할겁니다. 현실로 대입해보면 더 가야할순 있을거 같은데 그래도 거의 다왔다! 라고 생각하고 그 방향으로 가는것과 비슷합니다. 호들갑을 떨면서 가는것이라고 생각하면 됩니다. 이러한 행동을 미래지향적인 행동이라고 합니다. 지금 현재 받을 보상보다 미래에 받을 보상을 더 중요하게 생각하는 효과가 생깁니다."]}],"\n",["$","hr",null,{"className":"border-t-4 border-gray-300 mt-10 mb-10","children":"$undefined"}],"\n",["$","h1",null,{"children":"Q-update"}]]}]}]
