ref: https://ai-sinq.tistory.com/entry/Bellman-Equation%EB%B2%A8%EB%A7%8C-%EB%B0%A9%EC%A0%95%EC%8B%9D

# RL workflow 복습

***

# 벨만 방정식(Bellman Equation)
TODO: 좀더 자세히 써야..

벨만 방정식은 시점 t에서의 Value와 시점 t + 1에서의 Value 사이의 관계를 다루며, 가치 함수와 정책 함수 사이의 관계도 다루고 있습니다. 먼저 벨만 기대 방정식과 벨만 최적 방정식을 구해보겠습니다.


## 벨만 기대 방정식(Bellman Expectation Equation)
벨만 기대 방정식은 현재 상태의 가치함수와 다음 상태의 가치함수 사이의 관계를 식으로 나타낸 것입니다. 벨만 기대 방정식은 가치함수식에서 유도된 것인데 과정은 다음과 같습니다.

TODO: 유도과정 추가 필요
```math
v_{\pi}(s_t) = E_{\pi}[r_{t+1} + \gamma v_{\pi}(s_{t+1})]
```

행동 가치 함수도 위와 같이 유도 됩니다.
```math
q_{\pi}(s_t, a_t) = E_{\pi}[r_{t+1} + \gamma q_{\pi}(s_{t+1}, a_{t+1})]
```







***
***
***
# 정책
지금까지 정책을 많이 있는것처럼 말했지만 입입실론-Greedy 정책의, 결정적 정책과 확률적 정책이 전부라고 할수 있다.

이중ㅇ서 greedy 정책을의외로 많이 쓴데, 대부분

결정적 정책에서 argmax(qx...) 를 보면 지금까지 저누 배웠지만 q를 모른다.

즉 q를 학습하는것이 최적의 정책을 학습하는거다??? 이 줄은 애매하다..


Gt(액센 벨류 펑션) Gt + 1 는 다르다. Gt + 1 을 Gt(액션 벨류 펑션션 으로 바꿔줘야 한다.

정책에 대해 오해해할수 이는는게 정책은 어디까지나 탐험험이고 자의적으로 끝날수 있는게 아님, 탐험이 끝났다면 선택 정책으로? 바꿔야함

***

# 최적
이론적으로 존재하는 최적 상태 즉 참인상태로 실제로 우리가 알수 있는건 아님, 이 최적 상태를 이론적으로 알기 위해 학습을 하는거기도 함