# 상태 가치 함수 & 행동 가치 함수
이 두가지 함수를 한문장으로 요약하면 이겁니다 **Expected return**을 잘 표현하기 위한 함수입니다.

* State Value Function: 지금 이 순간, 지금 이 State 로부터 시작해서 기대되는 Return 값
    * 현재 state가 주어져 있을때 Return을 계산하는것, 또는 지금 현재 State에 대한 평가(가치)를 내리는거라 볼수도 있습니다.
* Action Value Function: 지금 State에서 지금 어떠한 행동으로부터 기대되는 Return
    * 이전 Q-learning 시간에서 이동하는 행동에 대해 점수를 매겼늗데. 이것이 Action Value Function 입니다. 즉 Q의 정체입니다.
* Optimal Policy: State Value Function 값을 최대화 Policy, 과거는 잊고 지금부터 기대되는 Return을 최대화 하는것, 과거는 지금 시점에서 잘했다 치고 미래를 계산하는것 입니다. 딴예기지만 과거미래현재중 어떤게 중요할까요, 현재부터 잘하자란 예기입니다.

## State Value Function
개념은 저게 전부입니다. 이제부턴 수식적으로 이해해보겠습니다.

**Return** 의 정의부터 알아보겠습니다.
```math
G_t = R_t + R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k}
```

Expectation을 구하는 공식을 다시한번 기억해 보겠습니다. $E[x]$은 도수(frequency)에 확률을 곱해서 더한것입니다. 다음은 $x$에 대한 기대값을 구하는 공식입니다.
```math
E[X] = \sum_{x} x P(x)
```

이제 이 **Return**에 기대값을 취해보겠습니다.
```math
V_{\pi}(s_t) = \sum_{}^{\infty} G_t * P(a_t, s_{t+1}, a_{t+1}, s_{t+2}, \dots | s_t)
```
해당 공식을 부분적으로 해석해 보겠습니다.
* $P(a_t, s_{t+1}, a_{t+1}, s_{t+2}, \dots | s_t)$: $s_t$는 현재 State를 나타냅니다. 현재 State 에서 $a_t$라는 행동을 했고 이 행동은 $s_{t+1}$로 이동했습니다. 그리고 $a_{t+1}$로 행동을 했습니다. $\infty$ 는 끝까지 같다, 에피소드가 끝났다 라는 뜻입니다.
* $G_t$: 현재 State에서 시작해서 끝까지의 Return 값입니다. 위에서 설명한 state아 action들은 전부 random variable 입니다. 그래서 이들의 모든 행동, 어떤 상태에서 이행동도 해보고 저행동도 해보고, 저상태에서 이행동도 해보고 저행동도 해보고.. 할수있는 action, 나올수 있는 state에 대한 모든걸 전부 계산해서 나온 Return에 대한 값 입니다.

이 모든것들을 전부 더해서 나온것이 **State Value Function** 의 값입니다. 이것은 **Expected Return**이고, 지금부터 미래까지의 Return을 계산한것입니다.

## Action Value Function
아까는 $s_t$에 대한 함수였다면 이번에는 $s_t, a_t$에 대한 함수입니다. 현재 State에서 어떤 행동을 했을때의 Return 값을 계산하기 위함이기 때문입니다.

$Q$를 앞에다 놨는데 Q-learning에서 배웠던 Q값들의 개념이 바로 이것입니다. Q-learning에서는 이 Q값을 학습하는것이 목표였습니다.

```math
Q_{\pi}(s_t, a_t) = \sum_{}^{\infty} G_t * P(s_{t+1}, a_{t+1}, s_{t+2}, a_{t+2} \dots | s_t, a_t)
```

위와 거의 동일합니다. 볼수 있듯이 $s_t, a_t$에 대한 함수입니다. 위에것은 현재 State에서 진행항것이고, 이것은 현재 State에서 어떤 행동을 했을때 쭉 진행을 해본 기댓값 입니다.

***

ref: https://ai-sinq.tistory.com/entry/Bellman-Equation%EB%B2%A8%EB%A7%8C-%EB%B0%A9%EC%A0%95%EC%8B%9D

# 벨만 방정식(Bellman Equation)
벨만 방정식을 요약하면 다음과 같습니다. $V_{\pi}(s_t)$ 를 $Q_{\pi}(s_t, a_t)$ 로 표현하는것, 혹은 그 반대, 아니면 $V_{\pi}(s_t)$를 $V_{\pi}(s_{t+1})$로 표현하는것, 혹은 $Q_{\pi}(s_t, a_t)$를 $Q_{\pi}(s_{t+1}, a_{t+1})$로 표현하는것입니다. 이것이 바로 벨만 방정식에 대한 간단한 요약 입니다.


## $V_{\pi}(s_t)$ to $Q_{\pi}(s_t, a_t)$
TODO: 모라는거야
```math
v_\pi(s) = \sum_{a \in A} \pi(a|s) q_\pi(s, a)
```

이전 Q-learning를 예로들어 설명해보면, 한 격자에 Q값이 상하좌우 4개 있었죠? 그 값들의 평균치가 그 상태, 그 격자에서부터 기대되는 기댓값, 그 상태에서부터 기대되는 기대값이 되는겁니다.


***
# Optimal Policy





***
***
***
# 정책
지금까지 정책을 많이 있는것처럼 말했지만 입입실론-Greedy 정책의, 결정적 정책과 확률적 정책이 전부라고 할수 있다.

이중ㅇ서 greedy 정책을의외로 많이 쓴데, 대부분

결정적 정책에서 argmax(qx...) 를 보면 지금까지 저누 배웠지만 q를 모른다.

즉 q를 학습하는것이 최적의 정책을 학습하는거다??? 이 줄은 애매하다..


Gt(액센 벨류 펑션) Gt + 1 는 다르다. Gt + 1 을 Gt(액션 벨류 펑션션 으로 바꿔줘야 한다.

정책에 대해 오해해할수 이는는게 정책은 어디까지나 탐험험이고 자의적으로 끝날수 있는게 아님, 탐험이 끝났다면 선택 정책으로? 바꿔야함

***

# 최적
이론적으로 존재하는 최적 상태 즉 참인상태로 실제로 우리가 알수 있는건 아님, 이 최적 상태를 이론적으로 알기 위해 학습을 하는거기도 함