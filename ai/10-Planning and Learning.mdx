* Experience Sampling: O -> 검은공 -> O 이거 하나가 경험이라 생각하고, 학습할때 그냥 이거 하나 뽑아서 쓰는

* Time?

* Decision Tome Planning
  * 알파고에 쓰는것
  * 정책이 필요 없음? 어떤 로봇이 있고 우로간다, 좌로간다가 있을때, 고려하는건 그냥 거리가 가깝냐임 이게 휴이스틱?
  * Model가지고 시뮬레이션 하고끝까지 하는거임, 그래서 받은 보상을 평균내면..? 그래서 매번 수익을 알수 있음(가짜일순 있음) 그래서 가치함수를 학습하지 않음, 쉽게마해 모든 지점에 몬테카를로를 쓰는거라 생각해도 됨, 엄밀하게 말하면 MCTS 긴 한데..
  
  
# Dyna-Q
모델을 만든다? 사실 AI나 모델이나 결국 함수기 때문에 그냥 만드면 됌, 물론 정해진 방법도 있긴 함
  
n개의 상태에 대한 가치함수를 업데이트 하니 연산은더빠른데 더 빠르게 최저긔 정책을 학습 가능또 에를들어 추천기능을 만들때 10번물업는걸 100번물어  보는것처럼 바꿀수 있음
  
# 모델이 정확하지 않다면?
모델이 중간에 바뀌면 문제가 생기는데, 안좋게 바뀌면 오히려 괜찮은데 더 환경에 대해선 좋은건 못찾음, 그래서 과거르 기반으로 다시 학습?

목표값과 과거의 추정치가 일치하면 더이상 가치 함수의 값 업데이트 x, 목저이 같게, 반대로 크면 바꿔야함, 우선순위큐에서 큰거 뽑아서 그거가ㅏ지고 업데이트 할수고 있음

모델 대충 그냥 배열로 해서 하면 됨, 하기 싫으면 리스트만들어서 대충 랜덤으로 뽑아, 확률분포 나오니까