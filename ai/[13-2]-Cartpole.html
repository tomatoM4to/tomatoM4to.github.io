<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=yes"/><link rel="preload" href="/_next/static/media/5455839c73f146e7-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/7626ed2c039b2726-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="./img/dqn-reward-history.png"/><link rel="stylesheet" href="/_next/static/css/98b6e1014510d512.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/e09042d6d37504f5.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/dd89e605550f760e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-d7dace7cd354c2cf.js"/><script src="/_next/static/chunks/fd9d1056-62aaf4b921c84028.js" async=""></script><script src="/_next/static/chunks/23-a9892337a8234d4f.js" async=""></script><script src="/_next/static/chunks/main-app-ce18d4723c8629f4.js" async=""></script><script src="/_next/static/chunks/578c2090-c1b891f3b6c746fd.js" async=""></script><script src="/_next/static/chunks/b563f954-758761ebc4ecc2e7.js" async=""></script><script src="/_next/static/chunks/8e1d74a4-44e18cb83de8b273.js" async=""></script><script src="/_next/static/chunks/0e762574-0fedad6633d82a8a.js" async=""></script><script src="/_next/static/chunks/231-87925b9c7247c60f.js" async=""></script><script src="/_next/static/chunks/app/%5Btheme%5D/layout-41c0f24cb97d52a9.js" async=""></script><title>tomatom4to&#x27;s Computer Science Blog</title><meta name="description" content="Comprehensive computer science knowledge covering OS, Database, AI, Networks, Linux, and Docker. Learn computer science concepts with clear explanations and practical examples."/><meta name="author" content="tomatom4to"/><meta name="keywords" content="Computer Science,Operating Systems,Database,AI,Network,Linux,Docker,Programming,Software Development"/><meta name="creator" content="tomatom4to"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://tomatom4to.github.io"/><meta name="format-detection" content="telephone=no, address=no, email=no"/><meta property="og:title" content="tomatom4to&#x27;s Computer Science Blog"/><meta property="og:description" content="Your gateway to comprehensive computer science knowledge and practical programming skills"/><meta property="og:url" content="https://tomatom4to.github.io"/><meta property="og:site_name" content="tomatom4to&#x27;s CS Blog"/><meta property="og:locale" content="ko_KR"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="tomatom4to&#x27;s Computer Science Blog"/><meta name="twitter:description" content="Your gateway to comprehensive computer science knowledge and practical programming skills"/><link rel="icon" href="/icon.ico?3c4912ce8b26c1d6" type="image/x-icon" sizes="256x256"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_098cd6 min-h-screen"><div class="grid grid-cols-[24rem_1fr] auto-rows-auto"><nav class="__className_92d895 bg-white w-full h-14 flex items-center justify-between pl-5 pr-5 mb-20 border-b-2 border-b-slate-300 text-2xl fixed"><a href="/">tomatoM4to&#x27;s blog</a><div class="hidden lg:flex items-center"><input type="text" class="w-36 h-7 rounded-full border-2 border-black pl-2" placeholder="search"/><div class="bg-slate-300 h-10 w-0.5 ml-2"></div><a class="p-2 rounded-full hover:bg-gray-300 transition-colors duration-300" href="https://github.com/tomatoM4to/tomatoM4to.github.io"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" class="text-2xl cursor-pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M7.976 0A7.977 7.977 0 0 0 0 7.976c0 3.522 2.3 6.507 5.431 7.584.392.049.538-.196.538-.392v-1.37c-2.201.49-2.69-1.076-2.69-1.076-.343-.93-.881-1.175-.881-1.175-.734-.489.048-.489.048-.489.783.049 1.224.832 1.224.832.734 1.223 1.859.88 2.3.685.048-.538.293-.88.489-1.076-1.762-.196-3.621-.881-3.621-3.964 0-.88.293-1.566.832-2.153-.05-.147-.343-.978.098-2.055 0 0 .685-.196 2.201.832.636-.196 1.322-.245 2.007-.245s1.37.098 2.006.245c1.517-1.027 2.202-.832 2.202-.832.44 1.077.146 1.908.097 2.104a3.16 3.16 0 0 1 .832 2.153c0 3.083-1.86 3.719-3.62 3.915.293.244.538.733.538 1.467v2.202c0 .196.146.44.538.392A7.984 7.984 0 0 0 16 7.976C15.951 3.572 12.38 0 7.976 0z"></path></svg></a><button class="p-2 rounded-full hover:bg-gray-300 transition-colors duration-300"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="text-2xl cursor-pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill="none" stroke-width="2" d="M12,23 C18.0751322,23 23,18.0751322 23,12 C23,5.92486775 18.0751322,1 12,1 C5.92486775,1 1,5.92486775 1,12 C1,18.0751322 5.92486775,23 12,23 Z M12,23 C15,23 16,18 16,12 C16,6 15,1 12,1 C9,1 8,6 8,12 C8,18 9,23 12,23 Z M2,16 L22,16 M2,8 L22,8"></path></svg></button><button class="p-2 rounded-full hover:bg-gray-300 transition-colors duration-300"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" class="text-2xl cursor-pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M6 .278a.77.77 0 0 1 .08.858 7.2 7.2 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277q.792-.001 1.533-.16a.79.79 0 0 1 .81.316.73.73 0 0 1-.031.893A8.35 8.35 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.75.75 0 0 1 6 .278"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.73 1.73 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.73 1.73 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.73 1.73 0 0 0 1.097-1.097zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.16 1.16 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.16 1.16 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732z"></path></svg></button></div></nav><main class="col-span-2"><div class="flex"><aside class="lg:hidden"><button class=" flex flex-col gap-1 justify-center items-center w-10 h-10 rounded-lg fixed right-2 top-2 active:outline-none p-2 hover:bg-gray-200 transition-all" style="z-index:15"><span class="w-6 h-0.5 bg-black rounded transform transition-transform duration-300 ease-in-out "></span><span class="w-6 h-0.5 bg-black rounded transition-opacity duration-300 ease-in-out opacity-100"></span><span class="w-6 h-0.5 bg-black rounded transform transition-transform duration-300 ease-in-out "></span></button><div class="
                fixed
                top-0
                right-0
                flex
                flex-col
                px-5
                h-full
                w-7/12
                bg-white
                shadow-lg
                transform
                transition-transform
                duration-300
                ease-in-out
                translate-x-full
                " style="z-index:10"><nav class="h-14 flex items-center border-b-2"><a class="p-2 rounded-full hover:bg-gray-300 transition-colors duration-300" href="https://github.com/tomatoM4to/tomatoM4to.github.io"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" class="text-2xl cursor-pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M7.976 0A7.977 7.977 0 0 0 0 7.976c0 3.522 2.3 6.507 5.431 7.584.392.049.538-.196.538-.392v-1.37c-2.201.49-2.69-1.076-2.69-1.076-.343-.93-.881-1.175-.881-1.175-.734-.489.048-.489.048-.489.783.049 1.224.832 1.224.832.734 1.223 1.859.88 2.3.685.048-.538.293-.88.489-1.076-1.762-.196-3.621-.881-3.621-3.964 0-.88.293-1.566.832-2.153-.05-.147-.343-.978.098-2.055 0 0 .685-.196 2.201.832.636-.196 1.322-.245 2.007-.245s1.37.098 2.006.245c1.517-1.027 2.202-.832 2.202-.832.44 1.077.146 1.908.097 2.104a3.16 3.16 0 0 1 .832 2.153c0 3.083-1.86 3.719-3.62 3.915.293.244.538.733.538 1.467v2.202c0 .196.146.44.538.392A7.984 7.984 0 0 0 16 7.976C15.951 3.572 12.38 0 7.976 0z"></path></svg></a><button class="p-2 rounded-full hover:bg-gray-300 transition-colors duration-300"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="text-xl cursor-pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill="none" stroke-width="2" d="M12,23 C18.0751322,23 23,18.0751322 23,12 C23,5.92486775 18.0751322,1 12,1 C5.92486775,1 1,5.92486775 1,12 C1,18.0751322 5.92486775,23 12,23 Z M12,23 C15,23 16,18 16,12 C16,6 15,1 12,1 C9,1 8,6 8,12 C8,18 9,23 12,23 Z M2,16 L22,16 M2,8 L22,8"></path></svg></button><button class="p-2 rounded-full hover:bg-gray-300 transition-colors duration-300"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" class="text-xl cursor-pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M6 .278a.77.77 0 0 1 .08.858 7.2 7.2 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277q.792-.001 1.533-.16a.79.79 0 0 1 .81.316.73.73 0 0 1-.031.893A8.35 8.35 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.75.75 0 0 1 6 .278"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.73 1.73 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.73 1.73 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.73 1.73 0 0 0 1.097-1.097zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.16 1.16 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.16 1.16 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732z"></path></svg></button><button class="p-2 rounded-full hover:bg-gray-300 transition-colors duration-300"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="text-xl cursor-pointer" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"></path></svg></button></nav><div class="pb-5 flex flex-col overflow-y-auto overscroll-contain"><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[1]-Introduction">1<!-- -->. <!-- -->Introduction</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[2]-Basis-math">2<!-- -->. <!-- -->Basis math</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[2-1]-Math-with-RL">2-1<!-- -->. <!-- -->Math with RL</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[3]-Q-learning">3<!-- -->. <!-- -->Q learning</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[3-1]-Basic-Concept">3-1<!-- -->. <!-- -->Basic Concept</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[3-2]-Greedy-action">3-2<!-- -->. <!-- -->Greedy action</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[3-3]-Discount-factor">3-3<!-- -->. <!-- -->Discount factor</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[3-4]-Learning-rate">3-4<!-- -->. <!-- -->Learning rate</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[4]-Markov-process">4<!-- -->. <!-- -->Markov process</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[5]-Bellman-Optimality-Equation">5<!-- -->. <!-- -->Bellman Optimality Equation</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[6]-DP">6<!-- -->. <!-- -->DP</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[7]-Monte-Carlo">7<!-- -->. <!-- -->Monte Carlo</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[8]-Temporal-Difference-Learning">8<!-- -->. <!-- -->Temporal Difference Learning</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[9]-nSTEP-Bootstrapping">9<!-- -->. <!-- -->nSTEP Bootstrapping</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[10]-Planning-and-Learning">10<!-- -->. <!-- -->Planning and Learning</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[10-1]-Function-Approximation">10-1<!-- -->. <!-- -->Function Approximation</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11]-Deep-learning">11<!-- -->. <!-- -->Deep learning</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11-1]-Concept">11-1<!-- -->. <!-- -->Concept</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11-2]-Newerl-Network">11-2<!-- -->. <!-- -->Newerl Network</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11-3]-Loss-function">11-3<!-- -->. <!-- -->Loss function</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11-4]-Activation-function">11-4<!-- -->. <!-- -->Activation function</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11-5]-Gradient-descent">11-5<!-- -->. <!-- -->Gradient descent</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11-6]-Back-Propagation">11-6<!-- -->. <!-- -->Back Propagation</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[12]-Q-Network">12<!-- -->. <!-- -->Q Network</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[12-1]-Frozen-Lake">12-1<!-- -->. <!-- -->Frozen Lake</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[12-2]-Cartpole">12-2<!-- -->. <!-- -->Cartpole</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[13]-DQN">13<!-- -->. <!-- -->DQN</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[13-1]-Concept">13-1<!-- -->. <!-- -->Concept</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[13-2]-Cartpole">13-2<!-- -->. <!-- -->Cartpole</a></div></div><div class="
        w-screen
        h-screen
        bg-white
        blur-lg
        fixed
        left-0
        top-0
        transition-opacity
        duration-300
        opacity-0 pointer-events-none" style="z-index:5"></div></aside><aside class="hidden lg:flex w-64 2xl:w-96 flex-col h-screen border-r-2 border-gray-300 mt-14 p-1 pl-5 fixed overflow-y-auto overscroll-contain"><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[1]-Introduction">1<!-- -->. <!-- -->Introduction</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[2]-Basis-math">2<!-- -->. <!-- -->Basis math</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[2-1]-Math-with-RL">2-1<!-- -->. <!-- -->Math with RL</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[3]-Q-learning">3<!-- -->. <!-- -->Q learning</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[3-1]-Basic-Concept">3-1<!-- -->. <!-- -->Basic Concept</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[3-2]-Greedy-action">3-2<!-- -->. <!-- -->Greedy action</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[3-3]-Discount-factor">3-3<!-- -->. <!-- -->Discount factor</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[3-4]-Learning-rate">3-4<!-- -->. <!-- -->Learning rate</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[4]-Markov-process">4<!-- -->. <!-- -->Markov process</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[5]-Bellman-Optimality-Equation">5<!-- -->. <!-- -->Bellman Optimality Equation</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[6]-DP">6<!-- -->. <!-- -->DP</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[7]-Monte-Carlo">7<!-- -->. <!-- -->Monte Carlo</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[8]-Temporal-Difference-Learning">8<!-- -->. <!-- -->Temporal Difference Learning</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[9]-nSTEP-Bootstrapping">9<!-- -->. <!-- -->nSTEP Bootstrapping</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[10]-Planning-and-Learning">10<!-- -->. <!-- -->Planning and Learning</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[10-1]-Function-Approximation">10-1<!-- -->. <!-- -->Function Approximation</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11]-Deep-learning">11<!-- -->. <!-- -->Deep learning</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11-1]-Concept">11-1<!-- -->. <!-- -->Concept</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11-2]-Newerl-Network">11-2<!-- -->. <!-- -->Newerl Network</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11-3]-Loss-function">11-3<!-- -->. <!-- -->Loss function</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11-4]-Activation-function">11-4<!-- -->. <!-- -->Activation function</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11-5]-Gradient-descent">11-5<!-- -->. <!-- -->Gradient descent</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[11-6]-Back-Propagation">11-6<!-- -->. <!-- -->Back Propagation</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[12]-Q-Network">12<!-- -->. <!-- -->Q Network</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[12-1]-Frozen-Lake">12-1<!-- -->. <!-- -->Frozen Lake</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[12-2]-Cartpole">12-2<!-- -->. <!-- -->Cartpole</a><a class="undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[13]-DQN">13<!-- -->. <!-- -->DQN</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[13-1]-Concept">13-1<!-- -->. <!-- -->Concept</a><a class="pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg" href="/ai/[13-2]-Cartpole">13-2<!-- -->. <!-- -->Cartpole</a></aside><div class="lg:ml-64 2xl:ml-96 mt-32 mb-32 flex-1 flex flex-col items-center overflow-x-hidden"><div class="w-11/12 md:w-3/4 lg:w-2/3 2xl:w-1/2 markdown-body"><pre class="bg-gray-200 rounded-md px-1 py-2"><code class="overflow-x-auto bg-gray-200 rounded-md px-1 language-python"><span class="pl-c"># gym 행동 공간에서 행동의 숫자를 얻습니다.</span>
n_actions <span class="pl-k">=</span> env.action_space.n
<span class="pl-c"># 상태 관측 횟수를 얻습니다.</span>
state, info <span class="pl-k">=</span> env.reset()
n_observations <span class="pl-k">=</span> <span class="pl-c1">len</span>(state)

<span class="pl-c1">print</span>(n_actions, state) <span class="pl-c"># 2, [ 0.03645075  0.04290273 -0.00414692 -0.0414    ]</span>
<span class="pl-c1">print</span>(n_observations) <span class="pl-c"># 4</span>
</code></pre>
<p><code class="overflow-x-auto bg-gray-200 rounded-md px-1 undefined">[-0.03755258 0.0260832 0.045742 -0.00931699]</code>는 벡터입니다. 이 벡터는 CartPole-v1 환경의 상태를 나타내는 네 개의 값으로 구성되어 있습니다. 각 값은 환경의 특정 속성을 나타냅니다:</p>
<ul class="list-disc ml-4">
<li>카트의 위치</li>
<li>카트의 속도</li>
<li>폴의 각도</li>
<li>폴의 각속도</li>
</ul>
<p>이 값들은 환경이 초기화될 때 반환된 상태를 나타내며, 벡터 형태로 표현됩니다.</p>
<hr class="border-t-4 border-gray-300 mt-10 mb-10"/>
<h1>전체 코드</h1>
<p><a href="https://tutorials.pytorch.kr/intermediate/reinforcement_q_learning.html">https://tutorials.pytorch.kr/intermediate/reinforcement_q_learning.html</a></p>
<pre class="bg-gray-200 rounded-md px-1 py-2"><code class="overflow-x-auto bg-gray-200 rounded-md px-1 language-python"><span class="pl-k">import</span> gymnasium <span class="pl-k">as</span> gym
<span class="pl-k">import</span> math
<span class="pl-k">import</span> random
<span class="pl-k">import</span> matplotlib.pyplot <span class="pl-k">as</span> plt
<span class="pl-k">from</span> collections <span class="pl-k">import</span> namedtuple, deque
<span class="pl-k">from</span> itertools <span class="pl-k">import</span> count

<span class="pl-k">import</span> torch
<span class="pl-k">import</span> torch.nn <span class="pl-k">as</span> nn
<span class="pl-k">import</span> torch.optim <span class="pl-k">as</span> optim
<span class="pl-k">import</span> torch.nn.functional <span class="pl-k">as</span> F

env <span class="pl-k">=</span> gym.make(<span class="pl-s"><span class="pl-pds">&quot;</span>CartPole-v1<span class="pl-pds">&quot;</span></span>)

plt.ion()

<span class="pl-c"># GPU를 사용할 경우</span>
device <span class="pl-k">=</span> torch.device(<span class="pl-s"><span class="pl-pds">&quot;</span>cuda<span class="pl-pds">&quot;</span></span> <span class="pl-k">if</span> torch.cuda.is_available() <span class="pl-k">else</span> <span class="pl-s"><span class="pl-pds">&quot;</span>cpu<span class="pl-pds">&quot;</span></span>)


Transition <span class="pl-k">=</span> namedtuple(<span class="pl-s"><span class="pl-pds">&#x27;</span>Transition<span class="pl-pds">&#x27;</span></span>,
                        (<span class="pl-s"><span class="pl-pds">&#x27;</span>state<span class="pl-pds">&#x27;</span></span>, <span class="pl-s"><span class="pl-pds">&#x27;</span>action<span class="pl-pds">&#x27;</span></span>, <span class="pl-s"><span class="pl-pds">&#x27;</span>next_state<span class="pl-pds">&#x27;</span></span>, <span class="pl-s"><span class="pl-pds">&#x27;</span>reward<span class="pl-pds">&#x27;</span></span>))


<span class="pl-k">class</span> <span class="pl-en">ReplayMemory</span>(<span class="pl-c1">object</span>):

    <span class="pl-k">def</span> <span class="pl-c1">__init__</span>(<span class="pl-smi">self</span>, <span class="pl-smi">capacity</span>):
        <span class="pl-c1">self</span>.memory <span class="pl-k">=</span> deque([], <span class="pl-v">maxlen</span><span class="pl-k">=</span>capacity)

    <span class="pl-k">def</span> <span class="pl-en">push</span>(<span class="pl-smi">self</span>, <span class="pl-k">*</span><span class="pl-smi">args</span>):
        <span class="pl-s"><span class="pl-pds">&quot;&quot;&quot;</span>transition 저장<span class="pl-pds">&quot;&quot;&quot;</span></span>
        <span class="pl-c1">self</span>.memory.append(Transition(<span class="pl-k">*</span>args))

    <span class="pl-k">def</span> <span class="pl-en">sample</span>(<span class="pl-smi">self</span>, <span class="pl-smi">batch_size</span>):
        <span class="pl-k">return</span> random.sample(<span class="pl-c1">self</span>.memory, batch_size)

    <span class="pl-k">def</span> <span class="pl-c1">__len__</span>(<span class="pl-smi">self</span>):
        <span class="pl-k">return</span> <span class="pl-c1">len</span>(<span class="pl-c1">self</span>.memory)

<span class="pl-k">class</span> <span class="pl-en">DQN</span>(<span class="pl-e">nn</span>.<span class="pl-e">Module</span>):

    <span class="pl-k">def</span> <span class="pl-c1">__init__</span>(<span class="pl-smi">self</span>, <span class="pl-smi">n_observations</span>, <span class="pl-smi">n_actions</span>):
        <span class="pl-c1">super</span>(<span class="pl-c1">DQN</span>, <span class="pl-c1">self</span>).<span class="pl-c1">__init__</span>()
        <span class="pl-c1">self</span>.layer1 <span class="pl-k">=</span> nn.Linear(n_observations, <span class="pl-c1">128</span>)
        <span class="pl-c1">self</span>.layer2 <span class="pl-k">=</span> nn.Linear(<span class="pl-c1">128</span>, <span class="pl-c1">128</span>)
        <span class="pl-c1">self</span>.layer3 <span class="pl-k">=</span> nn.Linear(<span class="pl-c1">128</span>, n_actions)

    <span class="pl-c"># 최적화 중에 다음 행동을 결정하기 위해서 하나의 요소 또는 배치를 이용해 호촐됩니다.</span>
    <span class="pl-c"># ([[left0exp,right0exp]...]) 를 반환합니다.</span>
    <span class="pl-k">def</span> <span class="pl-en">forward</span>(<span class="pl-smi">self</span>, <span class="pl-smi">x</span>):
        x <span class="pl-k">=</span> F.relu(<span class="pl-c1">self</span>.layer1(x))
        x <span class="pl-k">=</span> F.relu(<span class="pl-c1">self</span>.layer2(x))
        <span class="pl-k">return</span> <span class="pl-c1">self</span>.layer3(x)

<span class="pl-c"># BATCH_SIZE는 리플레이 버퍼에서 샘플링된 트랜지션의 수입니다.</span>
<span class="pl-c"># GAMMA는 이전 섹션에서 언급한 할인 계수입니다.</span>
<span class="pl-c"># EPS_START는 엡실론의 시작 값입니다.</span>
<span class="pl-c"># EPS_END는 엡실론의 최종 값입니다.</span>
<span class="pl-c"># EPS_DECAY는 엡실론의 지수 감쇠(exponential decay) 속도 제어하며, 높을수록 감쇠 속도가 느립니다.</span>
<span class="pl-c"># TAU는 목표 네트워크의 업데이트 속도입니다.</span>
<span class="pl-c"># LR은 ``AdamW`` 옵티마이저의 학습율(learning rate)입니다.</span>
<span class="pl-c1">BATCH_SIZE</span> <span class="pl-k">=</span> <span class="pl-c1">128</span>
<span class="pl-c1">GAMMA</span> <span class="pl-k">=</span> <span class="pl-c1">0.99</span>
<span class="pl-c1">EPS_START</span> <span class="pl-k">=</span> <span class="pl-c1">0.9</span>
<span class="pl-c1">EPS_END</span> <span class="pl-k">=</span> <span class="pl-c1">0.05</span>
<span class="pl-c1">EPS_DECAY</span> <span class="pl-k">=</span> <span class="pl-c1">1000</span>
<span class="pl-c1">TAU</span> <span class="pl-k">=</span> <span class="pl-c1">0.005</span>
<span class="pl-c1">LR</span> <span class="pl-k">=</span> <span class="pl-c1">1e-4</span>

<span class="pl-c"># gym 행동 공간에서 행동의 숫자를 얻습니다.</span>
n_actions <span class="pl-k">=</span> env.action_space.n
<span class="pl-c"># 상태 관측 횟수를 얻습니다.</span>
state, info <span class="pl-k">=</span> env.reset()
n_observations <span class="pl-k">=</span> <span class="pl-c1">len</span>(state)

policy_net <span class="pl-k">=</span> DQN(n_observations, n_actions).to(device)
target_net <span class="pl-k">=</span> DQN(n_observations, n_actions).to(device)
target_net.load_state_dict(policy_net.state_dict())

optimizer <span class="pl-k">=</span> optim.AdamW(policy_net.parameters(), <span class="pl-v">lr</span><span class="pl-k">=</span><span class="pl-c1">LR</span>, <span class="pl-v">amsgrad</span><span class="pl-k">=</span><span class="pl-c1">True</span>)
memory <span class="pl-k">=</span> ReplayMemory(<span class="pl-c1">10000</span>)


steps_done <span class="pl-k">=</span> <span class="pl-c1">0</span>


<span class="pl-k">def</span> <span class="pl-en">select_action</span>(<span class="pl-smi">state</span>):
    <span class="pl-k">global</span> steps_done
    sample <span class="pl-k">=</span> random.random()
    eps_threshold <span class="pl-k">=</span> <span class="pl-c1">EPS_END</span> <span class="pl-k">+</span> (<span class="pl-c1">EPS_START</span> <span class="pl-k">-</span> <span class="pl-c1">EPS_END</span>) <span class="pl-k">*</span> \
        math.exp(<span class="pl-k">-</span><span class="pl-c1">1</span>. <span class="pl-k">*</span> steps_done <span class="pl-k">/</span> <span class="pl-c1">EPS_DECAY</span>)
    steps_done <span class="pl-k">+=</span> <span class="pl-c1">1</span>
    <span class="pl-k">if</span> sample <span class="pl-k">&gt;</span> eps_threshold:
        <span class="pl-k">with</span> torch.no_grad():
            <span class="pl-c"># t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.</span>
            <span class="pl-c"># 최대 결과의 두번째 열은 최대 요소의 주소값이므로,</span>
            <span class="pl-c"># 기대 보상이 더 큰 행동을 선택할 수 있습니다.</span>
            <span class="pl-k">return</span> policy_net(state).max(<span class="pl-c1">1</span>).indices.view(<span class="pl-c1">1</span>, <span class="pl-c1">1</span>)
    <span class="pl-k">else</span>:
        <span class="pl-k">return</span> torch.tensor([[env.action_space.sample()]], <span class="pl-v">device</span><span class="pl-k">=</span>device, <span class="pl-v">dtype</span><span class="pl-k">=</span>torch.long)


<span class="pl-k">def</span> <span class="pl-en">optimize_model</span>():
    <span class="pl-k">if</span> <span class="pl-c1">len</span>(memory) <span class="pl-k">&lt;</span> <span class="pl-c1">BATCH_SIZE</span>:
        <span class="pl-k">return</span>
    transitions <span class="pl-k">=</span> memory.sample(<span class="pl-c1">BATCH_SIZE</span>)
    <span class="pl-c"># Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for</span>
    <span class="pl-c"># detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로</span>
    <span class="pl-c"># 전환합니다.</span>
    batch <span class="pl-k">=</span> Transition(<span class="pl-k">*</span><span class="pl-c1">zip</span>(<span class="pl-k">*</span>transitions))

    <span class="pl-c"># 최종이 아닌 상태의 마스크를 계산하고 배치 요소를 연결합니다</span>
    <span class="pl-c"># (최종 상태는 시뮬레이션이 종료 된 이후의 상태)</span>
    non_final_mask <span class="pl-k">=</span> torch.tensor(<span class="pl-c1">tuple</span>(<span class="pl-c1">map</span>(<span class="pl-k">lambda</span> <span class="pl-smi">s</span>: s <span class="pl-k">is</span> <span class="pl-k">not</span> <span class="pl-c1">None</span>,
                                          batch.next_state)), <span class="pl-v">device</span><span class="pl-k">=</span>device, <span class="pl-v">dtype</span><span class="pl-k">=</span>torch.bool)
    non_final_next_states <span class="pl-k">=</span> torch.cat([s <span class="pl-k">for</span> s <span class="pl-k">in</span> batch.next_state
                                                <span class="pl-k">if</span> s <span class="pl-k">is</span> <span class="pl-k">not</span> <span class="pl-c1">None</span>])
    state_batch <span class="pl-k">=</span> torch.cat(batch.state)
    action_batch <span class="pl-k">=</span> torch.cat(batch.action)
    reward_batch <span class="pl-k">=</span> torch.cat(batch.reward)

    <span class="pl-c"># Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.</span>
    <span class="pl-c"># 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.</span>
    state_action_values <span class="pl-k">=</span> policy_net(state_batch).gather(<span class="pl-c1">1</span>, action_batch)

    <span class="pl-c"># 모든 다음 상태를 위한 V(s_{t+1}) 계산</span>
    <span class="pl-c"># non_final_next_states의 행동들에 대한 기대값은 &quot;이전&quot; target_net을 기반으로 계산됩니다.</span>
    <span class="pl-c"># max(1).values로 최고의 보상을 선택하십시오.</span>
    <span class="pl-c"># 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.</span>
    next_state_values <span class="pl-k">=</span> torch.zeros(<span class="pl-c1">BATCH_SIZE</span>, <span class="pl-v">device</span><span class="pl-k">=</span>device)
    <span class="pl-k">with</span> torch.no_grad():
        next_state_values[non_final_mask] <span class="pl-k">=</span> target_net(non_final_next_states).max(<span class="pl-c1">1</span>).values
    <span class="pl-c"># 기대 Q 값 계산</span>
    expected_state_action_values <span class="pl-k">=</span> (next_state_values <span class="pl-k">*</span> <span class="pl-c1">GAMMA</span>) <span class="pl-k">+</span> reward_batch

    <span class="pl-c"># Huber 손실 계산</span>
    criterion <span class="pl-k">=</span> nn.SmoothL1Loss()
    loss <span class="pl-k">=</span> criterion(state_action_values, expected_state_action_values.unsqueeze(<span class="pl-c1">1</span>))

    <span class="pl-c"># 모델 최적화</span>
    optimizer.zero_grad()
    loss.backward()
    <span class="pl-c"># 변화도 클리핑 바꿔치기</span>
    torch.nn.utils.clip_grad_value_(policy_net.parameters(), <span class="pl-c1">100</span>)
    optimizer.step()

<span class="pl-k">if</span> torch.cuda.is_available():
    num_episodes <span class="pl-k">=</span> <span class="pl-c1">600</span>
<span class="pl-k">else</span>:
    num_episodes <span class="pl-k">=</span> <span class="pl-c1">50</span>


rewards_history <span class="pl-k">=</span> []

<span class="pl-k">for</span> i_episode <span class="pl-k">in</span> <span class="pl-c1">range</span>(num_episodes):
    <span class="pl-c"># 환경과 상태 초기화</span>
    state, info <span class="pl-k">=</span> env.reset()
    state <span class="pl-k">=</span> torch.tensor(state, <span class="pl-v">dtype</span><span class="pl-k">=</span>torch.float32, <span class="pl-v">device</span><span class="pl-k">=</span>device).unsqueeze(<span class="pl-c1">0</span>)
    episode_reward <span class="pl-k">=</span> <span class="pl-c1">0</span>
    <span class="pl-k">for</span> t <span class="pl-k">in</span> count():
        action <span class="pl-k">=</span> select_action(state)
        observation, reward, terminated, truncated, _ <span class="pl-k">=</span> env.step(action.item())
        episode_reward <span class="pl-k">+=</span> reward
        reward <span class="pl-k">=</span> torch.tensor([reward], <span class="pl-v">device</span><span class="pl-k">=</span>device)
        done <span class="pl-k">=</span> terminated <span class="pl-k">or</span> truncated

        <span class="pl-k">if</span> terminated:
            next_state <span class="pl-k">=</span> <span class="pl-c1">None</span>
        <span class="pl-k">else</span>:
            next_state <span class="pl-k">=</span> torch.tensor(observation, <span class="pl-v">dtype</span><span class="pl-k">=</span>torch.float32, <span class="pl-v">device</span><span class="pl-k">=</span>device).unsqueeze(<span class="pl-c1">0</span>)

        <span class="pl-c"># 메모리에 변이 저장</span>
        memory.push(state, action, next_state, reward)

        <span class="pl-c"># 다음 상태로 이동</span>
        state <span class="pl-k">=</span> next_state

        <span class="pl-c"># (정책 네트워크에서) 최적화 한단계 수행</span>
        optimize_model()

        <span class="pl-c"># 목표 네트워크의 가중치를 소프트 업데이트</span>
        <span class="pl-c"># θ′ ← τ θ + (1 −τ )θ′</span>
        target_net_state_dict <span class="pl-k">=</span> target_net.state_dict()
        policy_net_state_dict <span class="pl-k">=</span> policy_net.state_dict()
        <span class="pl-k">for</span> key <span class="pl-k">in</span> policy_net_state_dict:
            target_net_state_dict[key] <span class="pl-k">=</span> policy_net_state_dict[key]<span class="pl-k">*</span><span class="pl-c1">TAU</span> <span class="pl-k">+</span> target_net_state_dict[key]<span class="pl-k">*</span>(<span class="pl-c1">1</span><span class="pl-k">-</span><span class="pl-c1">TAU</span>)
        target_net.load_state_dict(target_net_state_dict)

        <span class="pl-k">if</span> done:
            <span class="pl-k">break</span>
    <span class="pl-k">if</span> i_episode <span class="pl-k">%</span> <span class="pl-c1">50</span> <span class="pl-k">==</span> <span class="pl-c1">0</span>:
        <span class="pl-c1">print</span>(<span class="pl-s"><span class="pl-k">f</span><span class="pl-pds">&quot;</span>Episode </span><span class="pl-c1">{</span>i_episode<span class="pl-c1">}</span><span class="pl-s">, Reward: </span><span class="pl-c1">{</span>episode_reward<span class="pl-c1">}</span><span class="pl-s"><span class="pl-pds">&quot;</span></span>)
    rewards_history.append(episode_reward)

<span class="pl-c1">print</span>(<span class="pl-s"><span class="pl-pds">&#x27;</span>Complete<span class="pl-pds">&#x27;</span></span>)
plt.plot(<span class="pl-c1">range</span>(<span class="pl-c1">len</span>(rewards_history)), rewards_history, <span class="pl-v">color</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">&quot;</span>blue<span class="pl-pds">&quot;</span></span>)
plt.xlabel(<span class="pl-s"><span class="pl-pds">&quot;</span>Episode<span class="pl-pds">&quot;</span></span>)
plt.ylabel(<span class="pl-s"><span class="pl-pds">&quot;</span>Reward<span class="pl-pds">&quot;</span></span>)
plt.title(<span class="pl-s"><span class="pl-pds">&quot;</span>Rewards per Episode<span class="pl-pds">&quot;</span></span>)
plt.savefig(<span class="pl-s"><span class="pl-pds">&quot;</span>dqn-reward-history.png<span class="pl-pds">&quot;</span></span>)

</code></pre>
<p><img src="./img/dqn-reward-history.png" alt="dqn-reward-history"/></p></div></div></div></main></div><script src="/_next/static/chunks/webpack-d7dace7cd354c2cf.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/5455839c73f146e7-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/7626ed2c039b2726-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/_next/static/css/98b6e1014510d512.css\",\"style\"]\n4:HL[\"/_next/static/css/e09042d6d37504f5.css\",\"style\"]\n5:HL[\"/_next/static/css/dd89e605550f760e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"6:I[5751,[],\"\"]\n9:I[9275,[],\"\"]\nc:I[1343,[],\"\"]\ne:I[231,[\"217\",\"static/chunks/578c2090-c1b891f3b6c746fd.js\",\"675\",\"static/chunks/b563f954-758761ebc4ecc2e7.js\",\"699\",\"static/chunks/8e1d74a4-44e18cb83de8b273.js\",\"779\",\"static/chunks/0e762574-0fedad6633d82a8a.js\",\"231\",\"static/chunks/231-87925b9c7247c60f.js\",\"575\",\"static/chunks/app/%5Btheme%5D/layout-41c0f24cb97d52a9.js\"],\"\"]\n10:I[6130,[],\"\"]\na:[\"theme\",\"ai\",\"d\"]\nb:[\"post\",\"%5B13-2%5D-Cartpole\",\"d\"]\n11:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/98b6e1014510d512.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L6\",null,{\"buildId\":\"5UnPUbTMlr2EspV43B2jm\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/ai/%5B13-2%5D-Cartpole\",\"initialTree\":[\"\",{\"children\":[[\"theme\",\"ai\",\"d\"],{\"children\":[[\"post\",\"%5B13-2%5D-Cartpole\",\"d\"],{\"children\":[\"__PAGE__?{\\\"theme\\\":\\\"ai\\\",\\\"post\\\":\\\"[13-2]-Cartpole\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"theme\",\"ai\",\"d\"],{\"children\":[[\"post\",\"%5B13-2%5D-Cartpole\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L7\",\"$L8\"],null],null]},[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$a\",\"children\",\"$b\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lc\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/dd89e605550f760e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]}],null]},[\"$Ld\",null],null]},[[\"$\",\"html\",null,{\"lang\":\"ko\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_098cd6 min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-[24rem_1fr] auto-rows-auto\",\"children\":[[\"$\",\"nav\",null,{\"className\":\"__className_92d895 bg-white w-full h-14 flex items-center justify-between pl-5 pr-5 mb-20 border-b-2 border-b-slate-300 text-2xl fixed\",\"children\":[[\"$\",\"$Le\",null,{\"href\":\"/\",\"children\":\"tomatoM4to's blog\"}],[\"$\",\"div\",null,{\"className\":\"hidden lg:flex items-center\",\"children\":[[\"$\",\"input\",null,{\"type\":\"text\",\"className\":\"w-36 h-7 rounded-full border-2 border-black pl-2\",\"placeholder\":\"search\"}],[\"$\",\"div\",null,{\"className\":\"bg-slate-300 h-10 w-0.5 ml-2\"}],[\"$\",\"$Le\",null,{\"href\":\"https://github.com/tomatoM4to/tomatoM4to.github.io\",\"className\":\"p-2 rounded-full hover:bg-gray-300 transition-colors duration-300\",\"children\":[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 16 16\",\"className\":\"text-2xl cursor-pointer\",\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\",\"d\":\"M7.976 0A7.977 7.977 0 0 0 0 7.976c0 3.522 2.3 6.507 5.431 7.584.392.049.538-.196.538-.392v-1.37c-2.201.49-2.69-1.076-2.69-1.076-.343-.93-.881-1.175-.881-1.175-.734-.489.048-.489.048-.489.783.049 1.224.832 1.224.832.734 1.223 1.859.88 2.3.685.048-.538.293-.88.489-1.076-1.762-.196-3.621-.881-3.621-3.964 0-.88.293-1.566.832-2.153-.05-.147-.343-.978.098-2.055 0 0 .685-.196 2.201.832.636-.196 1.322-.245 2.007-.245s1.37.098 2.006.245c1.517-1.027 2.202-.832 2.202-.832.44 1.077.146 1.908.097 2.104a3.16 3.16 0 0 1 .832 2.153c0 3.083-1.86 3.719-3.62 3.915.293.244.538.733.538 1.467v2.202c0 .196.146.44.538.392A7.984 7.984 0 0 0 16 7.976C15.951 3.572 12.38 0 7.976 0z\",\"children\":[]}]]],\"style\":{\"color\":\"$undefined\"},\"height\":\"1em\",\"width\":\"1em\",\"xmlns\":\"http://www.w3.org/2000/svg\"}]}],[\"$\",\"button\",null,{\"className\":\"p-2 rounded-full hover:bg-gray-300 transition-colors duration-300\",\"children\":[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 24 24\",\"className\":\"text-2xl cursor-pointer\",\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"fill\":\"none\",\"strokeWidth\":\"2\",\"d\":\"M12,23 C18.0751322,23 23,18.0751322 23,12 C23,5.92486775 18.0751322,1 12,1 C5.92486775,1 1,5.92486775 1,12 C1,18.0751322 5.92486775,23 12,23 Z M12,23 C15,23 16,18 16,12 C16,6 15,1 12,1 C9,1 8,6 8,12 C8,18 9,23 12,23 Z M2,16 L22,16 M2,8 L22,8\",\"children\":[]}]]],\"style\":{\"color\":\"$undefined\"},\"height\":\"1em\",\"width\":\"1em\",\"xmlns\":\"http://www.w3.org/2000/svg\"}]}],[\"$\",\"button\",null,{\"className\":\"p-2 rounded-full hover:bg-gray-300 transition-colors duration-300\",\"children\":[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 16 16\",\"className\":\"text-2xl cursor-pointer\",\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"d\":\"M6 .278a.77.77 0 0 1 .08.858 7.2 7.2 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277q.792-.001 1.533-.16a.79.79 0 0 1 .81.316.73.73 0 0 1-.031.893A8.35 8.35 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.75.75 0 0 1 6 .278\",\"children\":[]}],[\"$\",\"path\",\"1\",{\"d\":\"M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.73 1.73 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.73 1.73 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.73 1.73 0 0 0 1.097-1.097zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.16 1.16 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.16 1.16 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732z\",\"children\":[]}]]],\"style\":{\"color\":\"$undefined\"},\"height\":\"1em\",\"width\":\"1em\",\"xmlns\":\"http://www.w3.org/2000/svg\"}]}]]}]]}],[\"$\",\"main\",null,{\"className\":\"col-span-2\",\"children\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lc\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/e09042d6d37504f5.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]}]}]]}]}]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$Lf\"],\"globalErrorComponent\":\"$10\",\"missingSlots\":\"$W11\"}]]\n"])</script><script>self.__next_f.push([1,"12:I[1815,[\"217\",\"static/chunks/578c2090-c1b891f3b6c746fd.js\",\"675\",\"static/chunks/b563f954-758761ebc4ecc2e7.js\",\"699\",\"static/chunks/8e1d74a4-44e18cb83de8b273.js\",\"779\",\"static/chunks/0e762574-0fedad6633d82a8a.js\",\"231\",\"static/chunks/231-87925b9c7247c60f.js\",\"575\",\"static/chunks/app/%5Btheme%5D/layout-41c0f24cb97d52a9.js\"],\"Hamburger\"]\n"])</script><script>self.__next_f.push([1,"f:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1, maximum-scale=1, user-scalable=yes\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"tomatom4to's Computer Science Blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Comprehensive computer science knowledge covering OS, Database, AI, Networks, Linux, and Docker. Learn computer science concepts with clear explanations and practical examples.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"tomatom4to\"}],[\"$\",\"meta\",\"5\",{\"name\":\"keywords\",\"content\":\"Computer Science,Operating Systems,Database,AI,Network,Linux,Docker,Programming,Software Development\"}],[\"$\",\"meta\",\"6\",{\"name\":\"creator\",\"content\":\"tomatom4to\"}],[\"$\",\"meta\",\"7\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"8\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"9\",{\"rel\":\"canonical\",\"href\":\"https://tomatom4to.github.io\"}],[\"$\",\"meta\",\"10\",{\"name\":\"format-detection\",\"content\":\"telephone=no, address=no, email=no\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:title\",\"content\":\"tomatom4to's Computer Science Blog\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:description\",\"content\":\"Your gateway to comprehensive computer science knowledge and practical programming skills\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:url\",\"content\":\"https://tomatom4to.github.io\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:site_name\",\"content\":\"tomatom4to's CS Blog\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:locale\",\"content\":\"ko_KR\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:title\",\"content\":\"tomatom4to's Computer Science Blog\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:description\",\"content\":\"Your gateway to comprehensive computer science knowledge and practical programming skills\"}],[\"$\",\"link\",\"20\",{\"rel\":\"icon\",\"href\":\"/icon.ico?3c4912ce8b26c1d6\",\"type\":\"image/x-icon\",\"sizes\":\"256x256\"}],[\"$\",\"meta\",\"21\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"7:null\n"])</script><script>self.__next_f.push([1,"d:[\"$\",\"div\",null,{\"className\":\"flex\",\"children\":[[\"$\",\"$L12\",null,{\"res\":[{\"isOutLine\":false,\"firstOrder\":1,\"secondOrder\":-1,\"order\":\"1\",\"title\":\"Introduction\",\"originalName\":\"[1]-Introduction\"},{\"isOutLine\":false,\"firstOrder\":2,\"secondOrder\":-1,\"order\":\"2\",\"title\":\"Basis math\",\"originalName\":\"[2]-Basis-math\"},{\"isOutLine\":true,\"firstOrder\":2,\"secondOrder\":1,\"order\":\"2-1\",\"title\":\"Math with RL\",\"originalName\":\"[2-1]-Math-with-RL\"},{\"isOutLine\":false,\"firstOrder\":3,\"secondOrder\":-1,\"order\":\"3\",\"title\":\"Q learning\",\"originalName\":\"[3]-Q-learning\"},{\"isOutLine\":true,\"firstOrder\":3,\"secondOrder\":1,\"order\":\"3-1\",\"title\":\"Basic Concept\",\"originalName\":\"[3-1]-Basic-Concept\"},{\"isOutLine\":true,\"firstOrder\":3,\"secondOrder\":2,\"order\":\"3-2\",\"title\":\"Greedy action\",\"originalName\":\"[3-2]-Greedy-action\"},{\"isOutLine\":true,\"firstOrder\":3,\"secondOrder\":3,\"order\":\"3-3\",\"title\":\"Discount factor\",\"originalName\":\"[3-3]-Discount-factor\"},{\"isOutLine\":true,\"firstOrder\":3,\"secondOrder\":4,\"order\":\"3-4\",\"title\":\"Learning rate\",\"originalName\":\"[3-4]-Learning-rate\"},{\"isOutLine\":false,\"firstOrder\":4,\"secondOrder\":-1,\"order\":\"4\",\"title\":\"Markov process\",\"originalName\":\"[4]-Markov-process\"},{\"isOutLine\":false,\"firstOrder\":5,\"secondOrder\":-1,\"order\":\"5\",\"title\":\"Bellman Optimality Equation\",\"originalName\":\"[5]-Bellman-Optimality-Equation\"},{\"isOutLine\":false,\"firstOrder\":6,\"secondOrder\":-1,\"order\":\"6\",\"title\":\"DP\",\"originalName\":\"[6]-DP\"},{\"isOutLine\":false,\"firstOrder\":7,\"secondOrder\":-1,\"order\":\"7\",\"title\":\"Monte Carlo\",\"originalName\":\"[7]-Monte-Carlo\"},{\"isOutLine\":false,\"firstOrder\":8,\"secondOrder\":-1,\"order\":\"8\",\"title\":\"Temporal Difference Learning\",\"originalName\":\"[8]-Temporal-Difference-Learning\"},{\"isOutLine\":false,\"firstOrder\":9,\"secondOrder\":-1,\"order\":\"9\",\"title\":\"nSTEP Bootstrapping\",\"originalName\":\"[9]-nSTEP-Bootstrapping\"},{\"isOutLine\":false,\"firstOrder\":10,\"secondOrder\":-1,\"order\":\"10\",\"title\":\"Planning and Learning\",\"originalName\":\"[10]-Planning-and-Learning\"},{\"isOutLine\":true,\"firstOrder\":10,\"secondOrder\":1,\"order\":\"10-1\",\"title\":\"Function Approximation\",\"originalName\":\"[10-1]-Function-Approximation\"},{\"isOutLine\":false,\"firstOrder\":11,\"secondOrder\":-1,\"order\":\"11\",\"title\":\"Deep learning\",\"originalName\":\"[11]-Deep-learning\"},{\"isOutLine\":true,\"firstOrder\":11,\"secondOrder\":1,\"order\":\"11-1\",\"title\":\"Concept\",\"originalName\":\"[11-1]-Concept\"},{\"isOutLine\":true,\"firstOrder\":11,\"secondOrder\":2,\"order\":\"11-2\",\"title\":\"Newerl Network\",\"originalName\":\"[11-2]-Newerl-Network\"},{\"isOutLine\":true,\"firstOrder\":11,\"secondOrder\":3,\"order\":\"11-3\",\"title\":\"Loss function\",\"originalName\":\"[11-3]-Loss-function\"},{\"isOutLine\":true,\"firstOrder\":11,\"secondOrder\":4,\"order\":\"11-4\",\"title\":\"Activation function\",\"originalName\":\"[11-4]-Activation-function\"},{\"isOutLine\":true,\"firstOrder\":11,\"secondOrder\":5,\"order\":\"11-5\",\"title\":\"Gradient descent\",\"originalName\":\"[11-5]-Gradient-descent\"},{\"isOutLine\":true,\"firstOrder\":11,\"secondOrder\":6,\"order\":\"11-6\",\"title\":\"Back Propagation\",\"originalName\":\"[11-6]-Back-Propagation\"},{\"isOutLine\":false,\"firstOrder\":12,\"secondOrder\":-1,\"order\":\"12\",\"title\":\"Q Network\",\"originalName\":\"[12]-Q-Network\"},{\"isOutLine\":true,\"firstOrder\":12,\"secondOrder\":1,\"order\":\"12-1\",\"title\":\"Frozen Lake\",\"originalName\":\"[12-1]-Frozen-Lake\"},{\"isOutLine\":true,\"firstOrder\":12,\"secondOrder\":2,\"order\":\"12-2\",\"title\":\"Cartpole\",\"originalName\":\"[12-2]-Cartpole\"},{\"isOutLine\":false,\"firstOrder\":13,\"secondOrder\":-1,\"order\":\"13\",\"title\":\"DQN\",\"originalName\":\"[13]-DQN\"},{\"isOutLine\":true,\"firstOrder\":13,\"secondOrder\":1,\"order\":\"13-1\",\"title\":\"Concept\",\"originalName\":\"[13-1]-Concept\"},{\"isOutLine\":true,\"firstOrder\":13,\"secondOrder\":2,\"order\":\"13-2\",\"title\":\"Cartpole\",\"originalName\":\"[13-2]-Cartpole\"}],\"params\":{\"theme\":\"ai\"}}],[\"$\",\"aside\",null,{\"className\":\"hidden lg:flex w-64 2xl:w-96 flex-col h-screen border-r-2 border-gray-300 mt-14 p-1 pl-5 fixed overflow-y-auto overscroll-contain\",\"children\":[[\"$\",\"$Le\",null,{\"href\":\"/ai/[1]-Introduction\",\"className\":\"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"1\",\". \",\"Introduction\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[2]-Basis-math\",\"className\":\"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"2\",\". \",\"Basis math\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[2-1]-Math-with-RL\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"2-1\",\". \",\"Math with RL\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[3]-Q-learning\",\"className\":\"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"3\",\". \",\"Q learning\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[3-1]-Basic-Concept\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"3-1\",\". \",\"Basic Concept\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[3-2]-Greedy-action\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"3-2\",\". \",\"Greedy action\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[3-3]-Discount-factor\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"3-3\",\". \",\"Discount factor\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[3-4]-Learning-rate\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"3-4\",\". \",\"Learning rate\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[4]-Markov-process\",\"className\":\"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"4\",\". \",\"Markov process\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[5]-Bellman-Optimality-Equation\",\"className\":\"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"5\",\". \",\"Bellman Optimality Equation\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[6]-DP\",\"className\":\"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"6\",\". \",\"DP\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[7]-Monte-Carlo\",\"className\":\"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"7\",\". \",\"Monte Carlo\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[8]-Temporal-Difference-Learning\",\"className\":\"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"8\",\". \",\"Temporal Difference Learning\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[9]-nSTEP-Bootstrapping\",\"className\":\"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"9\",\". \",\"nSTEP Bootstrapping\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[10]-Planning-and-Learning\",\"className\":\"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"10\",\". \",\"Planning and Learning\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[10-1]-Function-Approximation\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"10-1\",\". \",\"Function Approximation\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[11]-Deep-learning\",\"className\":\"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"11\",\". \",\"Deep learning\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[11-1]-Concept\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"11-1\",\". \",\"Concept\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[11-2]-Newerl-Network\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"11-2\",\". \",\"Newerl Network\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[11-3]-Loss-function\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"11-3\",\". \",\"Loss function\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[11-4]-Activation-function\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"11-4\",\". \",\"Activation function\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[11-5]-Gradient-descent\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"11-5\",\". \",\"Gradient descent\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[11-6]-Back-Propagation\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"11-6\",\". \",\"Back Propagation\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[12]-Q-Network\",\"className\":\"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"12\",\". \",\"Q Network\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[12-1]-Frozen-Lake\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"12-1\",\". \",\"Frozen Lake\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[12-2]-Cartpole\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"12-2\",\". \",\"Cartpole\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[13]-DQN\",\"className\":\"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"13\",\". \",\"DQN\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[13-1]-Concept\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"13-1\",\". \",\"Concept\"]}],[\"$\",\"$Le\",null,{\"href\":\"/ai/[13-2]-Cartpole\",\"className\":\"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg\",\"onClick\":\"$undefined\",\"children\":[\"13-2\",\". \",\"Cartpole\"]}]]}],[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$a\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lc\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]}]\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"div\",null,{\"className\":\"lg:ml-64 2xl:ml-96 mt-32 mb-32 flex-1 flex flex-col items-center overflow-x-hidden\",\"children\":[\"$\",\"div\",null,{\"className\":\"w-11/12 md:w-3/4 lg:w-2/3 2xl:w-1/2 markdown-body\",\"children\":[[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"overflow-x-auto bg-gray-200 rounded-md px-1 language-python\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# gym 행동 공간에서 행동의 숫자를 얻습니다.\"}],\"\\nn_actions \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" env.action_space.n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 상태 관측 횟수를 얻습니다.\"}],\"\\nstate, info \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" env.reset()\\nn_observations \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"len\"}],\"(state)\\n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"print\"}],\"(n_actions, state) \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 2, [ 0.03645075  0.04290273 -0.00414692 -0.0414    ]\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"print\"}],\"(n_observations) \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 4\"}],\"\\n\"]}],\"className\":\"bg-gray-200 rounded-md px-1 py-2\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"[-0.03755258 0.0260832 0.045742 -0.00931699]\",\"className\":\"overflow-x-auto bg-gray-200 rounded-md px-1 undefined\"}],\"는 벡터입니다. 이 벡터는 CartPole-v1 환경의 상태를 나타내는 네 개의 값으로 구성되어 있습니다. 각 값은 환경의 특정 속성을 나타냅니다:\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"카트의 위치\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"카트의 속도\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"폴의 각도\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"폴의 각속도\"}],\"\\n\"],\"className\":\"list-disc ml-4\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"이 값들은 환경이 초기화될 때 반환된 상태를 나타내며, 벡터 형태로 표현됩니다.\"}],\"\\n\",[\"$\",\"hr\",null,{\"className\":\"border-t-4 border-gray-300 mt-10 mb-10\",\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"h1\",null,{\"children\":\"전체 코드\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://tutorials.pytorch.kr/intermediate/reinforcement_q_learning.html\",\"children\":\"https://tutorials.pytorch.kr/intermediate/reinforcement_q_learning.html\"}]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"overflow-x-auto bg-gray-200 rounded-md px-1 language-python\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"import\"}],\" gymnasium \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"as\"}],\" gym\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"import\"}],\" math\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"import\"}],\" random\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"import\"}],\" matplotlib.pyplot \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"as\"}],\" plt\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"from\"}],\" collections \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"import\"}],\" namedtuple, deque\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"from\"}],\" itertools \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"import\"}],\" count\\n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"import\"}],\" torch\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"import\"}],\" torch.nn \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"as\"}],\" nn\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"import\"}],\" torch.optim \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"as\"}],\" optim\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"import\"}],\" torch.nn.functional \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"as\"}],\" F\\n\\nenv \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" gym.make(\",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}],\"CartPole-v1\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}]]}],\")\\n\\nplt.ion()\\n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# GPU를 사용할 경우\"}],\"\\ndevice \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" torch.device(\",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}],\"cuda\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}]]}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"if\"}],\" torch.cuda.is_available() \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"else\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}],\"cpu\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}]]}],\")\\n\\n\\nTransition \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" namedtuple(\",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"'\"}],\"Transition\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"'\"}]]}],\",\\n                        (\",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"'\"}],\"state\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"'\"}]]}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"'\"}],\"action\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"'\"}]]}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"'\"}],\"next_state\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"'\"}]]}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"'\"}],\"reward\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"'\"}]]}],\"))\\n\\n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"class\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-en\",\"children\":\"ReplayMemory\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"object\"}],\"):\\n\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"def\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"__init__\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"self\"}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"capacity\"}],\"):\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"self\"}],\".memory \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" deque([], \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"maxlen\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\"capacity)\\n\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"def\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-en\",\"children\":\"push\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"self\"}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"*\"}],[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"args\"}],\"):\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\\\"\\\"\"}],\"transition 저장\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\\\"\\\"\"}]]}],\"\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"self\"}],\".memory.append(Transition(\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"*\"}],\"args))\\n\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"def\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-en\",\"children\":\"sample\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"self\"}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"batch_size\"}],\"):\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"return\"}],\" random.sample(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"self\"}],\".memory, batch_size)\\n\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"def\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"__len__\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"self\"}],\"):\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"return\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"len\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"self\"}],\".memory)\\n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"class\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-en\",\"children\":\"DQN\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-e\",\"children\":\"nn\"}],\".\",[\"$\",\"span\",null,{\"className\":\"pl-e\",\"children\":\"Module\"}],\"):\\n\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"def\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"__init__\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"self\"}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"n_observations\"}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"n_actions\"}],\"):\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"super\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"DQN\"}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"self\"}],\").\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"__init__\"}],\"()\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"self\"}],\".layer1 \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" nn.Linear(n_observations, \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"128\"}],\")\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"self\"}],\".layer2 \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" nn.Linear(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"128\"}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"128\"}],\")\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"self\"}],\".layer3 \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" nn.Linear(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"128\"}],\", n_actions)\\n\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 최적화 중에 다음 행동을 결정하기 위해서 하나의 요소 또는 배치를 이용해 호촐됩니다.\"}],\"\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# ([[left0exp,right0exp]...]) 를 반환합니다.\"}],\"\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"def\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-en\",\"children\":\"forward\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"self\"}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"x\"}],\"):\\n        x \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" F.relu(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"self\"}],\".layer1(x))\\n        x \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" F.relu(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"self\"}],\".layer2(x))\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"return\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"self\"}],\".layer3(x)\\n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# BATCH_SIZE는 리플레이 버퍼에서 샘플링된 트랜지션의 수입니다.\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# GAMMA는 이전 섹션에서 언급한 할인 계수입니다.\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# EPS_START는 엡실론의 시작 값입니다.\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# EPS_END는 엡실론의 최종 값입니다.\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# EPS_DECAY는 엡실론의 지수 감쇠(exponential decay) 속도 제어하며, 높을수록 감쇠 속도가 느립니다.\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# TAU는 목표 네트워크의 업데이트 속도입니다.\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# LR은 ``AdamW`` 옵티마이저의 학습율(learning rate)입니다.\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"BATCH_SIZE\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"128\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"GAMMA\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"0.99\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"EPS_START\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"0.9\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"EPS_END\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"0.05\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"EPS_DECAY\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"1000\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"TAU\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"0.005\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"LR\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"1e-4\"}],\"\\n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# gym 행동 공간에서 행동의 숫자를 얻습니다.\"}],\"\\nn_actions \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" env.action_space.n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 상태 관측 횟수를 얻습니다.\"}],\"\\nstate, info \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" env.reset()\\nn_observations \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"len\"}],\"(state)\\n\\npolicy_net \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" DQN(n_observations, n_actions).to(device)\\ntarget_net \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" DQN(n_observations, n_actions).to(device)\\ntarget_net.load_state_dict(policy_net.state_dict())\\n\\noptimizer \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" optim.AdamW(policy_net.parameters(), \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"lr\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"LR\"}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"amsgrad\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"True\"}],\")\\nmemory \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" ReplayMemory(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"10000\"}],\")\\n\\n\\nsteps_done \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"0\"}],\"\\n\\n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"def\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-en\",\"children\":\"select_action\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"state\"}],\"):\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"global\"}],\" steps_done\\n    sample \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" random.random()\\n    eps_threshold \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"EPS_END\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"+\"}],\" (\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"EPS_START\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"-\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"EPS_END\"}],\") \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"*\"}],\" \\\\\\n        math.exp(\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"-\"}],[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"1\"}],\". \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"*\"}],\" steps_done \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"/\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"EPS_DECAY\"}],\")\\n    steps_done \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"+=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"1\"}],\"\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"if\"}],\" sample \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"\u003e\"}],\" eps_threshold:\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"with\"}],\" torch.no_grad():\\n            \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\"}],\"\\n            \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\"}],\"\\n            \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 기대 보상이 더 큰 행동을 선택할 수 있습니다.\"}],\"\\n            \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"return\"}],\" policy_net(state).max(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"1\"}],\").indices.view(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"1\"}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"1\"}],\")\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"else\"}],\":\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"return\"}],\" torch.tensor([[env.action_space.sample()]], \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"device\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\"device, \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"dtype\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\"torch.long)\\n\\n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"def\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-en\",\"children\":\"optimize_model\"}],\"():\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"if\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"len\"}],\"(memory) \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"\u003c\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"BATCH_SIZE\"}],\":\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"return\"}],\"\\n    transitions \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" memory.sample(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"BATCH_SIZE\"}],\")\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\"}],\"\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로\"}],\"\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 전환합니다.\"}],\"\\n    batch \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" Transition(\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"*\"}],[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"zip\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"*\"}],\"transitions))\\n\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 최종이 아닌 상태의 마스크를 계산하고 배치 요소를 연결합니다\"}],\"\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# (최종 상태는 시뮬레이션이 종료 된 이후의 상태)\"}],\"\\n    non_final_mask \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" torch.tensor(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"tuple\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"map\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"lambda\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-smi\",\"children\":\"s\"}],\": s \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"is\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"not\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"None\"}],\",\\n                                          batch.next_state)), \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"device\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\"device, \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"dtype\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\"torch.bool)\\n    non_final_next_states \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" torch.cat([s \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"for\"}],\" s \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"in\"}],\" batch.next_state\\n                                                \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"if\"}],\" s \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"is\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"not\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"None\"}],\"])\\n    state_batch \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" torch.cat(batch.state)\\n    action_batch \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" torch.cat(batch.action)\\n    reward_batch \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" torch.cat(batch.reward)\\n\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.\"}],\"\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.\"}],\"\\n    state_action_values \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" policy_net(state_batch).gather(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"1\"}],\", action_batch)\\n\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 모든 다음 상태를 위한 V(s_{t+1}) 계산\"}],\"\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# non_final_next_states의 행동들에 대한 기대값은 \\\"이전\\\" target_net을 기반으로 계산됩니다.\"}],\"\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# max(1).values로 최고의 보상을 선택하십시오.\"}],\"\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.\"}],\"\\n    next_state_values \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" torch.zeros(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"BATCH_SIZE\"}],\", \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"device\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\"device)\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"with\"}],\" torch.no_grad():\\n        next_state_values[non_final_mask] \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" target_net(non_final_next_states).max(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"1\"}],\").values\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 기대 Q 값 계산\"}],\"\\n    expected_state_action_values \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" (next_state_values \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"*\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"GAMMA\"}],\") \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"+\"}],\" reward_batch\\n\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# Huber 손실 계산\"}],\"\\n    criterion \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" nn.SmoothL1Loss()\\n    loss \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" criterion(state_action_values, expected_state_action_values.unsqueeze(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"1\"}],\"))\\n\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 모델 최적화\"}],\"\\n    optimizer.zero_grad()\\n    loss.backward()\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 변화도 클리핑 바꿔치기\"}],\"\\n    torch.nn.utils.clip_grad_value_(policy_net.parameters(), \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"100\"}],\")\\n    optimizer.step()\\n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"if\"}],\" torch.cuda.is_available():\\n    num_episodes \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"600\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"else\"}],\":\\n    num_episodes \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"50\"}],\"\\n\\n\\nrewards_history \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" []\\n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"for\"}],\" i_episode \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"in\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"range\"}],\"(num_episodes):\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 환경과 상태 초기화\"}],\"\\n    state, info \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" env.reset()\\n    state \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" torch.tensor(state, \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"dtype\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\"torch.float32, \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"device\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\"device).unsqueeze(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"0\"}],\")\\n    episode_reward \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"0\"}],\"\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"for\"}],\" t \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"in\"}],\" count():\\n        action \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" select_action(state)\\n        observation, reward, terminated, truncated, _ \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" env.step(action.item())\\n        episode_reward \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"+=\"}],\" reward\\n        reward \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" torch.tensor([reward], \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"device\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\"device)\\n        done \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" terminated \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"or\"}],\" truncated\\n\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"if\"}],\" terminated:\\n            next_state \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"None\"}],\"\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"else\"}],\":\\n            next_state \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" torch.tensor(observation, \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"dtype\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\"torch.float32, \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"device\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\"device).unsqueeze(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"0\"}],\")\\n\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 메모리에 변이 저장\"}],\"\\n        memory.push(state, action, next_state, reward)\\n\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 다음 상태로 이동\"}],\"\\n        state \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" next_state\\n\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# (정책 네트워크에서) 최적화 한단계 수행\"}],\"\\n        optimize_model()\\n\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# 목표 네트워크의 가중치를 소프트 업데이트\"}],\"\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-c\",\"children\":\"# θ′ ← τ θ + (1 −τ )θ′\"}],\"\\n        target_net_state_dict \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" target_net.state_dict()\\n        policy_net_state_dict \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" policy_net.state_dict()\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"for\"}],\" key \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"in\"}],\" policy_net_state_dict:\\n            target_net_state_dict[key] \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],\" policy_net_state_dict[key]\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"*\"}],[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"TAU\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"+\"}],\" target_net_state_dict[key]\",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"*\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"1\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"-\"}],[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"TAU\"}],\")\\n        target_net.load_state_dict(target_net_state_dict)\\n\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"if\"}],\" done:\\n            \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"break\"}],\"\\n    \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"if\"}],\" i_episode \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"%\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"50\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"==\"}],\" \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"0\"}],\":\\n        \",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"print\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"f\"}],[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}],\"Episode \"]}],[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"{\"}],\"i_episode\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"}\"}],[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":\", Reward: \"}],[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"{\"}],\"episode_reward\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"}\"}],[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}]}],\")\\n    rewards_history.append(episode_reward)\\n\\n\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"print\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"'\"}],\"Complete\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"'\"}]]}],\")\\nplt.plot(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"range\"}],\"(\",[\"$\",\"span\",null,{\"className\":\"pl-c1\",\"children\":\"len\"}],\"(rewards_history)), rewards_history, \",[\"$\",\"span\",null,{\"className\":\"pl-v\",\"children\":\"color\"}],[\"$\",\"span\",null,{\"className\":\"pl-k\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}],\"blue\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}]]}],\")\\nplt.xlabel(\",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}],\"Episode\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}]]}],\")\\nplt.ylabel(\",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}],\"Reward\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}]]}],\")\\nplt.title(\",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}],\"Rewards per Episode\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}]]}],\")\\nplt.savefig(\",[\"$\",\"span\",null,{\"className\":\"pl-s\",\"children\":[[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}],\"dqn-reward-history.png\",[\"$\",\"span\",null,{\"className\":\"pl-pds\",\"children\":\"\\\"\"}]]}],\")\\n\\n\"]}],\"className\":\"bg-gray-200 rounded-md px-1 py-2\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"img\",null,{\"src\":\"./img/dqn-reward-history.png\",\"alt\":\"dqn-reward-history\"}]}]]}]}]\n"])</script></body></html>