# Q-learning
먼저 Q-learning라는 알고리즘이 어떻게 동작하는지 살펴보면서 강화학습이란것이 어떤식으로 작동하는지 살펴보겠 습니다. $4*4$ 격자 지도가 있다 가정해 보겠습니다. $(0, 0)$ 에서 출발해 $(3, 3)$에 도착해야 한다 가정해 보겠습니다.

이 지도를 처음부터 전부 알고 있다면 한번에 가장 빠른 길을 찾을 수 있겠지만, RL에선 보통 이 지도를 모른다고 가정합니다. 때문에, 바로 최적의 길을 찾는것이 아니라 아래에도 가보고 위에도 가보고 하는 과정을 거치다가 우연히 $(3, 3)$에 도착하게 됩니다. 그러면서 이 경로를 점점 최적의 경로로 만들어 나가는 것이 RL의 핵심입니다. 핵심적인 부분은 지금 $(3, 3)$ 에 도착했다 라는 사실많을 알게 되는겁니다. 이렇게 하면 첫번째 episode가 끝나게 됩니다.

그리고 2번째, 3번째 episode를 거치면서 점점 최적의 경로를 찾아나가게 됩니다. 이렇게 최적의 경로를 찾아나가는 과정을 Q-learning이라고 합니다. Q-learning은 이러한 과정을 통해 최적의 경로를 찾아나가는 알고리즘입니다.

## Greedy action
이러한 과정을 구현하기 위해 Q-learning에선 Greedy-action라는 행동을 합니다. 직역하면 탐욕적인 행동이라 할수 있는데요, 움직이는 각 경로마다 점수를 매기고 해당 점수가 가장 큰 방향으로 움직이는 것입니다.

1. 초기에 상하좌우로 이동할수 있는 격자지도는 $[0, 0, 0, 0]$ 으로 초기화 되어 있습니다. 각각 상하좌우에 대한 값입니다. 이때는 전부 점수가 같은 상황이니 Random하게 행동하게 됩니다. 각 방향에 대해 0.25의 확률로 움직이게 됩니다.

|  |  |  |  |
|---|---|---|---|
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | R = 1 |
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] |
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] |
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] |


2. 그러다가 우연히 $(3, 3)$에 도착하게 된다면, Q-learning는 이곳에 무엇인가 있어! 라면서 헨젤과 그레텔처럼 무엇인가를 남기고 episode를 끝내게 됩니다. 이때는 위로 가는 방향에 대해 점수를 R로 매기게 됩니다. R은 Reward의 약자로 정수의 형태로 존재합니다. 그 이외인 좌, 우, 아래 방향에 대해는 아직 0점입니다.

|  |  |  |  |
|---|---|---|---|
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | R = 1 |
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | [R, 0, 0, 0] |
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] |
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] |

3. 2번째 episode를 시작합니다. 이번에도 아직 $(3, 1)$ 에서 위로가는 방향에 대한 점수많이 R이고 그 이외의 나머지는 아직 0인 상태입니다. 그러다가 우연히 $(2, 1)$ 번째 좌표에 들려 또 우연히 오른쪽으로 이동했다 가정해 보겠습니다. 이렇게 되면 Q-learning는 0이 아닌 더 큰 값을 발견하게 됩니다. 이때 Q-learning는 $(3, 1)$의 가장 큰값을 $(2, 1)$에 매기게 됩니다. 이때는 $(2, 1)$에서 오른쪽으로 가는 방향에 대해 점수를 R로 매기게 됩니다. Q-learning의 중요한 특징을 여기서 관찰할수 있는데요, Q-learning은 이동을 하면서 바로바로 업데이트 합니다. 그렇기 때문에 에피소드가 끝나기 전에도 업데이트를 수행 합니다. 오른쪽으로 이동을 함과 동시에 오른쪽으로 갔을때의 형가를 실시간으로 업데이트 합니다. 이렇든 이 판을 만드는 과정이 Q-learning이고 이러한 값을 Q-value라고 합니다.

|  |  |  |  |
|---|---|---|---|
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | R = 1 |
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, R] | [R, 0, 0, 0] |
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] |
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] |

4. 이런식으로 episode를 지속한다면 최종적으로 이러한 방향이 나오게 됩니다.

|  |  |  |  |
|---|---|---|---|
| [0, R, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | R = 1 |
| [0, 0, 0, R] | [0, 0, 0, R] | [0, 0, 0, R] | [R, 0, 0, 0] |
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] |
| [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] | [0, 0, 0, 0] |

### 구현
```python
import random
import copy

def greedy_q_learning():
    # 4x4 맵 초기화, 각 위치마다 4개의 행동(상,하,좌,우)에 대한 Q값
    map = [[[0, 0, 0, 0] for _ in range(4)] for _ in range(4)]

    # 목표 지점의 Q값을 99로 설정
    map[3][3] = [99, 99, 99, 99]
    count = 0
    for episode in range(1000):
        state = [0, 0]  # 시작 위치
        cp_map = copy.deepcopy(map)  # 맵 변화 감지를 위한 복사

        while state != [3, 3]:  # 목표 도달까지 반복
            loc = map[state[0]][state[1]]

            # 현재 위치에서의 행동 선택
            if loc.count(0) == 4:  # 모든 Q값이 0이면 랜덤 선택
                action = random.randint(0, 3)
            else:  # 가장 큰 Q값을 가진 행동 선택
                action = loc.index(max(loc))

            # 이전 상태 저장
            prev_state = copy.deepcopy(state)

            # 선택한 행동으로 이동
            if action == 0 and state[0] > 0:  # 상
                state[0] -= 1
            elif action == 1 and state[0] < 3:  # 하
                state[0] += 1
            elif action == 2 and state[1] > 0:  # 좌
                state[1] -= 1
            elif action == 3 and state[1] < 3:  # 우
                state[1] += 1

            # 맵 밖으로 나가는 경우 처리
            if state[0] < 0:
                state[0] = 0
            elif state[0] > 3:
                state[0] = 3
            if state[1] < 0:
                state[1] = 0
            elif state[1] > 3:
                state[1] = 3

            # Q값 업데이트: Q값을 그대로 전파 함으로써, 한번 설정된 경로를 게속 따라가게 됩니다
            next_q_values = map[state[0]][state[1]]
            if max(next_q_values) > 0:
                map[prev_state[0]][prev_state[1]][action] = max(next_q_values)

            # 목표 도달 시
            if state == [3, 3]:
                map[prev_state[0]][prev_state[1]][action] = 99
                break

        # 맵이 변화하지 않을시 카운트
        if map == cp_map:
            count += 1
    return map, count

# 실행
q_map, count = greedy_q_learning()

# 결과 출력
print("Final Q-values:")
for row in q_map:
    print(row)

print("Count:", count)
```

***