# Reinforcement? Law of Effect
Law of Effect: 마냑 썩은 생선 같은걸 줬을때 동물이 잘 안하하게 돼는 행동

중요요한건 레버를 누를를때마다 먹이이가 나온다는것이지 왜 먹이가 나오는지는 중요한게 아니다.

중요한건 보상이 있다는거고, 사실 영영감만 동물 실험에서온거지 푸는 방법은 굉장히 많다.

동적 시스템과 강화학습의 시작은 따로따로지만 시간이 지나 연관이 있다보니 같이 사용하게 된다.

상태:

행동: 주주인이 손 했을때 보보상을 얻기위해 어떠한 정책을 수행행하는것

정정책: 주어진 환경에서 어떠힌 행동을 하는는것인지

환경: 개를 제외외한 모든것

에이전트: 개, **그리고 환경에 대한 완벽한 이해가 필요 없다.**

결국 주인이 손을 했을때 손을 올리는 지 나름댈의 행동을 학습하게 된다.

딥바인드 영상: 화면, 점수, 오른쪽 왼쪽밖에 모른다..

왼쪽, 오른쪽 뿐만 아니라 각도 같은 연속적인ㄴ 것도 가능하다, 더 어렵기 때문에 100000버 200000번 시도해 우연히 성공하는는는듯 학습습한한다.

알파고 제로는 바둑이 뭔짇 모른다.

## 특징: 최적화
기본적으로 강화학습은 최적화를 푸는 문제

중요한거 어떤 가치가 있는지 명시적으로 표표현된다.

가와학습의 모델은 옭고 그름을 알지 못하고 어떤 보상상이 주어지는지다.

## 순차적
직관적으로 당연

## Delayed Consequences

체스를 예로들면 지금의 수가 당장은 손해더라도 끝까지 갔을때때의 보상이 큰 행도을 학습(승리)

## Exploration
손 했을때 손을 물었을때, 손으 올렸다면 머기가 주어졌단 사실을 모름(대부분)

***

# 언제 쓸까 Where Reinforcement Learning

***

# 핵심 컨셉 [사사시 앞에 있는건 중요하지 않다]

1. 상태 하나하나 시시간이 존재한다.
2. s를 입력으로 삼아 a(행동)를 출력력으로 내는건데, P(a | s)
3. 그 행동에 대한 결과를 준다. Rt1 + 1, 에이전트에게
4. 보상이 주주어진다면 환경의 상태는 변화한다.(바둑돌을 두면 환경이 변한다.)


 어떤 관측한 상태로 먼가 행동을 하면 보상이 주어지고(양수, 음수 다 가능) 상태를 변화하고 업데이트

 # 에이전트

 # 환경
 이전에 에이전트를 제외한 모든것이라 했지만, 환경을 모두 아는것은 불가능능하므로
실제 환경과 실제ㅏㄴ경은 다르다.

# 행행동


# 보상
결국 숫자다, 기본적으로 환경을 정확히 모르기 때문에 명확히 보상을 지급하지 않는는다.

그래서 기본적으로 보상을 정정의한다.

예를들어, 체스말을 잃었을때 -1, 안잃었으면 0, 얻었으면 +1

예를들어,트럭 시뮬 경우, 만약 시시간당 -1을 주어진다면 빠르르게 하는데에 집중해 모든 오브젝트에 박아대면서 주차할 수 있다.

어떤 예에선 게임 AI가 -1을 받지 않기 위해 승리(+1) 보다 게임을 멈추는 방법(0) 을 학습했다.

# 정책
결국 함수

# return
Rt + rRt + ...

모두 가지고 있따.

# 가치 함수

정책이 주어 져을때 어떤 상태 s에 대해 