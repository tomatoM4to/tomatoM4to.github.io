https://dryjelly.tistory.com/138

# 소개
이전에 공부했던 Q-network를 Frozen Lake에 적용해 보겠습니다. 하지만 여러분들도 알다 싶이 Frozen Lake는 굉장히 쉬운 문제입니다. 이 문제를 해결하기 위해 NN을 적용한다는것은 마치 사과를 깎기 위해 팔뚝만한 중식도를 사용하는것과 같습니다. 그저 이해를 돕기 위한 코드란 점을 감안해 주세요.

코드응 pytorch 를 사용합니다.


전체 코드
```python
import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

class QNetwork(nn.Module):
    def __init__(self, input_size, output_size):
        super(QNetwork, self).__init__()
        self.layer1 = nn.Linear(input_size, output_size)
        # self.layer2 = nn.Linear(input_size, output_size)

    def forward(self, x):
        return self.layer1(x)

class DQNAgent:
    def __init__(self, input_size, output_size, learning_rate=0.1, gamma=0.99):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.q_network = QNetwork(input_size, output_size).to(self.device)
        self.optimizer = optim.SGD(self.q_network.parameters(), lr=learning_rate)
        self.gamma = gamma

    def get_action(self, state, epsilon):
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).to(self.device)
            q_values = self.q_network(state_tensor)

            if np.random.random() < epsilon:
                return np.random.randint(q_values.size(-1))
            return q_values.argmax().item()

    def train_step(self, state, action, reward, next_state, done):
        state_tensor = torch.FloatTensor(state).to(self.device)
        next_state_tensor = torch.FloatTensor(next_state).to(self.device)

        # 현재 Q 값 계산
        current_q_values = self.q_network(state_tensor)

        # 타겟 Q 값 계산
        with torch.no_grad():
            next_q_values = self.q_network(next_state_tensor)
            max_next_q = next_q_values.max(1)[0]
            target = reward + (1 - done) * self.gamma * max_next_q

        # Q 값 업데이트를 위한 마스크 생성
        mask = torch.zeros_like(current_q_values)
        mask[:, action] = 1

        # 손실 계산 및 최적화
        self.optimizer.zero_grad()
        q_value = current_q_values.masked_select(mask.bool())
        loss = nn.MSELoss()(q_value, target)
        loss.backward()
        self.optimizer.step()

        return loss.item()

def one_hot(x, n_classes=16):
    return np.identity(n_classes)[x:x+1]

def train():
    env = gym.make('FrozenLake-v1')
    input_size = env.observation_space.n
    output_size = env.action_space.n
    num_episodes = 500
    print(f"input_size: {input_size}, output_size: {output_size}")
    agent = DQNAgent(input_size, output_size)
    rewards_history = []

    for episode in range(num_episodes):
        state = env.reset()[0]
        state = one_hot(state)

        epsilon = 1.0 / ((episode / 50) + 10)
        episode_reward = 0
        done = False

        while not done:
            action = agent.get_action(state, epsilon)
            next_state, reward, done, _, _ = env.step(action)
            next_state = one_hot(next_state)

            loss = agent.train_step(state, action, reward, next_state, done)
            episode_reward += reward
            state = next_state

        rewards_history.append(episode_reward)

    return rewards_history

if __name__ == "__main__":
    rewards_history = train()
    success_rate = sum(rewards_history) / len(rewards_history) * 100
    print(f"Success rate: {success_rate:.2f}%")

    plt.bar(range(len(rewards_history)), rewards_history, color="blue")
    plt.xlabel("Episode")
    plt.ylabel("Reward")
    plt.title("Rewards per Episode")
    plt.show()
```