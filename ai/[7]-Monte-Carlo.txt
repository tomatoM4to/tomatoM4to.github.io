1:"$Sreact.fragment"
2:I[6874,["874","static/chunks/874-8826d48c805a2f7c.js","271","static/chunks/app/%5Btheme%5D/%5Bpost%5D/page-b8dfcb5d7be705e0.js"],""]
3:I[3853,["87","static/chunks/0e762574-2e51da01949c836e.js","874","static/chunks/874-8826d48c805a2f7c.js","177","static/chunks/app/layout-60ba4044e3534e4b.js"],"DarkModeButton"]
4:I[7555,[],""]
5:I[1295,[],""]
8:I[9665,[],"MetadataBoundary"]
a:I[9665,[],"OutletBoundary"]
d:I[4911,[],"AsyncMetadataOutlet"]
f:I[9665,[],"ViewportBoundary"]
11:I[6614,[],""]
:HL["/_next/static/media/93ed22edef6e70f7-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/9da3363fa3d1a334.css","style"]
0:{"P":null,"b":"salxnsTlxYptAH5Gn2n9j","p":"","c":["","ai","%5B7%5D-Monte-Carlo"],"i":false,"f":[[["",{"children":[["theme","ai","d"],{"children":[["post","%5B7%5D-Monte-Carlo","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/9da3363fa3d1a334.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"ko","children":["$","body",null,{"className":"__className_fa1bf2\n                min-h-screen\n                bg-slate-50 text-slate-900 dark:bg-slate-900 dark:text-slate-50","children":["$","div",null,{"className":"grid grid-cols-[24rem_1fr] auto-rows-auto","children":[["$","nav",null,{"className":"\n        __className_88a782\n        z-20\n        text-xl h-12 md:h-14 md:text-2xl\n        bg-slate-50/30 dark:bg-slate-900/30 border-b-slate-300 dark:border-b-slate-600\n        backdrop-blur-md\n        w-full\n        flex\n        items-center\n        justify-between\n        pl-5\n        pr-5\n        mb-20\n        border-b-2\n        border-b-slate-300\n        fixed","children":[["$","$L2",null,{"href":"/","children":"tomatoM4to's blog"}],["$","div",null,{"className":"hidden lg:flex items-center","children":[["$","input",null,{"type":"text","className":"w-36 h-7 rounded-full border-2 border-black pl-2","placeholder":"search"}],["$","div",null,{"className":"bg-slate-300 h-10 w-0.5 ml-2"}],["$","$L2",null,{"href":"https://github.com/tomatoM4to/tomatoM4to.github.io","className":"p-2 rounded-full hover:bg-slate-300 dark:hover:bg-slate-600 transition-colors duration-300","children":["$","svg",null,{"stroke":"currentColor","fill":"currentColor","strokeWidth":"0","viewBox":"0 0 16 16","className":"text-2xl cursor-pointer","children":["$undefined",[["$","path","0",{"fillRule":"evenodd","clipRule":"evenodd","d":"M7.976 0A7.977 7.977 0 0 0 0 7.976c0 3.522 2.3 6.507 5.431 7.584.392.049.538-.196.538-.392v-1.37c-2.201.49-2.69-1.076-2.69-1.076-.343-.93-.881-1.175-.881-1.175-.734-.489.048-.489.048-.489.783.049 1.224.832 1.224.832.734 1.223 1.859.88 2.3.685.048-.538.293-.88.489-1.076-1.762-.196-3.621-.881-3.621-3.964 0-.88.293-1.566.832-2.153-.05-.147-.343-.978.098-2.055 0 0 .685-.196 2.201.832.636-.196 1.322-.245 2.007-.245s1.37.098 2.006.245c1.517-1.027 2.202-.832 2.202-.832.44 1.077.146 1.908.097 2.104a3.16 3.16 0 0 1 .832 2.153c0 3.083-1.86 3.719-3.62 3.915.293.244.538.733.538 1.467v2.202c0 .196.146.44.538.392A7.984 7.984 0 0 0 16 7.976C15.951 3.572 12.38 0 7.976 0z","children":[]}]]],"style":{"color":"$undefined"},"height":"1em","width":"1em","xmlns":"http://www.w3.org/2000/svg"}]}],["$","button",null,{"className":"p-2 rounded-full hover:bg-slate-300 dark:hover:bg-slate-600 transition-colors duration-300","children":["$","svg",null,{"stroke":"currentColor","fill":"currentColor","strokeWidth":"0","viewBox":"0 0 24 24","className":"text-2xl cursor-pointer","children":["$undefined",[["$","path","0",{"fill":"none","strokeWidth":"2","d":"M12,23 C18.0751322,23 23,18.0751322 23,12 C23,5.92486775 18.0751322,1 12,1 C5.92486775,1 1,5.92486775 1,12 C1,18.0751322 5.92486775,23 12,23 Z M12,23 C15,23 16,18 16,12 C16,6 15,1 12,1 C9,1 8,6 8,12 C8,18 9,23 12,23 Z M2,16 L22,16 M2,8 L22,8","children":[]}]]],"style":{"color":"$undefined"},"height":"1em","width":"1em","xmlns":"http://www.w3.org/2000/svg"}]}],["$","$L3",null,{}]]}]]}],["$","main",null,{"className":"col-span-2","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]]}]}]}]]}],{"children":[["theme","ai","d"],["$","$1","c",{"children":[null,"$L6"]}],{"children":[["post","%5B7%5D-Monte-Carlo","d"],["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L7",["$","$L8",null,{"children":"$L9"}],null,["$","$La",null,{"children":["$Lb","$Lc",["$","$Ld",null,{"promise":"$@e"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","mF5xtPw04d_09vaorqGQ9",{"children":[["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],null]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:"$Sreact.suspense"
13:I[4911,[],"AsyncMetadata"]
9:["$","$12",null,{"fallback":null,"children":["$","$L13",null,{"promise":"$@14"}]}]
c:null
10:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
b:null
14:{"metadata":[["$","title","0",{"children":"Create Next App"}],["$","meta","1",{"name":"description","content":"Generated by create next app"}],["$","link","2",{"rel":"icon","href":"/icon.png?09be0199f64930e1","type":"image/png","sizes":"500x500"}]],"error":null,"digest":"$undefined"}
e:{"metadata":"$14:metadata","error":null,"digest":"$undefined"}
15:I[6903,["87","static/chunks/0e762574-2e51da01949c836e.js","206","static/chunks/5e22fd23-3096271f6014a2ab.js","949","static/chunks/578c2090-be88bc710a39c490.js","874","static/chunks/874-8826d48c805a2f7c.js","402","static/chunks/402-8a45e42f5bc1d79a.js","859","static/chunks/app/%5Btheme%5D/layout-89361a932fedae41.js"],"Hamburger"]
16:I[5830,["87","static/chunks/0e762574-2e51da01949c836e.js","206","static/chunks/5e22fd23-3096271f6014a2ab.js","949","static/chunks/578c2090-be88bc710a39c490.js","874","static/chunks/874-8826d48c805a2f7c.js","402","static/chunks/402-8a45e42f5bc1d79a.js","859","static/chunks/app/%5Btheme%5D/layout-89361a932fedae41.js"],"NonAccordionLink"]
17:I[5830,["87","static/chunks/0e762574-2e51da01949c836e.js","206","static/chunks/5e22fd23-3096271f6014a2ab.js","949","static/chunks/578c2090-be88bc710a39c490.js","874","static/chunks/874-8826d48c805a2f7c.js","402","static/chunks/402-8a45e42f5bc1d79a.js","859","static/chunks/app/%5Btheme%5D/layout-89361a932fedae41.js"],"Accordion"]
18:I[5830,["87","static/chunks/0e762574-2e51da01949c836e.js","206","static/chunks/5e22fd23-3096271f6014a2ab.js","949","static/chunks/578c2090-be88bc710a39c490.js","874","static/chunks/874-8826d48c805a2f7c.js","402","static/chunks/402-8a45e42f5bc1d79a.js","859","static/chunks/app/%5Btheme%5D/layout-89361a932fedae41.js"],"AccordionItem"]
6:["$","div",null,{"className":"flex","children":[["$","$L15",null,{"res":[{"includeHyphen":false,"firstOrder":1,"secondOrder":-1,"URL":"[1]-Introduction","title":"Introduction","contentList":[]},{"includeHyphen":false,"firstOrder":2,"secondOrder":-1,"URL":"[2]-Basis-math","title":"Basis math","contentList":[{"includeHyphen":true,"firstOrder":2,"secondOrder":1,"URL":"[2-1]-Math-with-RL","title":"Math with RL","contentList":[]}]},{"includeHyphen":false,"firstOrder":3,"secondOrder":-1,"URL":"[3]-Q-learning","title":"Q learning","contentList":[{"includeHyphen":true,"firstOrder":3,"secondOrder":1,"URL":"[3-1]-Basic-Concept","title":"Basic Concept","contentList":[]},{"includeHyphen":true,"firstOrder":3,"secondOrder":2,"URL":"[3-2]-Greedy-action","title":"Greedy action","contentList":[]},{"includeHyphen":true,"firstOrder":3,"secondOrder":3,"URL":"[3-3]-Discount-factor","title":"Discount factor","contentList":[]},{"includeHyphen":true,"firstOrder":3,"secondOrder":4,"URL":"[3-4]-Learning-rate","title":"Learning rate","contentList":[]}]},{"includeHyphen":false,"firstOrder":4,"secondOrder":-1,"URL":"[4]-Markov-process","title":"Markov process","contentList":[]},{"includeHyphen":false,"firstOrder":5,"secondOrder":-1,"URL":"[5]-Bellman-Optimality-Equation","title":"Bellman Optimality Equation","contentList":[]},{"includeHyphen":false,"firstOrder":6,"secondOrder":-1,"URL":"[6]-DP","title":"DP","contentList":[]},{"includeHyphen":false,"firstOrder":7,"secondOrder":-1,"URL":"[7]-Monte-Carlo","title":"Monte Carlo","contentList":[]},{"includeHyphen":false,"firstOrder":8,"secondOrder":-1,"URL":"[8]-Temporal-Difference-Learning","title":"Temporal Difference Learning","contentList":[]},{"includeHyphen":false,"firstOrder":9,"secondOrder":-1,"URL":"[9]-nSTEP-Bootstrapping","title":"nSTEP Bootstrapping","contentList":[]},{"includeHyphen":false,"firstOrder":10,"secondOrder":-1,"URL":"[10]-Planning-and-Learning","title":"Planning and Learning","contentList":[{"includeHyphen":true,"firstOrder":10,"secondOrder":1,"URL":"[10-1]-Function-Approximation","title":"Function Approximation","contentList":[]}]},{"includeHyphen":false,"firstOrder":11,"secondOrder":-1,"URL":"[11]-Deep-learning","title":"Deep learning","contentList":[{"includeHyphen":true,"firstOrder":11,"secondOrder":1,"URL":"[11-1]-Concept","title":"Concept","contentList":[]},{"includeHyphen":true,"firstOrder":11,"secondOrder":2,"URL":"[11-2]-Newerl-Network","title":"Newerl Network","contentList":[]},{"includeHyphen":true,"firstOrder":11,"secondOrder":3,"URL":"[11-3]-Loss-function","title":"Loss function","contentList":[]},{"includeHyphen":true,"firstOrder":11,"secondOrder":4,"URL":"[11-4]-Activation-function","title":"Activation function","contentList":[]},{"includeHyphen":true,"firstOrder":11,"secondOrder":5,"URL":"[11-5]-Gradient-descent","title":"Gradient descent","contentList":[]},{"includeHyphen":true,"firstOrder":11,"secondOrder":6,"URL":"[11-6]-Back-Propagation","title":"Back Propagation","contentList":[]}]},{"includeHyphen":false,"firstOrder":12,"secondOrder":-1,"URL":"[12]-Q-Network","title":"Q Network","contentList":[{"includeHyphen":true,"firstOrder":12,"secondOrder":1,"URL":"[12-1]-Overview","title":"Overview","contentList":[]},{"includeHyphen":true,"firstOrder":12,"secondOrder":2,"URL":"[12-2]-Frozen-Lake","title":"Frozen Lake","contentList":[]},{"includeHyphen":true,"firstOrder":12,"secondOrder":3,"URL":"[12-3]-Cartpole","title":"Cartpole","contentList":[]}]},{"includeHyphen":false,"firstOrder":13,"secondOrder":-1,"URL":"[13]-DQN","title":"DQN","contentList":[{"includeHyphen":true,"firstOrder":13,"secondOrder":1,"URL":"[13-1]-Concept","title":"Concept","contentList":[]},{"includeHyphen":true,"firstOrder":13,"secondOrder":2,"URL":"[13-2]-Cartpole","title":"Cartpole","contentList":[]}]},{"includeHyphen":false,"firstOrder":14,"secondOrder":-1,"URL":"[14]-Policy-gradient","title":"Policy gradient","contentList":[{"includeHyphen":true,"firstOrder":14,"secondOrder":1,"URL":"[14-1]-A3C","title":"A3C","contentList":[]}]}]}],["$","aside",null,{"className":"\n                hidden lg:flex w-64 2xl:w-96\n                border-r-slate-300 dark:border-r-slate-600\n                flex-col\n                h-screen\n                border-r-2\n                p-1\n                pl-5\n                pt-14\n                fixed\n                overflow-y-auto\n                overscroll-contain\n                z-auto\n                ","children":[["$","$L16","0",{"href":"./[1]-Introduction","label":"Introduction"}],["$","$L17","1",{"label":"Basis math","children":["$","$L18",null,{"contentList":"$6:props:children:0:props:res:1:contentList"}]}],["$","$L17","2",{"label":"Q learning","children":["$","$L18",null,{"contentList":"$6:props:children:0:props:res:2:contentList"}]}],["$","$L16","3",{"href":"./[4]-Markov-process","label":"Markov process"}],["$","$L16","4",{"href":"./[5]-Bellman-Optimality-Equation","label":"Bellman Optimality Equation"}],["$","$L16","5",{"href":"./[6]-DP","label":"DP"}],["$","$L16","6",{"href":"./[7]-Monte-Carlo","label":"Monte Carlo"}],["$","$L16","7",{"href":"./[8]-Temporal-Difference-Learning","label":"Temporal Difference Learning"}],["$","$L16","8",{"href":"./[9]-nSTEP-Bootstrapping","label":"nSTEP Bootstrapping"}],["$","$L17","9",{"label":"Planning and Learning","children":["$","$L18",null,{"contentList":"$6:props:children:0:props:res:9:contentList"}]}],["$","$L17","10",{"label":"Deep learning","children":["$","$L18",null,{"contentList":"$6:props:children:0:props:res:10:contentList"}]}],["$","$L17","11",{"label":"Q Network","children":["$","$L18",null,{"contentList":"$6:props:children:0:props:res:11:contentList"}]}],["$","$L17","12",{"label":"DQN","children":["$","$L18",null,{"contentList":"$6:props:children:0:props:res:12:contentList"}]}],["$","$L17","13",{"label":"Policy gradient","children":["$","$L18",null,{"contentList":"$6:props:children:0:props:res:13:contentList"}]}]]}],["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
7:["$","div",null,{"className":"lg:ml-64 2xl:ml-96 mt-32 mb-32 flex-1 flex flex-col items-center overflow-x-hidden","children":[["$","div",null,{"className":"w-11/12 lg:w-2/3 2xl:w-1/2 markdown-body","children":[["$","h1",null,{"children":"How can we get Q*","className":" text-3xl md:text-4xl font-extrabold my-6 text-transparent bg-clip-text bg-gradient-to-r from-blue-500 via-purple-500 to-pink-500 dark:from-blue-400 dark:via-purple-400 dark:to-pink-400"}],"\n",["$","p",null,{"children":"그래서 Q* 값을 얻을수 있을까? 에 대한 의문이 생깁니다. 사실 단번에 얻는거는 불가능하다 봐야하고, 알고리즘을 통해 수렴 하는 방법을 찾아야 합니다."}],"\n",["$","p",null,{"children":["episode를 진행하면서 Q를 점점 업데이트 하며 Q",["$","em",null,{"children":["에 점점 다가가게 하고, 그러면 최종적으로 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"하게 greedy 하게 하면 됩니다. 트레이닝이 끝난다면 이 Q값을 Q"]}],"라 믿고 사용하면 됩니다. 더이상 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"(탐험)을 수행 안해도 됩니다. 물론 잘 안되면 다시 ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"ϵ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\epsilon"}]]}]}]}],"을 수행해야 합니다."]}],"\n",["$","p",null,{"children":"이전 미로찾기 문제에서 Q-learning의 q값을 계속 업데이트 했는데 이 과정이 Q*를 찾기 위한 과정입니다."}],"\n",["$","p",null,{"children":"이렇듯 여러번 수많은 episode를 통해 Q*를 구하는 방법에는 크게 2가지가 있습니다. 그중 첫번쨰인 Monte-Carlo 방법에 대해 알아보겠습니다."}],"\n",["$","hr",null,{}],"\n",["$","h1",null,{"children":"Monte-Carlo Methods","className":" text-3xl md:text-4xl font-extrabold my-6 text-transparent bg-clip-text bg-gradient-to-r from-blue-500 via-purple-500 to-pink-500 dark:from-blue-400 dark:via-purple-400 dark:to-pink-400"}],"\n",["$","hr",null,{}],"\n",["$","h1",null,{"children":"학습 방법","className":" text-3xl md:text-4xl font-extrabold my-6 text-transparent bg-clip-text bg-gradient-to-r from-blue-500 via-purple-500 to-pink-500 dark:from-blue-400 dark:via-purple-400 dark:to-pink-400"}],"\n",["$","p",null,{"children":"다이나믹 프로그래밍 =! 강화학습이란걸 알아야함, 따로따로 발발전하다 나나중에 합친 사람이 대단한거지"}],"\n",["$","p",null,{"children":"프리딕션 == 애벌루에이션, 똑같은의미임"}],"\n",["$","p",null,{"children":"S0 -> A0: 여기서파이(S0) 수행 후-> R0"}],"\n",["$","p",null,{"children":"애피소드라는걸 다 알수 있단거는, 지금까지 아는대 한에선수익을 전부 알수있다"}],"\n",["$","h1",null,{"children":"First visitVS. Every visit","className":" text-3xl md:text-4xl font-extrabold my-6 text-transparent bg-clip-text bg-gradient-to-r from-blue-500 via-purple-500 to-pink-500 dark:from-blue-400 dark:via-purple-400 dark:to-pink-400"}],"\n",["$","p",null,{"children":"미로찾기에서 만약내가 왔던길을 다시 가거 다시 반복하는 x1 -> x2 -> x1 -> x2.."}],"\n",["$","p",null,{"children":"이런식으로 됄때 어느걸 선택할가에 대한 것"}],"\n",["$","hr",null,{}],"\n",["$","p",null,{"children":"몬테 카를로는 수렴할때까지 하지 않않음, 그래서 의사코드에서 while 빼고 그냥 마노이 돌돌려도됌"}],"\n",["$","p",null,{"children":"환환경이 없기 때문문에 강화학습에선 행동 가치함수만 사용하게 됀다."}],"\n",["$","p",null,{"children":"몬테카를로에선 우리가 아는 부분에서만 가능하고, 모르는애들한테는 모모름, 환경을 아예 모르기 때문에"}],"\n",["$","p",null,{"children":"모르는 애들들한테 어떤 정책? 행행동을 수행할지지에 대해선 모험과 착취가 있겠는데"}],"\n",["$","p",null,{"children":"입실론 Greedy라고더 괜찮은 방법이 있음"}],"\n",["$","p",null,{"children":"몬테카를로에선 현재의 정책이 과거의 정책보다 더 좋다는걸 알수 이어야한다."}],"\n",["$","p",null,{"children":"입실론 그리디를 했을때, 과거의 정책볻 현재가 더 좋다는걸 증명, 알수 있을까?"}],"\n",["$","p",null,{"children":"결국 최적의 정책으 찾을수 있는지?"}],"\n",["$","p",null,{"children":"시시그마 파이((a'|s) -e/||A|)/1-e= 1"}],"\n",["$","p",null,{"children":"온폴리시 몬테카카를로의 문제는 e/|A| 확률로만 탐험을 하게 됀다, 한마디로 자기가 아는 조그만 지식 내에서 몬갈 하는데 결국 탐험이 부족함"}],"\n",["$","p",null,{"children":"Off Policy가 강화학습에서 대부분 사용됌, 근데 좀 느리긴 함"}],"\n",["$","p",null,{"children":"예를들어 과제기간 3일남았는데 Off policy 쓰면 과제 못함, 학습시간이 3일, 2일이면 On policy 써야 함"}],"\n",["$","p",null,{"children":"DP랑 강화학습은 사실 완전히 다른 방향으로 발전했고 일부분 비슷한게 있어서 배운것 뿐이지 사실 그그냥  다른거임"}],"\n",["$","hr",null,{}],"\n",["$","h1",null,{"children":"Off-Policy Monye-Calo Methods","className":" text-3xl md:text-4xl font-extrabold my-6 text-transparent bg-clip-text bg-gradient-to-r from-blue-500 via-purple-500 to-pink-500 dark:from-blue-400 dark:via-purple-400 dark:to-pink-400"}],"\n",["$","p",null,{"children":"최적의 정책을 찾는 문제의 딜레마가 있음, 최적의 행동에 대한 가치함수를 학습을 해야함, V*를 알아야 함, 근데 이걸 하려면 최적이 아닌 행동을 해야함."}],"\n",["$","p",null,{"children":"On-Policy는 그래서 최적의 행동에 대한 가치 함수를 학습하는 대신..(ppt 내용용"}],"\n",["$","p",null,{"children":"Behavior Policy는 에피소드를 다음과 같이 생성: 배울 필요 없음, 그냥 아무거나 막 생성함 다 같은확률로"}],"\n",["$","p",null,{"children":"감마(s) 는 현재 시간에 대한 타임스탬프/ V파이(s) 가 아니라 사실 Vb(s) 다?"}],"\n",["$","p",null,{"children":"핵심: 파랑줄"}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"children":"Importance Sampling","className":" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200"}],"\n",["$","p",null,{"children":"Gt: 랜덤 긷댓값, ㄱ냥 랜덤값?"}],"\n",["$","h2",null,{"children":"Estimtor","className":" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200"}],"\n",["$","p",null,{"children":"사실 이러한 과저이 그렇게 간단하진 않음"}],"\n",["$","p",null,{"children":"T(t) 에피소드가 언제끝났냐에대한 예기임"}],"\n",["$","p",null,{"children":"공식상으론 맞는데 분산이 커지는걸 막아야함 -> 분모를 증가시켜야함"}],"\n",["$","p",null,{"children":"[다음 ppt장]그래서 Off Policy 몬테카를를로 방식을 살거면 무지성으로 이걸 쓰면 잘 됀다."}],"\n",["$","p",null,{"children":"왜 지금까지 V파이를 계산했나요 Q파이를 계계산하지, 더 짧으니까.. 비슷하게 생겼으니까.. V를 Q로, a 추가가하면 끝이니까"}],"\n",["$","h2",null,{"children":"Peudocode","className":" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200"}],"\n",["$","h2",null,{"children":"Importance Sampling","className":" text-2xl md:text-3xl font-bold my-5 text-purple-600 hover:text-purple-800 dark:text-purple-400 dark:hover:text-purple-500 transition-colors duration-200"}],"\n",["$","p",null,{"children":"감마: 할인율을 고려 안한 문문제가 발생해서 고려해야함"}],"\n",["$","p",null,{"children":"여기서 수식중에 - 여야 하는게 +로 써져있는 오류가 있음"}],"\n",["$","p",null,{"children":"오더네리 펄 디시션 어쩌구[마지막 ppt 내용용"}],"\n",["$","p",null,{"children":"증명명돼지 않고.. 그냥 잘 돼더라"}],"\n",["$","p",null,{"children":"옾폴리시 몬테 카를를로는 아직연구가  많이 필요하다 한다."}],"\n",["$","p",null,{"children":"실제론 MC 보단 TD를 많이 씀, 아마 과제도 TD를 할할거임"}]]}],["$","div",null,{"className":"\n            flex\n            flex-col sm:flex-row\n            justify-between\n            items-center\n            w-full\n            mt-8\n            pt-4\n            border-t-slate-300 dark:border-t-slate-600\n            border-t-2\n            w-11/12 lg:w-2/3 2xl:w-1/2\n        ","children":[["$","$L2",null,{"href":"[6]-DP","className":"\n                w-full\n                flex\n                gap-2\n                justify-start\n                items-center\n                font-medium\n                py-2 sm:py-4\n                px-2\n                rounded-lg\n                hover:bg-slate-300 dark:hover:bg-slate-600\n            ","children":[["$","svg",null,{"stroke":"currentColor","fill":"currentColor","strokeWidth":"0","viewBox":"0 0 512 512","children":["$undefined",[["$","path","0",{"d":"M217.9 256L345 129c9.4-9.4 9.4-24.6 0-33.9-9.4-9.4-24.6-9.3-34 0L167 239c-9.1 9.1-9.3 23.7-.7 33.1L310.9 417c4.7 4.7 10.9 7 17 7s12.3-2.3 17-7c9.4-9.4 9.4-24.6 0-33.9L217.9 256z","children":[]}]]],"className":"$undefined","style":{"color":"$undefined"},"height":"1em","width":"1em","xmlns":"http://www.w3.org/2000/svg"}],["$","span",null,{"children":"DP"}],false]}],["$","$L2",null,{"href":"[8]-Temporal-Difference-Learning","className":"\n                w-full\n                flex\n                gap-2\n                justify-end\n                items-center\n                font-medium\n                py-2 sm:py-4\n                px-2\n                rounded-lg\n                hover:bg-slate-300 dark:hover:bg-slate-600\n            ","children":[false,["$","span",null,{"children":"Temporal Difference Learning"}],["$","svg",null,{"stroke":"currentColor","fill":"currentColor","strokeWidth":"0","viewBox":"0 0 512 512","children":["$undefined",[["$","path","0",{"d":"M294.1 256L167 129c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.3 34 0L345 239c9.1 9.1 9.3 23.7.7 33.1L201.1 417c-4.7 4.7-10.9 7-17 7s-12.3-2.3-17-7c-9.4-9.4-9.4-24.6 0-33.9l127-127.1z","children":[]}]]],"className":"$undefined","style":{"color":"$undefined"},"height":"1em","width":"1em","xmlns":"http://www.w3.org/2000/svg"}]]}]]}]]}]
