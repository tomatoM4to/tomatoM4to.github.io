3:I[9275,[],""]
6:I[1343,[],""]
8:I[231,["217","static/chunks/578c2090-c3154b89dc3bb825.js","231","static/chunks/231-87925b9c7247c60f.js","347","static/chunks/347-809ad2a9645bf6ea.js","874","static/chunks/app/%5Bsubject%5D/layout-b0de0e566f81dfd1.js"],""]
4:["subject","ai","d"]
5:["post","%5B1%5D-RL-overview","d"]
0:["vIpdwy1K0aOTtCAIblcSX",[[["",{"children":[["subject","ai","d"],{"children":[["post","%5B1%5D-RL-overview","d"],{"children":["__PAGE__?{\"subject\":\"ai\",\"post\":\"[1]-RL-overview\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":[["subject","ai","d"],{"children":[["post","%5B1%5D-RL-overview","d"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children","$5","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/dd89e605550f760e.css","precedence":"next","crossOrigin":"$undefined"}]]}],null]},["$L7",null],null]},[["$","html",null,{"lang":"ko","children":["$","body",null,{"className":"__className_1e1d11 min-h-screen","children":["$","div",null,{"className":"grid grid-cols-[24rem_1fr] auto-rows-auto","children":[["$","nav",null,{"className":"bg-white w-full h-14 flex items-center justify-between pl-5 pr-5 mb-20 border-b-2 border-b-slate-300 text-2xl fixed","children":[["$","$L8",null,{"href":"/","children":"tomatoM4to's blog"}],["$","div",null,{"className":"hidden lg:flex items-center","children":[["$","input",null,{"type":"text","className":"w-36 h-7 rounded-full border-2 border-black pl-2","placeholder":"search"}],["$","div",null,{"className":"bg-slate-300 h-10 w-0.5 ml-2"}],["$","$L8",null,{"href":"https://github.com/tomatoM4to/tomatoM4to.github.io","className":"p-2 rounded-full hover:bg-gray-300 transition-colors duration-300","children":["$","svg",null,{"stroke":"currentColor","fill":"currentColor","strokeWidth":"0","viewBox":"0 0 16 16","className":"text-2xl cursor-pointer","children":["$undefined",[["$","path","0",{"fillRule":"evenodd","clipRule":"evenodd","d":"M7.976 0A7.977 7.977 0 0 0 0 7.976c0 3.522 2.3 6.507 5.431 7.584.392.049.538-.196.538-.392v-1.37c-2.201.49-2.69-1.076-2.69-1.076-.343-.93-.881-1.175-.881-1.175-.734-.489.048-.489.048-.489.783.049 1.224.832 1.224.832.734 1.223 1.859.88 2.3.685.048-.538.293-.88.489-1.076-1.762-.196-3.621-.881-3.621-3.964 0-.88.293-1.566.832-2.153-.05-.147-.343-.978.098-2.055 0 0 .685-.196 2.201.832.636-.196 1.322-.245 2.007-.245s1.37.098 2.006.245c1.517-1.027 2.202-.832 2.202-.832.44 1.077.146 1.908.097 2.104a3.16 3.16 0 0 1 .832 2.153c0 3.083-1.86 3.719-3.62 3.915.293.244.538.733.538 1.467v2.202c0 .196.146.44.538.392A7.984 7.984 0 0 0 16 7.976C15.951 3.572 12.38 0 7.976 0z","children":[]}]]],"style":{"color":"$undefined"},"height":"1em","width":"1em","xmlns":"http://www.w3.org/2000/svg"}]}],["$","button",null,{"className":"p-2 rounded-full hover:bg-gray-300 transition-colors duration-300","children":["$","svg",null,{"stroke":"currentColor","fill":"currentColor","strokeWidth":"0","viewBox":"0 0 24 24","className":"text-2xl cursor-pointer","children":["$undefined",[["$","path","0",{"fill":"none","strokeWidth":"2","d":"M12,23 C18.0751322,23 23,18.0751322 23,12 C23,5.92486775 18.0751322,1 12,1 C5.92486775,1 1,5.92486775 1,12 C1,18.0751322 5.92486775,23 12,23 Z M12,23 C15,23 16,18 16,12 C16,6 15,1 12,1 C9,1 8,6 8,12 C8,18 9,23 12,23 Z M2,16 L22,16 M2,8 L22,8","children":[]}]]],"style":{"color":"$undefined"},"height":"1em","width":"1em","xmlns":"http://www.w3.org/2000/svg"}]}],["$","button",null,{"className":"p-2 rounded-full hover:bg-gray-300 transition-colors duration-300","children":["$","svg",null,{"stroke":"currentColor","fill":"currentColor","strokeWidth":"0","viewBox":"0 0 16 16","className":"text-2xl cursor-pointer","children":["$undefined",[["$","path","0",{"d":"M6 .278a.77.77 0 0 1 .08.858 7.2 7.2 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277q.792-.001 1.533-.16a.79.79 0 0 1 .81.316.73.73 0 0 1-.031.893A8.35 8.35 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.75.75 0 0 1 6 .278","children":[]}],["$","path","1",{"d":"M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.73 1.73 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.73 1.73 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.73 1.73 0 0 0 1.097-1.097zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.16 1.16 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.16 1.16 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732z","children":[]}]]],"style":{"color":"$undefined"},"height":"1em","width":"1em","xmlns":"http://www.w3.org/2000/svg"}]}]]}]]}],["$","main",null,{"className":"col-span-2","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/ea9287ddd32ae283.css","precedence":"next","crossOrigin":"$undefined"}]]}]}]]}]}]}],null],null],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/40f13b8fccf4d106.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/ffa3379a40279862.css","precedence":"next","crossOrigin":"$undefined"}]],"$L9"]]]]
9:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1, maximum-scale=1, user-scalable=yes"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"tomatom4to's Computer Science Blog"}],["$","meta","3",{"name":"description","content":"Comprehensive computer science knowledge covering OS, Database, AI, Networks, Linux, and Docker. Learn computer science concepts with clear explanations and practical examples."}],["$","meta","4",{"name":"author","content":"tomatom4to"}],["$","meta","5",{"name":"keywords","content":"Computer Science,Operating Systems,Database,AI,Network,Linux,Docker,Programming,Software Development"}],["$","meta","6",{"name":"creator","content":"tomatom4to"}],["$","meta","7",{"name":"robots","content":"index, follow"}],["$","meta","8",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","9",{"rel":"canonical","href":"https://tomatom4to.github.io"}],["$","meta","10",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","11",{"property":"og:title","content":"tomatom4to's Computer Science Blog"}],["$","meta","12",{"property":"og:description","content":"Your gateway to comprehensive computer science knowledge and practical programming skills"}],["$","meta","13",{"property":"og:url","content":"https://tomatom4to.github.io"}],["$","meta","14",{"property":"og:site_name","content":"tomatom4to's CS Blog"}],["$","meta","15",{"property":"og:locale","content":"ko_KR"}],["$","meta","16",{"property":"og:type","content":"website"}],["$","meta","17",{"name":"twitter:card","content":"summary"}],["$","meta","18",{"name":"twitter:title","content":"tomatom4to's Computer Science Blog"}],["$","meta","19",{"name":"twitter:description","content":"Your gateway to comprehensive computer science knowledge and practical programming skills"}],["$","link","20",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","meta","21",{"name":"next-size-adjust"}]]
1:null
a:I[8263,["217","static/chunks/578c2090-c3154b89dc3bb825.js","231","static/chunks/231-87925b9c7247c60f.js","347","static/chunks/347-809ad2a9645bf6ea.js","874","static/chunks/app/%5Bsubject%5D/layout-b0de0e566f81dfd1.js"],"SideMenu"]
7:["$","div",null,{"className":"flex","children":[["$","$La",null,{"res":[{"isOutLine":false,"firstOrder":1,"secondOrder":-1,"order":"1","title":"RL overview","originalName":"[1]-RL-overview"},{"isOutLine":false,"firstOrder":2,"secondOrder":-1,"order":"2","title":"Basis math","originalName":"[2]-Basis-math"},{"isOutLine":true,"firstOrder":2,"secondOrder":1,"order":"2-1","title":"Math with RL","originalName":"[2-1]-Math-with-RL"},{"isOutLine":false,"firstOrder":3,"secondOrder":-1,"order":"3","title":"Q learning","originalName":"[3]-Q-learning"},{"isOutLine":false,"firstOrder":4,"secondOrder":-1,"order":"4","title":"Markov process","originalName":"[4]-Markov-process"},{"isOutLine":false,"firstOrder":5,"secondOrder":-1,"order":"5","title":"Bellman Optimality Equation","originalName":"[5]-Bellman-Optimality-Equation"},{"isOutLine":false,"firstOrder":6,"secondOrder":-1,"order":"6","title":"DP","originalName":"[6]-DP"},{"isOutLine":false,"firstOrder":7,"secondOrder":-1,"order":"7","title":"Monte Carlo","originalName":"[7]-Monte-Carlo"},{"isOutLine":false,"firstOrder":8,"secondOrder":-1,"order":"8","title":"Temporal Difference Learning","originalName":"[8]-Temporal-Difference-Learning"},{"isOutLine":false,"firstOrder":9,"secondOrder":-1,"order":"9","title":"nSTEP Bootstrapping","originalName":"[9]-nSTEP-Bootstrapping"},{"isOutLine":false,"firstOrder":10,"secondOrder":-1,"order":"10","title":"Planning and Learning","originalName":"[10]-Planning-and-Learning"},{"isOutLine":true,"firstOrder":10,"secondOrder":1,"order":"10-1","title":"Function Approximation","originalName":"[10-1]-Function-Approximation"},{"isOutLine":false,"firstOrder":11,"secondOrder":-1,"order":"11","title":"Deep learning overview","originalName":"[11]-Deep-learning-overview"}],"params":{"subject":"ai"}}],["$","div",null,{"className":"hidden lg:flex w-64 2xl:w-96 flex-col h-screen border-r-2 border-gray-300 mt-14 p-1 pl-5 fixed overflow-y-auto overscroll-contain","children":[["$","$L8",null,{"href":"/ai/[1]-RL-overview","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["1",". ","RL overview"]}],["$","$L8",null,{"href":"/ai/[2]-Basis-math","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["2",". ","Basis math"]}],["$","$L8",null,{"href":"/ai/[2-1]-Math-with-RL","className":"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["2-1",". ","Math with RL"]}],["$","$L8",null,{"href":"/ai/[3]-Q-learning","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["3",". ","Q learning"]}],["$","$L8",null,{"href":"/ai/[4]-Markov-process","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["4",". ","Markov process"]}],["$","$L8",null,{"href":"/ai/[5]-Bellman-Optimality-Equation","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["5",". ","Bellman Optimality Equation"]}],["$","$L8",null,{"href":"/ai/[6]-DP","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["6",". ","DP"]}],["$","$L8",null,{"href":"/ai/[7]-Monte-Carlo","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["7",". ","Monte Carlo"]}],["$","$L8",null,{"href":"/ai/[8]-Temporal-Difference-Learning","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["8",". ","Temporal Difference Learning"]}],["$","$L8",null,{"href":"/ai/[9]-nSTEP-Bootstrapping","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["9",". ","nSTEP Bootstrapping"]}],["$","$L8",null,{"href":"/ai/[10]-Planning-and-Learning","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["10",". ","Planning and Learning"]}],["$","$L8",null,{"href":"/ai/[10-1]-Function-Approximation","className":"pl-4 px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["10-1",". ","Function Approximation"]}],["$","$L8",null,{"href":"/ai/[11]-Deep-learning-overview","className":"undefined px-2 py-1 mb-1 hover:bg-gray-300 transition-colors rounded-lg","onClick":"$undefined","children":["11",". ","Deep learning overview"]}]]}],["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}]]}]
2:["$","div",null,{"className":"lg:ml-64 2xl:ml-96 mt-32 mb-32 flex-1 flex flex-col items-center overflow-x-hidden","children":["$","div",null,{"className":"w-11/12 md:w-3/4 lg:w-2/3 2xl:w-1/2 markdown-body","children":[["$","h1",null,{"children":"Reinforcement Learning"}],"\n",["$","p",null,{"children":["배고픈 고양이를 버튼을 눌러야 문이 열리는 특별한 상자에 가두고, 상자 밖에는 먹이를 두었다고 사정해 봅시다. 만약 고양이가 우연히 버튼을 누르게 되면 상자 밖으로 빠져나와 사료를 먹을수 있을겁니다. 이를 반복하게 되면, 고양이는 ",["$","span",null,{"style":{"color":"red"},"children":"버튼을 누르는 것이 긍정적인 결과를 낸다"}]," 를 학습하고 점점 더 상자에서 빠져나오는 시간이 짧아지게 될겁니다. 이것은, 버튼을 누르는 행동과 긍적적인 결과간의 관계가 강화(Reinforcement)되었다 라고 할수 있습니다. 이것이 바로 강화학습의 기본적인 개념입니다."]}],"\n",["$","p",null,{"children":"이를 위에서 배웠던 용어로 다시 정리해 보겠습니다."}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"에이전트(Agent)"}],": 고양이"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"환경(Environment)"}],": 고양이를 제외한 모든것"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"상태(State)"}],": 고양이가 상자 안에 있는 상황, 상자 밖에 있는 상황 등 현재 환경의 상태"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"행동(Action)"}],": 버튼을 누르거나, 움직이거나 하는 등 에이전트가 선택할 수 있는 모든 행동들"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"정책(Policy)"}],": 특정 상태에서 어떤 행동을 선택할지 결정하는 전략 (예: 배고픈 상태에서 버튼을 누르는 행동을 선택)"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"보상(Reward)"}],": 행동의 결과로 얻는 피드백 (예: 먹이를 먹었을 때의 만족감)"]}],"\n"],"className":"list-disc ml-4"}],"\n",["$","p",null,{"children":["추가적으로 강화학습의의 중요한 핵심적인 특징중 하나는, ",["$","strong",null,{"children":"버튼을 누르면 왜 문이 열리는가?"}]," 와 같은 원리를 이해할 필요가 없습니다. 대신 ",["$","strong",null,{"children":"이 상황(상태)에서 이 행동을 했더니 좋은 결과가 났다"}],"라는 사실만 알면 됩니다."]}],"\n",["$","p",null,{"children":"아래 예시로 보여드릴 딥바인드에서 만든 게임 AI는 화면, 점수, 오른쪽 왼쪽밖에 모른다고 합니다. 이 AI는 화면에서 어떤 행동을 해야 좋은 결과를 얻는지를 학습하게 됩니다. 아래 예시에선 왼쪽, 오른쪽 밖에 없는 간단한 상황이지만, 운전시 핸들을 어떻게 돌려야 하는지 같은 각도 같은 연속적인 이러한 경우는 훨씬 어렵기 때문에 100000번, 20000번 시도해 우연히 성공하듯 학습하게 됩니다. 추가적으로 알파고 제로 또한 바둑이 뭔지 모르는 상태에서 학습을 시작합니다."}],"\n",["$","h2",null,{"children":"Characteristics: Optimalization"}],"\n",["$","p",null,{"children":"강화학습은 최대의 보상을 얻을수 있는 최적의 정책을 학습하는 것이 목표 입니다.\n예를들어, 체스에서는 승리하는 것이 최대의 보상이 될겁니다. 혹은 최단거리로 미로를 빠져 나가는 경로를 찾는 경우도 있을수 있거요."}],"\n",["$","p",null,{"children":"주요한 특징은, 현재 선택한 행동에"}],"\n",["$","h2",null,{"children":"특징: 최적화"}],"\n",["$","p",null,{"children":"기본적으로 강화학습은 최적화를 푸는 문제"}],"\n",["$","p",null,{"children":"중요한거 어떤 가치가 있는지 명시적으로 표표현된다."}],"\n",["$","p",null,{"children":"가와학습의 모델은 옭고 그름을 알지 못하고 어떤 보상상이 주어지는지다."}],"\n",["$","h2",null,{"children":"순차적"}],"\n",["$","p",null,{"children":"직관적으로 당연"}],"\n",["$","h2",null,{"children":"Delayed Consequences"}],"\n",["$","p",null,{"children":"체스를 예로들면 지금의 수가 당장은 손해더라도 끝까지 갔을때때의 보상이 큰 행도을 학습(승리)"}],"\n",["$","h2",null,{"children":"Exploration"}],"\n",["$","p",null,{"children":"손 했을때 손을 물었을때, 손으 올렸다면 머기가 주어졌단 사실을 모름(대부분)"}],"\n",["$","hr",null,{"className":"border-t-4 border-gray-300 mt-10 mb-10","children":"$undefined"}],"\n",["$","h1",null,{"children":"WhereReinforcementLearning?"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":"최적의정책을인간이알수없는경우"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"예.인간의능력을뛰어넘는최적의정책을찾기(AlphaGoZero등)"}],"\n",["$","li",null,{"children":"예.기존에알려지지않은분야에서최적의정책을찾기(핵융합로플라즈마 제어 등)"}],"\n"],"className":"list-disc ml-4"}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":"현재시점에서선택된행동의(최종적인)결과를나중에라도알수있는 경우"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"예.AtariBreakout:점수"}],"\n",["$","li",null,{"children":"예.체스나바둑:승패여부"}],"\n"],"className":"list-disc ml-4"}],"\n"]}],"\n"],"className":"list-disc ml-4"}],"\n",["$","hr",null,{"className":"border-t-4 border-gray-300 mt-10 mb-10","children":"$undefined"}],"\n",["$","h1",null,{"children":"Workflow"}],"\n",["$","p",null,{"children":"Workflow를 보기 전 알아두어야 할점은 상태 하나하나 시간이 존재한다는것입니다."}],"\n",["$","p",null,{"children":["$","img",null,{"src":"./img/RL-workflow.png","alt":"Reinforcement Learning Workflow"}]}],"\n",["$","p",null,{"children":"순서대로 살펴보겠습니다."}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":["강화 학습 Agent는 환경의 상태를 인지 합니다.","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Environment -> Agent:"}]," ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"S"}],["$","mi",null,{"children":"t"}]]}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"s"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"S_{t} = s"}]]}]}]}]]}],"\n"],"className":"list-disc ml-4"}],"\n"]}],"\n",["$","li",null,{"children":["정책을 바탕으로 인지된 상태에 대한 행동을 선택","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"π"}],["$","mo",null,{"stretchy":"false","children":"("}],["$","mi",null,{"children":"a"}],["$","mi",null,{"mathvariant":"normal","children":"∣"}],["$","mi",null,{"children":"s"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\pi(a | s)"}]]}]}]}]," -> ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"A"}],["$","mi",null,{"children":"t"}]]}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"a"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"A_{t} = a"}]]}]}]}]]}],"\n"],"className":"list-disc ml-4"}],"\n"]}],"\n",["$","li",null,{"children":["환경은 선택된 행동에 대해 보상을 에이전트에게 제공하고, 새로운 상태로 변경, 마치 바둑돌을 두면 환경이 변화 하는것과 같습니다.","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"R"}],["$","mrow",null,{"children":[["$","mi",null,{"children":"t"}],["$","mo",null,{"children":"+"}],["$","mn",null,{"children":"1"}]]}]]}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"r"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R_{t + 1} = r"}]]}]}]}],", ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"S"}],["$","mrow",null,{"children":[["$","mi",null,{"children":"t"}],["$","mo",null,{"children":"+"}],["$","mn",null,{"children":"1"}]]}]]}],["$","mo",null,{"children":"="}],["$","msup",null,{"children":[["$","mi",null,{"children":"s"}],["$","mo",null,{"mathvariant":"normal","lspace":"0em","rspace":"0em","children":"′"}]]}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"S_{t + 1} = s'"}]]}]}]}]]}],"\n"],"className":"list-disc ml-4"}],"\n"]}],"\n",["$","li",null,{"children":["상태-행동-보상을 참고하여, 현재 정책의 가치를 평가하고 더 나은 정책을 학습","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"v"}],["$","mi",null,{"children":"π"}]]}],["$","mo",null,{"stretchy":"false","children":"("}],["$","mi",null,{"children":"s"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"v_{\\pi}(s)"}]]}]}]}]," / ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"q"}],["$","mi",null,{"children":"π"}]]}],["$","mo",null,{"stretchy":"false","children":"("}],["$","mi",null,{"children":"s"}],["$","mo",null,{"separator":"true","children":","}],["$","mi",null,{"children":"a"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"q_{\\pi}(s, a)"}]]}]}]}]]}],"\n"],"className":"list-disc ml-4"}],"\n"]}],"\n"],"className":"list-decimal ml-4"}],"\n",["$","p",null,{"children":"요약: 어떤 관측한 상태에서 어떠한 행동을 하면 보상이 주어지고(양수, 음수 다 가능), 상태를 변화하고 업데이트"}],"\n",["$","h2",null,{"children":"에이전트"}],"\n",["$","p",null,{"children":"환경과 상호작용하면서 주어진 목적을 달성할 수 있는 최적의 정책을 학습하는 주체 입니다. 학습된 정책을 바탕으로 주어진 상태에 대한 행동을 선택하고, 보상을 받습니다."}],"\n",["$","h2",null,{"children":"환경"}],"\n",["$","p",null,{"children":"Agent와 상호작용 하되, Agent에 포함되지 않는 모든것을 의미합니다. Agent가 취하는 행동의 결과인 보상을 제공하며, 자신의 상태를 Agent에게 드러냅니다."}],"\n",["$","p",null,{"children":"현실의 문제에서 환경을 전부 파악하는 경우는 거의 불가능에 가까우므로, Agent에게 드러내는 환경은 실제 환경과 다를수 있습니다."}],"\n",["$","h2",null,{"children":"상태"}],"\n",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"S"}],["$","mi",null,{"children":"t"}]]}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"s"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"S_{t} = s\n"}]]}]}]}],"\n",["$","p",null,{"children":"Agent가 관측할 수 있는 정보를 바탕으로 인지된 환경의 상태를 의미합니다. 실제 환경의 상태와 Agent에 의해 인지된 상태는 (보통) 같지 않으나, 많은 경우 같다고 가정합니다."}],"\n",["$","h2",null,{"children":"행동"}],"\n",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"A"}],["$","mi",null,{"children":"t"}]]}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"a"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"A_{t} = a\n"}]]}]}]}],"\n",["$","p",null,{"children":"학습된 정책에 기반하여, 주어진 상태에서 선택한 의사결정을 의미합니다. Agent가 환경에게 전달하는 유일한 정보입니다."}],"\n",["$","p",null,{"children":"행동은 이산적인 경우와 연속적인 경우로 나뉩니다. 이산적인 경우는 Breakout의 좌우 이동, 연속적인 경우는 앵그리버드의 발사 각도 조절이 있습니다."}],"\n",["$","h2",null,{"children":"보상"}],"\n",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"R"}],["$","mrow",null,{"children":[["$","mi",null,{"children":"t"}],["$","mo",null,{"children":"+"}],["$","mn",null,{"children":"1"}]]}]]}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"r"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"R_{t + 1} = r\n"}]]}]}]}],"\n",["$","p",null,{"children":"Agent가 환경에게 받는 피드백을 의미합니다. Agent가 선택한 행동에 대한 결과로 주어지며, 보상은 양수, 음수, 0 등 Scalar Value로 주어집니다."}],"\n",["$","p",null,{"children":"예를들어, 체스말을 잃었을때 -1, 안잃었으면 0, 얻었으면 +1 인 경우가 있습니다."}],"\n",["$","p",null,{"children":"또 ,트럭 시뮬레이션의 경우 시간당 -1을 주어진다면 빠르게 하는데에 집중해 모든 오브젝트에 박아대면서 주차할 수 있습니다."}],"\n",["$","p",null,{"children":"어떤 예에선 게임 AI가 -1을 받지 않기 위해 승리(+1) 보다 게임을 멈추는 방법(0) 을 학습했다."}],"\n",["$","p",null,{"children":"이러한 적절한 보상의 설게가 RL의 주요한 난제 입니다."}],"\n",["$","h2",null,{"children":"정책"}],"\n",["$","p",null,{"children":"상태를 입력으로, 출력을 행동으로 하는 함수로, 주어진 상태에 대한 Agent의 행동을 결정합니다."}],"\n",["$","h3",null,{"children":"결정론적 정책(Deterministic Policy)"}],"\n",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"a"}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"π"}],["$","mo",null,{"stretchy":"false","children":"("}],["$","mi",null,{"children":"s"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"a = \\pi(s)\n"}]]}]}]}],"\n",["$","p",null,{"children":"결정적 정책에선 동일한 상태에 대해 동일한 행동을 하는 정책을 의미합니다."}],"\n",["$","h3",null,{"children":"확률적 정책(Stochastic Policy)"}],"\n",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"π"}],["$","mo",null,{"stretchy":"false","children":"("}],["$","mi",null,{"children":"a"}],["$","mi",null,{"mathvariant":"normal","children":"∣"}],["$","mi",null,{"children":"s"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\pi(a | s)\n"}]]}]}]}],"\n",["$","p",null,{"children":"상태에 대해 행동을할 확률을 출력으로 내는 정책을 의미합니다. 이는 동일한 상태에 대해 여러 행동을 선택할 수 있습니다."}],"\n",["$","h2",null,{"children":"수익(return)"}],"\n",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"G"}],["$","mi",null,{"children":"t"}]]}],["$","mo",null,{"children":"="}],["$","msub",null,{"children":[["$","mi",null,{"children":"R"}],["$","mi",null,{"children":"t"}]]}],["$","mo",null,{"children":"+"}],["$","mi",null,{"children":"γ"}],["$","msub",null,{"children":[["$","mi",null,{"children":"R"}],["$","mrow",null,{"children":[["$","mi",null,{"children":"t"}],["$","mo",null,{"children":"+"}],["$","mn",null,{"children":"1"}]]}]]}],["$","mo",null,{"children":"+"}],["$","msup",null,{"children":[["$","mi",null,{"children":"γ"}],["$","mn",null,{"children":"2"}]]}],["$","msub",null,{"children":[["$","mi",null,{"children":"R"}],["$","mrow",null,{"children":[["$","mi",null,{"children":"t"}],["$","mo",null,{"children":"+"}],["$","mn",null,{"children":"2"}]]}]]}],["$","mo",null,{"children":"+"}],["$","msup",null,{"children":[["$","mi",null,{"children":"γ"}],["$","mn",null,{"children":"3"}]]}],["$","msub",null,{"children":[["$","mi",null,{"children":"R"}],["$","mrow",null,{"children":[["$","mi",null,{"children":"t"}],["$","mo",null,{"children":"+"}],["$","mn",null,{"children":"3"}]]}]]}],["$","mo",null,{"children":"+"}],["$","mo",null,{"children":"⋯"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"G_{t} = R_{t} + {\\gamma}R_{t + 1} + {\\gamma}^2R_{t + 2} + {\\gamma}^3R_{t + 3} + \\cdots\n"}]]}]}]}],"\n",["$","p",null,{"children":["현재 시점부터 미래까지 받을 수 있는 보상의 누적합을 의미합니다. ",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":["$","mi",null,{"children":"γ"}]}],["$","annotation",null,{"encoding":"application/x-tex","children":"\\gamma"}]]}]}]}],"는 0 ~ 1값을 가지며 할인율을 의미합니다. 미래의 보상이 현재의 보상보다 얼마나 중요한지를 결정합니다. 현재 시점에 가까울 수록 보상의 가치가 크게 됩니다."]}],"\n",["$","p",null,{"children":"강화학습의 Agent는 Return을 최대화 하는것을 목적으로 학습합니다. 정확히는 Expected Return을 최대화 하는것을 목적으로 합니다."}],"\n",["$","p",null,{"children":"왜 Explected Return 이냐면, state와 action이 random variable이기 때문에, 어떤 행동을 했을때 어떤 보상또한 ranom variable이기 때문입니다."}],"\n",["$","h2",null,{"children":"가치 함수(Value Function)"}],"\n",["$","p",null,{"children":"정책의 가치를 평가하는 함수를 의미하고 2가지 종류가 있습니다. 가치란 특정 상태에서 얻을 수 있는 수익의 기대값을 의미합니다."}],"\n",["$","h3",null,{"children":"상태 가치 함수(State Value Function)"}],"\n",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"v"}],["$","mi",null,{"children":"π"}]]}],["$","mo",null,{"stretchy":"false","children":"("}],["$","mi",null,{"children":"s"}],["$","mo",null,{"stretchy":"false","children":")"}],["$","mo",null,{"children":"="}],["$","msub",null,{"children":[["$","mi",null,{"children":"E"}],["$","mi",null,{"children":"π"}]]}],["$","mo",null,{"stretchy":"false","children":"["}],["$","msub",null,{"children":[["$","mi",null,{"children":"G"}],["$","mi",null,{"children":"t"}]]}],["$","mi",null,{"mathvariant":"normal","children":"∣"}],["$","msub",null,{"children":[["$","mi",null,{"children":"S"}],["$","mi",null,{"children":"t"}]]}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"s"}],["$","mo",null,{"stretchy":"false","children":"]"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"v_{\\pi}(s) = E_{\\pi}[G_{t} | S_{t} = s]\n"}]]}]}]}],"\n",["$","p",null,{"children":"정책이 주어 졌을때, 주어진 상태에서 얻을 수 있는 수익의 기대값을 의미합니다."}],"\n",["$","h3",null,{"children":"행동 가치 함수(Action Value Function)"}],"\n",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","msub",null,{"children":[["$","mi",null,{"children":"q"}],["$","mi",null,{"children":"π"}]]}],["$","mo",null,{"stretchy":"false","children":"("}],["$","mi",null,{"children":"s"}],["$","mo",null,{"separator":"true","children":","}],["$","mi",null,{"children":"a"}],["$","mo",null,{"stretchy":"false","children":")"}],["$","mo",null,{"children":"="}],["$","msub",null,{"children":[["$","mi",null,{"children":"E"}],["$","mi",null,{"children":"π"}]]}],["$","mo",null,{"stretchy":"false","children":"["}],["$","msub",null,{"children":[["$","mi",null,{"children":"G"}],["$","mi",null,{"children":"t"}]]}],["$","mi",null,{"mathvariant":"normal","children":"∣"}],["$","msub",null,{"children":[["$","mi",null,{"children":"S"}],["$","mi",null,{"children":"t"}]]}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"s"}],["$","mo",null,{"separator":"true","children":","}],["$","msub",null,{"children":[["$","mi",null,{"children":"A"}],["$","mi",null,{"children":"t"}]]}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"a"}],["$","mo",null,{"stretchy":"false","children":"]"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"q_{\\pi}(s, a) = E_{\\pi}[G_{t} | S_{t} = s, A_{t} = a]\n"}]]}]}]}],"\n",["$","p",null,{"children":"정책이 주어졌을때, 주어진 상태와 행동에서 얻을 수 있는 수익의 기대값을 의미합니다."}],"\n",["$","h2",null,{"children":"모델(Model)"}],"\n",["$","span",null,{"className":"katex","children":["$","math",null,{"xmlns":"http://www.w3.org/1998/Math/MathML","display":"block","children":["$","semantics",null,{"children":[["$","mrow",null,{"children":[["$","mi",null,{"children":"P"}],["$","mo",null,{"stretchy":"false","children":"("}],["$","msup",null,{"children":[["$","mi",null,{"children":"s"}],["$","mo",null,{"mathvariant":"normal","lspace":"0em","rspace":"0em","children":"′"}]]}],["$","mo",null,{"separator":"true","children":","}],["$","mi",null,{"children":"r"}],["$","mi",null,{"mathvariant":"normal","children":"∣"}],["$","mi",null,{"children":"s"}],["$","mo",null,{"separator":"true","children":","}],["$","mi",null,{"children":"a"}],["$","mo",null,{"stretchy":"false","children":")"}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"P"}],["$","mo",null,{"stretchy":"false","children":"("}],["$","msub",null,{"children":[["$","mi",null,{"children":"S"}],["$","mrow",null,{"children":[["$","mi",null,{"children":"t"}],["$","mo",null,{"children":"+"}],["$","mn",null,{"children":"1"}]]}]]}],["$","mo",null,{"children":"="}],["$","msup",null,{"children":[["$","mi",null,{"children":"s"}],["$","mo",null,{"mathvariant":"normal","lspace":"0em","rspace":"0em","children":"′"}]]}],["$","mo",null,{"separator":"true","children":","}],["$","msub",null,{"children":[["$","mi",null,{"children":"R"}],["$","mrow",null,{"children":[["$","mi",null,{"children":"t"}],["$","mo",null,{"children":"+"}],["$","mn",null,{"children":"1"}]]}]]}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"r"}],["$","mi",null,{"mathvariant":"normal","children":"∣"}],["$","msub",null,{"children":[["$","mi",null,{"children":"S"}],["$","mi",null,{"children":"t"}]]}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"s"}],["$","mo",null,{"separator":"true","children":","}],["$","msub",null,{"children":[["$","mi",null,{"children":"A"}],["$","mi",null,{"children":"t"}]]}],["$","mo",null,{"children":"="}],["$","mi",null,{"children":"a"}],["$","mo",null,{"stretchy":"false","children":")"}]]}],["$","annotation",null,{"encoding":"application/x-tex","children":"P(s', r | s, a) = P(S_{t + 1} = s', R_{t + 1} = r | S_{t} = s, A_{t} = a)\n"}]]}]}]}],"\n",["$","p",null,{"children":"이전에 실제 환경과 Agent에게 드러난 환경이 다르다고 했습니다. Model은 주어진 상태에서 Agent가 내린 행동에 대해, 환경의 다음 반응(보상 및 다음 상태)를 예층하는 함수 입니다."}],"\n",["$","p",null,{"children":"오직 관측된 정보를 바탕으로 추정하며, 실제 환경과 다를 수 있습니다. 또 Model은 RL Agent를 학습시키는 데 필수적인 요소는 아닙니다."}]]}]}]
