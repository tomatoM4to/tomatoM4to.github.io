# 동적계획법과 벨만 방정식
여기서 Q(s) = ...Q(s') 이때 s'는 다음 상태지만, s도 포함하는 상태다, s는 현재상태

현재의 가치상태 함수 값은 당장 받을거랑 그다음 상태로 같을때의 수익을 더한것임

***

# 최적 상태 가치 함수의 추정
DP 는 모델을 알고있고 자연스럽게 현재 상태의 최적의 행동도 알고 있다.

근데 순차적으로 가는거라 최적의 행동을 해도 진짜 최적의 결과가 나오진 않는다.

결국 모르는건 V*(s) or V*(s') 다.

계속 말하지만 V*는 이론적으로 존재하는거지, 실제 조재하는지 조차 못한다.

그래서 결국 순순차적으로 추정해야 한한다. 사실 추정하는건 불가가능하다.

### EX)
전부 계산해 줬다 (교수님이)

1. 액션은 2개, 가속을 밟을지, 미친듯이 밟을지

2. 그냥 엑셀을 밟을경우 노말로 들엉ㅈ만 reward 1임

3. 미친듯이 밟으면 reward는 2지만 과열을 갈수도 노말로 갈수도 있음(50, 50)

4. 과열된 상태에서 그냥 엘셀을 밟으면 50, 50 으로 노말을 가거나 유지함

5. 과열에서 오버앨셀 하면 터짐, reward 는 10임


terminated에서 가치함수는 무조건 0이다 <- 중요함

***

# 싱크 & 어싱크
그냥 어싱크 쓰셈 그게 더 좋음, 이유는 그냥 최최신걸 쓰니까.. 왜 그런진 설명 못함

***

# 단점: 비효율적
DP의 단점이기도 함

이전 설명에서 사실 v1 에서 최적을 찾았기 때문에 v2에 갈 필요도 없음, 더 정확해지긴 하지만

더이상 가치함수에 대한 변화는 없다는 거지, 근본적으로 우리가 원하는건 파이*(a) 임

바로 정책을 찾는거지 가지함수를 찾는게 아니기에 쓰잘데기 없는 계산을 많이함

이 예예에선 2개의 상태지만, 아아타리 블럭깨기만 하더라도 상태가 수억개임

그래서 많이 사용돼는 방식은 아니지만매우 간단함

두번째론 시그그마가 겹쳐져 있게 모든 상태? 를 다 계계산 해야함

프로그래밍 적으로 보면 2중포문을 제곱만 돌돌려야함

강화학습에선 속도도 매우 중요함, 게임 같은 경우는 괜찮은데 사람이랑 상호작용 하려면 비용이 매우 많이듬

DP는 풀면 최적 정책을 100% 보장해주지만 현실적으론 어려움


# Policy Evaluation
여기서 과거의 추정치, 새새로ㅜㄴ 추정치 같은 빨빨간색, 파란색, 보라색, 핑크색 있는 페이지에서

until V*(k) 여기 *가 아니라 파이임, V*(K-1) 도 *가 아니라 파이임

더 낳은정책을 위해 결과값을 다시 넣고 돌리면 돼는데, 참 신기한 방식식임

이걸 반복하면 언젠간 수렴하고 이를 이용해 V*을 찾을수 있음

정책은 참 어려운 말이지만 일단 파이썬 딕셔너리라 하자

키로 s1, s2, s3를 넣는다

s1 넣으면 오른쪽 나오오고 s2 집집어 넣으면 위 나오고...

수렴이란건 특정 키를 넣으면 항상 오른쪽 나오는 그런거임

이 페이지에서 공부부한건 결국 벨만최적방정식을 찾기 위함임

***

# Generalized Policy Iteration, GPI
댑부분의 AI는 Evaliation과 Improvment를 바뀌뀌는게 없을때까지 반복함

ProceduCODE는 이런작업물 없으니까 그냥 복붙 ㄱㄱ

***
# Policy Iteration
정책은 변하지 않고 가치함수가 변한다다는것을 생각


***

# 동적계획법 VS 강화학습
사실 강화학습은 나가지도 않않았음

## Backup Diagram
어떤 상태에서 정책에 따라 행동을 한후 보상을 받은다음 다음 상태로 감

DP는 모든 경우를 고고려함 이이를 full backup이라 한다.
